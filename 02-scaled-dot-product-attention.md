# Scaled Dot-Product Attentionï¼šTransformerçš„æ ¸å¿ƒå¼•æ“æ·±åº¦è§£æ

## ğŸ¯ å¼•è¨€

æƒ³è±¡ä¸€ä¸‹ä½ åœ¨ä¸€ä¸ªå˜ˆæ‚çš„æ´¾å¯¹ä¸Šï¼Œæƒ³è¦æ‰¾åˆ°æœ€æœ‰è¶£çš„äººèŠå¤©ã€‚ä½ å¯ä»¥ï¼š

1. **ç®€å•æ–¹æ³•**ï¼šå’Œæ¯ä¸ªäººç®€å•èŠå‡ å¥ï¼Œçœ‹çœ‹è°æœ€æœ‰è¶£
2. **å¤æ‚æ–¹æ³•**ï¼šæ·±å…¥äº†è§£æ¯ä¸ªäººçš„èƒŒæ™¯ã€å…´è¶£ã€æ€§æ ¼ï¼Œç„¶ååšå‡ºç²¾å‡†åˆ¤æ–­

åœ¨Attentionæœºåˆ¶ä¸­ï¼Œè¿™ä¸¤ç§æ–¹æ³•å¯¹åº”ç€ä¸åŒçš„ç›¸ä¼¼åº¦è®¡ç®—ã€‚è€ŒScaled Dot-Product Attentioné€‰æ‹©äº†ç¬¬ä¸€ç§æ–¹æ³•ï¼Œä½†åŠ å…¥äº†ä¸€ä¸ªå…³é”®çš„"ç¼©æ”¾"æ­¥éª¤ï¼Œè®©è¿™ä¸ªç®€å•æ–¹æ³•å˜å¾—å¼‚å¸¸å¼ºå¤§ã€‚

Scaled Dot-Product Attentionæ˜¯Transformeræ¨¡å‹çš„æ ¸å¿ƒå¼•æ“ï¼Œå®ƒçš„è®¾è®¡çœ‹ä¼¼ç®€å•â€”â€”ä»…ä»…æ˜¯"ç‚¹ç§¯+ç¼©æ”¾+Softmax"ï¼Œä½†å…¶ä¸­è•´å«çš„æ•°å­¦åŸç†å’Œå·¥ç¨‹æ™ºæ…§å´å€¼å¾—æ·±å…¥æ¢ç©¶ã€‚

æœ¬æ–‡å°†ä»¥"æµ…è€…è§‰å…¶æµ…ï¼Œæ·±è€…è§‰å…¶æ·±"çš„æ–¹å¼ï¼Œä»æœ€ç›´è§‚çš„æ•°å­¦ç›´è§‰å¼€å§‹ï¼Œé€æ­¥æ·±å…¥åˆ°å¤æ‚çš„æ•°å€¼ç¨³å®šæ€§é—®é¢˜ï¼Œè®©ä½ çœŸæ­£ç†è§£ä¸ºä»€ä¹ˆè¿™ä¸ªçœ‹ä¼¼ç®€å•çš„å…¬å¼èƒ½å¤Ÿæ”¯æ’‘èµ·æ•´ä¸ªTransformeræ¶æ„ã€‚

## ğŸ” ç‚¹ç§¯ç›¸ä¼¼åº¦çš„æ•°å­¦æœ¬è´¨

### ä¸ºä»€ä¹ˆé€‰æ‹©ç‚¹ç§¯ï¼Ÿ

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆç†è§£ä¸ºä»€ä¹ˆç‚¹ç§¯æ˜¯è®¡ç®—ç›¸ä¼¼åº¦çš„å¥½æ–¹æ³•ã€‚

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def dot_product_similarity_demo():
    """æ¼”ç¤ºç‚¹ç§¯ç›¸ä¼¼åº¦çš„å‡ ä½•å«ä¹‰"""

    print("ç‚¹ç§¯ç›¸ä¼¼åº¦çš„å‡ ä½•å«ä¹‰")
    print("=" * 50)

    # åˆ›å»ºä¸€äº›3Då‘é‡
    vectors = {
        "å‘é‡A": np.array([1, 0, 0]),      # xè½´æ–¹å‘
        "å‘é‡B": np.array([0, 1, 0]),      # yè½´æ–¹å‘
        "å‘é‡C": np.array([0.7, 0.7, 0]),   # 45åº¦æ–¹å‘
        "å‘é‡D": np.array([-1, 0, 0]),     # åæ–¹å‘
        "å‘é‡E": np.array([0.5, 0.5, 0.5])  # ç›¸åŒæ–¹å‘
    }

    # è®¡ç®—ä¸¤ä¸¤ä¹‹é—´çš„ç‚¹ç§¯
    names = list(vectors.keys())
    print("ç‚¹ç§¯ç›¸ä¼¼åº¦çŸ©é˜µ:")
    print("-" * 30)
    print(f"{'':<8}", end='')
    for name in names:
        print(f"{name:<8}", end='')
    print()

    for i, name1 in enumerate(names):
        print(f"{name1:<8}", end='')
        for j, name2 in enumerate(names):
            dot_product = np.dot(vectors[name1], vectors[name2])
            print(f"{dot_product:<8.1f}", end='')
        print()

    # 3Då¯è§†åŒ–
    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')

    # ç»˜åˆ¶å‘é‡
    colors = ['red', 'blue', 'green', 'orange', 'purple']
    for i, (name, vec) in enumerate(vectors.items()):
        ax.quiver(0, 0, 0, vec[0], vec[1], vec[2],
                  color=colors[i], arrow_length_ratio=0.1,
                  linewidth=3, label=name)

    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    ax.set_title('å‘é‡ç‚¹ç§¯çš„å‡ ä½•å«ä¹‰')
    ax.legend()

    # æ·»åŠ æ–‡å­—è¯´æ˜
    ax.text2D(0.05, 0.95, "è§‚å¯Ÿç‚¹ç§¯æ¨¡å¼:", transform=ax.transAxes)
    ax.text2D(0.05, 0.90, "â€¢ ç›¸åŒæ–¹å‘: å¤§æ­£å€¼ (Eå’ŒC)", transform=ax.transAxes)
    ax.text2D(0.05, 0.85, "â€¢ å‚ç›´æ–¹å‘: é›¶ (Aå’ŒB)", transform=ax.transAxes)
    ax.text2D(0.05, 0.80, "â€¢ ç›¸åæ–¹å‘: å¤§è´Ÿå€¼ (Aå’ŒD)", transform=ax.transAxes)

    plt.tight_layout()
    plt.show()

dot_product_similarity_demo()
```

### ç‚¹ç§¯çš„æ•°å­¦ç‰¹æ€§

ç‚¹ç§¯æœ‰å‡ ä¸ªé‡è¦çš„æ•°å­¦ç‰¹æ€§ï¼Œä½¿å…¶æˆä¸ºè®¡ç®—ç›¸ä¼¼åº¦çš„ç†æƒ³é€‰æ‹©ï¼š

1. **æ–¹å‘æ€§**ï¼šå‘é‡è¶Šç›¸ä¼¼ï¼Œç‚¹ç§¯è¶Šå¤§
2. **è¿ç»­æ€§**ï¼šå°å˜åŒ–å¯¼è‡´å°å˜åŒ–ï¼Œé€‚åˆæ¢¯åº¦ä¼˜åŒ–
3. **è®¡ç®—æ•ˆç‡**ï¼šç¡¬ä»¶å‹å¥½çš„å¹¶è¡Œè®¡ç®—
4. **ç†è®ºåŸºç¡€**ï¼šåŸºäºå‡ ä½•å­¦ï¼Œæœ‰åšå®çš„æ•°å­¦åŸºç¡€

## ğŸ“ ç¼©æ”¾å› å­çš„æ·±å±‚å«ä¹‰

### ä¸ºä»€ä¹ˆéœ€è¦ç¼©æ”¾ï¼Ÿ

å¾ˆå¤šäººå›°æƒ‘äºä¸ºä»€ä¹ˆè¦é™¤ä»¥âˆšdï¼ˆdæ˜¯å‘é‡ç»´åº¦ï¼‰ã€‚è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå®éªŒæ¥ç†è§£è¿™ä¸ªé—®é¢˜ï¼š

```python
def scaling_factor_importance():
    """æ¼”ç¤ºç¼©æ”¾å› å­çš„é‡è¦æ€§"""

    print("ç¼©æ”¾å› å­çš„é‡è¦æ€§åˆ†æ")
    print("=" * 50)

    def softmax(x, scale=1.0):
        """å¸¦ç¼©æ”¾çš„Softmax"""
        x_scaled = x * scale
        exp_x = np.exp(x_scaled - np.max(x_scaled))
        return exp_x / np.sum(exp_x)

    # æµ‹è¯•ä¸åŒç»´åº¦ä¸‹çš„softmaxè¡Œä¸º
    dimensions = [8, 64, 512, 2048, 8192]
    scale_factors = [1.0, 1/np.sqrt(d) for d in dimensions]

    print(f"{'ç»´åº¦':<8} {'æ ‡å‡†ç¼©æ”¾':<12} {'å®é™…ç¼©æ”¾':<12} {'æ¢¯åº¦æ–¹å·®':<12}")
    print("-" * 50)

    for i, d in enumerate(dimensions):
        # åˆ›å»ºä¸€äº›æµ‹è¯•åˆ†æ•°
        np.random.seed(42)
        scores = np.random.randn(100) * np.sqrt(d)  # æ–¹å·®ä¸ç»´åº¦ç›¸å…³

        # æ ‡å‡†softmax (æ— ç¼©æ”¾)
        std_softmax = softmax(scores, scale=1.0)
        std_grad_variance = np.var(std_softmax * (1 - std_softmax))

        # ç¼©æ”¾softmax
        scaled_softmax = softmax(scores, scale=scale_factors[i])
        scaled_grad_variance = np.var(scaled_softmax * (1 - scaled_softmax))

        print(f"{d:<8} {1.0:<12.6f} {scale_factors[i]:<12.6f} {std_grad_variance:<12.6f}")

    print(f"\nè§‚å¯Ÿç»“æœ:")
    print("â€¢ éšç€ç»´åº¦å¢åŠ ï¼Œæ ‡å‡†Softmaxçš„æ¢¯åº¦æ–¹å·®å¢å¤§")
    print("â€¢ é€‚å½“ç¼©æ”¾åï¼Œæ¢¯åº¦æ–¹å·®ä¿æŒç¨³å®š")
    print("â€¢ ç¼©æ”¾å› å­ = 1/âˆšd æ˜¯æœ€ä¼˜é€‰æ‹©")

scaling_factor_importance()
```

### æ•°å­¦æ¨å¯¼ï¼šç¼©æ”¾å› å­çš„é€‰æ‹©

è®©æˆ‘ä»¬é€šè¿‡æ•°å­¦æ¨å¯¼æ¥ç†è§£ä¸ºä»€ä¹ˆé€‰æ‹©1/âˆšdä½œä¸ºç¼©æ”¾å› å­ï¼š

```python
def scaling_factor_derivation():
    """ç¼©æ”¾å› å­çš„æ•°å­¦æ¨å¯¼"""

    print("ç¼©æ”¾å› å­é€‰æ‹©çš„æ•°å­¦æ¨å¯¼")
    print("=" * 50)

    print("å‡è®¾æ¡ä»¶:")
    print("1. Qå’ŒKçš„æ¯ä¸ªåˆ†é‡éƒ½æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒ N(0,1)")
    print("2. Qå’ŒKæ˜¯ç‹¬ç«‹çš„")
    print("3. å‘é‡ç»´åº¦ä¸º d")
    print()

    print("æ­¥éª¤1: è®¡ç®—ç‚¹ç§¯ QÂ·K çš„æœŸæœ›å’Œæ–¹å·®")
    print("-" * 40)
    print("E[Q_i] = 0, E[K_i] = 0")
    print("Var[Q_i] = 1, Var[K_i] = 1")
    print("E[QÂ·K] = Î£_i E[Q_i * K_i] = Î£_i E[Q_i] * E[K_i] = 0")
    print("Var[QÂ·K] = Î£_i Var[Q_i * K_i] = Î£_i E[Q_iÂ²] * E[K_iÂ²] = d")
    print()

    print("æ­¥éª¤2: åˆ†æSoftmaxçš„æ¢¯åº¦")
    print("-" * 40)
    print("Softmaxæ¢¯åº¦: âˆ‚softmax_i/âˆ‚x_j = softmax_i * (Î´_ij - softmax_j)")
    print("æ¢¯åº¦æ–¹å·®: Var[âˆ‡softmax] â‰ˆ softmax_i * (1 - softmax_i)")
    print()

    print("æ­¥éª¤3: ç»´åº¦å¯¹æ¢¯åº¦çš„å½±å“")
    print("-" * 40)
    print("å½“ d å¢å¤§æ—¶:")
    print("â€¢ QÂ·K çš„æ–¹å·® Var = d å¢å¤§")
    print("â€¢ Softmaxè¾“å…¥çš„åˆ†å¸ƒå˜å®½")
    print("â€¢ Softmaxè¾“å‡ºçš„åˆ†å¸ƒå˜å¾—æ›´å°–é”")
    print("â€¢ æ¢¯åº¦å˜å¾—æå°ï¼Œæ¢¯åº¦æ¶ˆå¤±é—®é¢˜åŠ å‰§")
    print()

    print("æ­¥éª¤4: å¯»æ‰¾æœ€ä¼˜ç¼©æ”¾å› å­")
    print("-" * 40)
    print("ç›®æ ‡: ä½¿ QÂ·K çš„æ–¹å·®ä¸º 1 (ç¨³å®š)")
    print("æ–¹æ³•: ä»¤ scale Ã— Var[QÂ·K] = 1")
    print("æ¨å¯¼: scale Ã— d = 1 â‡’ scale = 1/d")
    print("é—®é¢˜: 1/d ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±")
    print()

    print("æ­¥éª¤5: æœ€ä¼˜ç¼©æ”¾å› å­çš„é€‰æ‹©")
    print("-" * 40)
    print("ç†è®ºåˆ†æè¡¨æ˜ï¼Œscale = 1/âˆšd æ˜¯æœ€ä¼˜é€‰æ‹©:")
    print("â€¢ ä¿æŒæ–¹å·®é€‚ä¸­: scale Ã— d = âˆšd")
    print("â€¢ æ¢¯åº¦ç¨³å®š: é¿å…æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸")
    print("â€¢ æ•°å€¼ç¨³å®š: é€‚åˆä¸åŒç»´åº¦")
    print()

    print("ç»“è®º: Scaled Dot-Product Attention ä½¿ç”¨ 1/âˆšd ä½œä¸ºç¼©æ”¾å› å­")

scaling_factor_derivation()
```

### å®éªŒéªŒè¯ï¼šä¸åŒç¼©æ”¾å› å­çš„æ•ˆæœ

```python
def compare_scaling_factors():
    """æ¯”è¾ƒä¸åŒç¼©æ”¾å› å­çš„æ•ˆæœ"""

    import torch
    import torch.nn.functional as F

    print("ä¸åŒç¼©æ”¾å› å­çš„æ•ˆæœå¯¹æ¯”")
    print("=" * 50)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    torch.manual_seed(42)
    d = 512
    seq_len = 1000
    batch_size = 4

    Q = torch.randn(batch_size, seq_len, d)
    K = torch.randn(batch_size, seq_len, d)
    V = torch.randn(batch_size, seq_len, d)

    # ä¸åŒçš„ç¼©æ”¾å› å­
    scaling_factors = {
        "æ— ç¼©æ”¾": 1.0,
        "1/âˆšd": 1.0 / np.sqrt(d),
        "1/d": 1.0 / d,
        "âˆšd": np.sqrt(d)
    }

    print(f"å‘é‡ç»´åº¦: d = {d}")
    print(f"æœ€ä¼˜ç¼©æ”¾: 1/âˆšd = {scaling_factors['1/âˆšd']:.6f}")
    print()
    print(f"{'ç¼©æ”¾å› å­':<12} {'æ¢¯åº¦èŒƒæ•°':<12} {'è¾“å‡ºèŒƒæ•°':<12} {'æ•°å€¼ç¨³å®šæ€§':<12}")
    print("-" * 50)

    results = {}
    for name, scale in scaling_factors.items():
        # è®¡ç®—Attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) * scale
        weights = F.softmax(scores, dim=-1)
        output = torch.matmul(weights, V)

        # è®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼ˆæ¨¡æ‹Ÿï¼‰
        dummy_loss = output.sum()
        dummy_loss.backward(retain_graph=True)

        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        q_grad_norm = Q.grad.norm().item()

        # è®¡ç®—è¾“å‡ºçš„ç»Ÿè®¡ç‰¹æ€§
        output_mean = output.mean().item()
        output_std = output.std().item()

        # è¯„ä¼°æ•°å€¼ç¨³å®šæ€§
        weight_max = weights.max().item()
        weight_min = weights.min().item()
        stability = weight_max / weight_min if weight_min > 1e-8 else float('inf')

        print(f"{name:<12} {q_grad_norm:<12.2f} {output_std:<12.4f} {stability:<12.2e}")

        # æ¸…ç†æ¢¯åº¦
        Q.grad = None

        results[name] = {
            'grad_norm': q_grad_norm,
            'output_std': output_std,
            'stability': stability
        }

    # ç»“æœåˆ†æ
    print("\nç»“æœåˆ†æ:")
    print("-" * 30)
    print("1. 1/âˆšd çš„æ¢¯åº¦èŒƒæ•°é€‚ä¸­ï¼Œæ—¢ä¸è¿‡å¤§ä¹Ÿä¸è¿‡å°")
    print("2. 1/âˆšd çš„è¾“å‡ºæ ‡å‡†å·®ç¨³å®š")
    print("3. 1/âˆšd çš„æ•°å€¼ç¨³å®šæ€§æœ€å¥½")

compare_scaling_factors()
```

## ğŸ§® å®Œæ•´çš„Scaled Dot-Product Attentionå®ç°

### é€æ­¥å®ç°

è®©æˆ‘ä»¬ä»é›¶å¼€å§‹å®ç°ä¸€ä¸ªå®Œæ•´çš„Scaled Dot-Product Attentionï¼š

```python
import torch
import torch.nn.functional as F

class ScaledDotProductAttention:
    """Scaled Dot-Product Attentionçš„å®Œæ•´å®ç°"""

    def __init__(self, d_model, dropout=0.1):
        """
        åˆå§‹åŒ–Scaled Dot-Product Attention

        Args:
            d_model: æ¨¡å‹ç»´åº¦
            dropout: Dropoutæ¦‚ç‡
        """
        self.d_model = d_model
        self.dropout = dropout

    def forward(self, Q, K, V, mask=None, training=True):
        """
        å‰å‘ä¼ æ’­

        Args:
            Q: Queryå¼ é‡ [batch_size, seq_len, d_model]
            K: Keyå¼ é‡ [batch_size, seq_len, d_model]
            V: Valueå¼ é‡ [batch_size, seq_len, d_model]
            mask: æ³¨æ„åŠ›æ©ç  [batch_size, seq_len, seq_len] (å¯é€‰)
            training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼

        Returns:
            output: æ³¨æ„åŠ›è¾“å‡º [batch_size, seq_len, d_model]
            attention_weights: æ³¨æ„åŠ›æƒé‡ [batch_size, seq_len, seq_len]
        """
        batch_size, seq_len, d_model = Q.shape

        # æ­¥éª¤1: è®¡ç®—ç‚¹ç§¯ç›¸ä¼¼åº¦
        # [batch_size, seq_len, d_model] Ã— [batch_size, d_model, seq_len] â†’ [batch_size, seq_len, seq_len]
        scores = torch.matmul(Q, K.transpose(-2, -1))

        print(f"æ­¥éª¤1: ç‚¹ç§¯è®¡ç®—å®Œæˆï¼Œå½¢çŠ¶: {scores.shape}")
        print(f"ç‚¹ç§¯ç»Ÿè®¡: å‡å€¼={scores.mean():.4f}, æ ‡å‡†å·®={scores.std():.4f}")

        # æ­¥éª¤2: ç¼©æ”¾ (å…³é”®æ­¥éª¤!)
        scale_factor = torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))
        scores = scores / scale_factor

        print(f"æ­¥éª¤2: ç¼©æ”¾å®Œæˆï¼Œç¼©æ”¾å› å­={scale_factor:.4f}")
        print(f"ç¼©æ”¾åç»Ÿè®¡: å‡å€¼={scores.mean():.4f}, æ ‡å‡†å·®={scores.std():.4f}")

        # æ­¥éª¤3: åº”ç”¨æ©ç  (å¦‚æœæä¾›)
        if mask is not None:
            print(f"æ­¥éª¤3: åº”ç”¨æ©ç ï¼Œæ©ç å½¢çŠ¶: {mask.shape}")
            scores = scores.masked_fill(mask == 0, -1e9)
            print("æ©ç åº”ç”¨å®Œæˆ")

        # æ­¥éª¤4: Softmaxå½’ä¸€åŒ–
        attention_weights = F.softmax(scores, dim=-1)
        print(f"æ­¥éª¤4: Softmaxå®Œæˆï¼Œå½¢çŠ¶: {attention_weights.shape}")
        print(f"Softmaxç»Ÿè®¡: æœ€å¤§={attention_weights.max():.4f}, æœ€å°={attention_weights.min():.4f}")

        # æ­¥éª¤5: Dropout (ä»…åœ¨è®­ç»ƒæ—¶)
        if training and self.dropout > 0:
            print(f"æ­¥éª¤5: åº”ç”¨Dropout, æ¦‚ç‡={self.dropout}")
            attention_weights = F.dropout(attention_weights, p=self.dropout, training=True)
            print("Dropoutåº”ç”¨å®Œæˆ")

        # æ­¥éª¤6: åŠ æƒæ±‚å’Œ
        # [batch_size, seq_len, seq_len] Ã— [batch_size, seq_len, d_model] â†’ [batch_size, seq_len, d_model]
        output = torch.matmul(attention_weights, V)
        print(f"æ­¥éª¤6: åŠ æƒæ±‚å’Œå®Œæˆï¼Œå½¢çŠ¶: {output.shape}")

        return output, attention_weights

    def backward(self, grad_output, attention_weights, Q, K, V):
        """
        åå‘ä¼ æ’­ (ç®€åŒ–ç‰ˆ)

        Args:
            grad_output: è¾“å‡ºæ¢¯åº¦ [batch_size, seq_len, d_model]
            attention_weights: æ³¨æ„åŠ›æƒé‡ [batch_size, seq_len, seq_len]
            Q, K, V: åŸå§‹è¾“å…¥å¼ é‡

        Returns:
            dQ, dK, dV: è¾“å…¥æ¢¯åº¦
        """
        batch_size, seq_len, d_model = Q.shape
        scale = torch.sqrt(torch.tensor(d_model, dtype=torch.float32))

        # Vçš„æ¢¯åº¦
        dV = torch.matmul(attention_weights.transpose(-2, -1), grad_output)

        # æ³¨æ„åŠ›æƒé‡çš„æ¢¯åº¦
        d_attention_weights = torch.matmul(grad_output, V.transpose(-2, -1))

        # Qå’ŒKçš„æ¢¯åº¦
        dK = torch.matmul(d_attention_weights.transpose(-2, -1), Q) / scale
        dQ = torch.matmul(d_attention_weights, K) / scale

        print("åå‘ä¼ æ’­å®Œæˆ:")
        print(f"dVå½¢çŠ¶: {dV.shape}")
        print(f"dKå½¢çŠ¶: {dK.shape}")
        print(f"dQå½¢çŠ¶: {dQ.shape}")

        return dQ, dK, dV

# æµ‹è¯•å®ç°
def test_scaled_dot_product_attention():
    """æµ‹è¯•Scaled Dot-Product Attentionå®ç°"""

    print("Scaled Dot-Product Attentionæµ‹è¯•")
    print("=" * 60)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, d_model = 2, 8, 16
    torch.manual_seed(42)

    Q = torch.randn(batch_size, seq_len, d_model, requires_grad=True)
    K = torch.randn(batch_size, seq_len, d_model, requires_grad=True)
    V = torch.randn(batch_size, seq_len, d_model, requires_grad=True)

    # åˆ›å»ºå› æœæ©ç 
    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)
    mask = mask.expand(batch_size, 1, seq_len, seq_len)

    print(f"è¾“å…¥å½¢çŠ¶: Q={Q.shape}, K={K.shape}, V={V.shape}")
    print(f"æ©ç å½¢çŠ¶: {mask.shape}")
    print()

    # å®ä¾‹åŒ–Attention
    attention = ScaledDotProductAttention(d_model=d_model, dropout=0.1)

    # å‰å‘ä¼ æ’­
    print("å‰å‘ä¼ æ’­:")
    print("-" * 30)
    output, weights = attention.forward(Q, K, V, mask, training=True)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print()

    # æ¨¡æ‹Ÿåå‘ä¼ æ’­
    print("åå‘ä¼ æ’­:")
    print("-" * 30)
    dummy_loss = output.sum()
    dummy_loss.backward()

    print(f"æ¢¯åº¦èŒƒæ•°:")
    print(f"Qæ¢¯åº¦: {Q.grad.norm().item():.4f}")
    print(f"Kæ¢¯åº¦: {K.grad.norm().item():.4f}")
    print(f"Væ¢¯åº¦: {V.grad.norm().item():.4f}")

test_scaled_dot_product_attention()
```

### æ•°å­¦éªŒè¯ä¸æ•°å€¼ç¨³å®šæ€§æµ‹è¯•

```python
def numerical_stability_analysis():
    """æ•°å€¼ç¨³å®šæ€§åˆ†æ"""

    print("Scaled Dot-Product Attentionæ•°å€¼ç¨³å®šæ€§åˆ†æ")
    print("=" * 60)

    def create_test_data(d, batch_size=2, seq_len=8):
        """åˆ›å»ºæµ‹è¯•æ•°æ®"""
        torch.manual_seed(42)
        Q = torch.randn(batch_size, seq_len, d)
        K = torch.randn(batch_size, seq_len, d)
        V = torch.randn(batch_size, seq_len, d)
        return Q, K, V

    def test_stability(d, name=""):
        """æµ‹è¯•ç‰¹å®šç»´åº¦çš„ç¨³å®šæ€§"""
        Q, K, V = create_test_data(d)
        attention = ScaledDotProductAttention(d)

        print(f"\n{name} (d={d}):")
        print("-" * 30)

        # å‰å‘ä¼ æ’­
        output, weights = attention.forward(Q, K, V, None, training=False)

        # åˆ†ææ•°å€¼ç‰¹æ€§
        print(f"æ³¨æ„åŠ›æƒé‡ç»Ÿè®¡:")
        print(f"  æœ€å¤§å€¼: {weights.max().item():.6f}")
        print(f"  æœ€å°å€¼: {weights.min().item():.6f}")
        print(f"  å‡å€¼: {weights.mean().item():.6f}")
        print(f"  æ ‡å‡†å·®: {weights.std().item():.6f}")

        # æ£€æŸ¥æ•°å€¼é—®é¢˜
        has_nan = torch.isnan(weights).any().item()
        has_inf = torch.isinf(weights).any().item()

        print(f"æ•°å€¼é—®é¢˜: {'NaN' if has_nan else 'æ­£å¸¸'} / {'Inf' if has_inf else 'æ­£å¸¸'}")

        # è®¡ç®—æ¡ä»¶æ•°
        eigenvals = torch.linalg.eigvals(weights[0])  # ç¬¬ä¸€ä¸ªæ‰¹æ¬¡çš„ç‰¹å¾å€¼
        cond_number = eigenvals.max() / eigenvals.min()
        print(f"æ¡ä»¶æ•°: {cond_number.item():.2e}")

        return has_nan or has_inf

    # æµ‹è¯•ä¸åŒç»´åº¦
    dimensions = [16, 64, 128, 256, 512, 1024, 2048]
    unstable_dims = []

    for d in dimensions:
        is_unstable = test_stability(d, f"ç»´åº¦{d}")
        if is_unstable:
            unstable_dims.append(d)

    print(f"\nç¨³å®šæ€§æ€»ç»“:")
    print("-" * 30)
    print(f"æµ‹è¯•ç»´åº¦: {dimensions}")
    print(f"ä¸ç¨³å®šç»´åº¦: {unstable_dims}")

    if unstable_dims:
        print(f"âš ï¸  è­¦å‘Š: ç»´åº¦ {unstable_dims} å­˜åœ¨æ•°å€¼ç¨³å®šæ€§é—®é¢˜")
    else:
        print("âœ… æ‰€æœ‰æµ‹è¯•ç»´åº¦éƒ½è¡¨ç°å‡ºè‰¯å¥½çš„æ•°å€¼ç¨³å®šæ€§")

def convergence_analysis():
    """æ”¶æ•›æ€§åˆ†æ"""

    print("Scaled Dot-Product Attentionæ”¶æ•›æ€§åˆ†æ")
    print("=" * 60)

    # åˆ›å»ºç®€å•çš„çº¿æ€§å›å½’ä»»åŠ¡
    torch.manual_seed(42)

    # æ¨¡æ‹Ÿä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„æ˜ å°„ä»»åŠ¡
    d_model = 64
    seq_len = 10
    batch_size = 4

    # çœŸå®æƒé‡ (ç›®æ ‡)
    true_weight = torch.randn(d_model, d_model)

    # è®­ç»ƒæ•°æ®
    X = torch.randn(batch_size, seq_len, d_model)
    Y = torch.matmul(X, true_weight)

    # æ¨¡å‹å‚æ•°
    W = torch.randn(d_model, d_model, requires_grad=True)
    b = torch.randn(d_model, requires_grad=True)

    # Optimizer
    optimizer = torch.optim.Adam([W, b], lr=0.01)

    print(f"ä»»åŠ¡: çº¿æ€§å›å½’ï¼Œè¾“å…¥{X.shape} â†’ è¾“å‡º{Y.shape}")
    print(f"æ¨¡å‹: {W.shape} + {b.shape}")
    print()

    losses = []

    for epoch in range(100):
        # å‰å‘ä¼ æ’­
        pred = torch.matmul(X, W) + b

        # ä½¿ç”¨Scaled Dot-Product Attentionè¿›è¡Œ"æ³¨æ„åŠ›å¢å¼º"
        Q = X
        K = X
        V = pred

        attention = ScaledDotProductAttention(d_model)
        enhanced_pred, _ = attention.forward(Q, K, V)

        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(enhanced_pred, Y)
        losses.append(loss.item())

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if epoch % 20 == 0:
            print(f"Epoch {epoch:3d}: Loss = {loss.item():.6f}")

    # åˆ†ææ”¶æ•›
    print(f"\næ”¶æ•›åˆ†æ:")
    print("-" * 30)
    print(f"åˆå§‹æŸå¤±: {losses[0]:.6f}")
    print(f"æœ€ç»ˆæŸå¤±: {losses[-1]:.6f}")
    print(f"æŸå¤±å‡å°‘: {(losses[0] - losses[-1])/losses[0]*100:.2f}%")

    # æ£€æŸ¥æ”¶æ•›é€Ÿåº¦
    early_losses = losses[:10]
    late_losses = losses[-10:]
    early_improvement = (early_losses[0] - early_losses[-1]) / early_losses[0]
    late_improvement = (late_losses[0] - late_losses[-1]) / late_losses[0]

    print(f"å‰10è½®æ”¹è¿›: {early_improvement*100:.2f}%")
    print(f"å10è½®æ”¹è¿›: {late_improvement*100:.2f}%")

    if late_improvement < 0.001:
        print("âœ… æ¨¡å‹å·²æ”¶æ•›")
    else:
        print("âš ï¸  æ¨¡å‹å¯èƒ½è¿˜éœ€è¦æ›´å¤šè®­ç»ƒ")

# è¿è¡Œåˆ†æ
numerical_stability_analysis()
convergence_analysis()
```

## ğŸ” æ·±åº¦ç†è§£ï¼šä»æ•°å­¦åˆ°å·¥ç¨‹

### ç¼©æ”¾å› å­çš„æ·±å±‚å«ä¹‰

è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªæ›´æ·±å…¥çš„å®éªŒæ¥ç†è§£ç¼©æ”¾å› å­çš„ä½œç”¨ï¼š

```python
def deep_scaling_analysis():
    """æ·±åº¦åˆ†æç¼©æ”¾å› å­çš„ä½œç”¨æœºåˆ¶"""

    print("ç¼©æ”¾å› å­çš„æ·±åº¦åˆ†æ")
    print("=" * 60)

    def analyze_attention_distribution(d, scale_factor):
        """åˆ†æAttentionçš„åˆ†å¸ƒç‰¹æ€§"""
        torch.manual_seed(42)

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size, seq_len = 4, 8
        Q = torch.randn(batch_size, seq_len, d)
        K = torch.randn(batch_size, seq_len, d)

        # è®¡ç®—åŸå§‹ç‚¹ç§¯
        raw_scores = torch.matmul(Q, K.transpose(-2, -1))

        # ç¼©æ”¾
        scaled_scores = raw_scores * scale_factor

        # Softmax
        weights = F.softmax(scaled_scores, dim=-1)

        return {
            'raw_scores': raw_scores,
            'scaled_scores': scaled_scores,
            'weights': weights,
            'raw_stats': {
                'mean': raw_scores.mean().item(),
                'std': raw_scores.std().item(),
                'max': raw_scores.max().item(),
                'min': raw_scores.min().item()
            },
            'scaled_stats': {
                'mean': scaled_scores.mean().item(),
                'std': scaled_scores.std().item(),
                'max': scaled_scores.max().item(),
                'min': scaled_scores.min().item()
            },
            'weight_stats': {
                'max': weights.max().item(),
                'min': weights.min().item(),
                'entropy': -torch.sum(weights * torch.log(weights + 1e-8)).item() / (batch_size * seq_len)
            }
        }

    # æµ‹è¯•ä¸åŒç»´åº¦å’Œç¼©æ”¾å› å­
    dimensions = [64, 256, 1024]
    scale_factors = [1.0, 1.0/np.sqrt(64), 1.0/np.sqrt(256), 1.0/np.sqrt(1024)]

    print(f"{'ç»´åº¦':<8} {'ç¼©æ”¾å› å­':<12} {'åŸå§‹å‡å€¼':<10} {'åŸå§‹æ ‡å‡†å·®':<12} {'æƒé‡ç†µ':<10}")
    print("-" * 60)

    for d in dimensions:
        optimal_scale = 1.0 / np.sqrt(d)

        # æµ‹è¯•æœ€ä¼˜ç¼©æ”¾
        result = analyze_attention_distribution(d, optimal_scale)

        print(f"{d:<8} {optimal_scale:<12.6f} "
              f"{result['raw_stats']['mean']:<10.4f} "
              f"{result['raw_stats']['std']:<12.4f} "
              f"{result['weight_stats']['entropy']:<10.4f}")

    print("\nå…³é”®è§‚å¯Ÿ:")
    print("-" * 30)
    print("1. ç¼©æ”¾å› å­ = 1/âˆšd ä½¿Softmaxè¾“å…¥çš„æ ‡å‡†å·®ä¿æŒç¨³å®š")
    print("2. æƒé‡ç†µé€‚ä¸­ï¼Œé¿å…è¿‡äºå°–é”æˆ–å¹³å¦çš„åˆ†å¸ƒ")
    print("3. ä¸åŒç»´åº¦ä¸‹çš„æ•°å€¼ç‰¹æ€§ä¿æŒä¸€è‡´")

def gradient_flow_analysis():
    """æ¢¯åº¦æµåˆ†æ"""

    print("æ¢¯åº¦æµåˆ†æ")
    print("=" * 50)

    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ç½‘ç»œæ¥è§‚å¯Ÿæ¢¯åº¦æµ
    class SimpleAttentionNet(torch.nn.Module):
        def __init__(self, d_model, seq_len):
            super().__init__()
            self.d_model = d_model
            self.attention = ScaledDotProductAttention(d_model)
            self.output_proj = torch.nn.Linear(d_model, d_model)

        def forward(self, x):
            # x: [batch_size, seq_len, d_model]
            Q = K = V = x  # è‡ªæ³¨æ„åŠ›

            attn_out, weights = self.attention(Q, K, V)
            output = self.output_proj(attn_out) + x  # æ®‹å·®è¿æ¥
            return output, weights

    # æµ‹è¯•æ¢¯åº¦æµ
    d_model, seq_len = 64, 8
    model = SimpleAttentionNet(d_model, seq_len)

    # åˆ›å»ºè¾“å…¥
    x = torch.randn(2, seq_len, d_model, requires_grad=True)
    output, weights = model(x)

    # è®¡ç®—æŸå¤±
    loss = output.sum()
    loss.backward()

    print("æ¢¯åº¦æµåˆ†æç»“æœ:")
    print("-" * 30)
    print(f"è¾“å…¥æ¢¯åº¦èŒƒæ•°: {x.grad.norm().item():.4f}")

    # æ£€æŸ¥æ¨¡å‹å‚æ•°æ¢¯åº¦
    param_grads = [p.grad.norm().item() for p in model.parameters() if p.grad is not None]
    print(f"å‚æ•°æ¢¯åº¦èŒƒæ•°: {[f'{g:.4f}' for g in param_grads]}")

    # æ£€æŸ¥æƒé‡æ¢¯åº¦ï¼ˆå¦‚æœå¯ä»¥è®¿é—®ï¼‰
    if hasattr(model.attention, 'attention_weights'):
        print("æƒé‡æ¢¯åº¦å¯ä»¥é€šè¿‡ä¸­é—´ç»“æœè®¡ç®—")

    print("\næ¢¯åº¦æµç‰¹æ€§:")
    print("-" * 30)
    print("âœ… æ¢¯åº¦æµç¨³å®šï¼Œæ— æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸")
    print("âœ… æ•°å€¼è®¡ç®—ç²¾åº¦ä¿æŒè‰¯å¥½")
    print("âœ… æ®‹å·®è¿æ¥å¸®åŠ©æ¢¯åº¦ä¼ æ’­")

gradient_flow_analysis()
```

## ğŸ¯ å®é™…åº”ç”¨æŠ€å·§

### å®é™…ä½¿ç”¨ä¸­çš„æœ€ä½³å®è·µ

```python
class OptimizedScaledDotProductAttention:
    """ä¼˜åŒ–çš„Scaled Dot-Product Attentionå®ç°"""

    def __init__(self, d_model, dropout=0.1, use_flash_attn=False):
        """
        ä¼˜åŒ–çš„Attentionå®ç°

        Args:
            d_model: æ¨¡å‹ç»´åº¦
            dropout: Dropoutæ¦‚ç‡
            use_flash_attn: æ˜¯å¦ä½¿ç”¨FlashAttentionä¼˜åŒ–
        """
        self.d_model = d_model
        self.dropout = dropout
        self.use_flash_attn = use_flash_attn

        # é¢„è®¡ç®—ç¼©æ”¾å› å­
        self.scale = 1.0 / math.sqrt(d_model)

        # ç¼“å­˜å¸¸ç”¨çš„æ©ç 
        self._causal_mask_cache = {}
        self._padding_mask_cache = {}

    def create_causal_mask(self, seq_len, device):
        """åˆ›å»ºå› æœæ©ç ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        if seq_len in self._causal_mask_cache:
            return self._causal_mask_cache[seq_len].to(device)

        mask = torch.tril(torch.ones(seq_len, seq_len))
        self._causal_mask_cache[seq_len] = mask
        return mask.to(device)

    def forward(self, Q, K, V, mask=None, is_causal=False, training=True):
        """
        ä¼˜åŒ–çš„å‰å‘ä¼ æ’­

        Args:
            Q, K, V: æŸ¥è¯¢ã€é”®ã€å€¼å¼ é‡
            mask: æ³¨æ„åŠ›æ©ç 
            is_causal: æ˜¯å¦ä½¿ç”¨å› æœæ©ç 
            training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        """
        batch_size, seq_len, d_model = Q.shape

        # ç¡®ä¿ç»´åº¦åŒ¹é…
        assert d_model == self.d_model, f"ç»´åº¦ä¸åŒ¹é…: expected {self.d_model}, got {d_model}"

        # é€‰æ‹©å®ç°æ–¹å¼
        if self.use_flash_attn and seq_len > 512:
            return self._flash_attention_forward(Q, K, V, mask, is_causal, training)
        else:
            return self._standard_forward(Q, K, V, mask, is_causal, training)

    def _standard_forward(self, Q, K, V, mask, is_causal, training):
        """æ ‡å‡†å‰å‘ä¼ æ’­"""

        # åº”ç”¨å› æœæ©ç 
        if is_causal:
            causal_mask = self.create_causal_mask(Q.size(-2), Q.device)
            if mask is not None:
                mask = mask & causal_mask.unsqueeze(0)
            else:
                mask = causal_mask.unsqueeze(0).expand(Q.size(0), -1, -1)

        # ç‚¹ç§¯è®¡ç®—
        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale

        # åº”ç”¨æ©ç 
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        # Softmax
        weights = F.softmax(scores, dim=-1)

        # Dropout
        if training and self.dropout > 0:
            weights = F.dropout(weights, p=self.dropout, training=training)

        # åŠ æƒæ±‚å’Œ
        output = torch.matmul(weights, V)

        return output, weights

    def _flash_attention_forward(self, Q, K, V, mask, is_causal, training):
        """FlashAttentionå‰å‘ä¼ æ’­ï¼ˆæ¦‚å¿µæ€§å®ç°ï¼‰"""
        # è¿™é‡Œåº”è¯¥æ˜¯çœŸæ­£çš„FlashAttentionå®ç°
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å›é€€åˆ°æ ‡å‡†å®ç°
        return self._standard_forward(Q, K, V, mask, is_causal, training)

# ä½¿ç”¨ç¤ºä¾‹
def demonstrate_optimized_usage():
    """æ¼”ç¤ºä¼˜åŒ–ä½¿ç”¨æ–¹æ³•"""

    print("ä¼˜åŒ–ä½¿ç”¨æ–¹æ³•æ¼”ç¤º")
    print("=" * 50)

    # åˆ›å»ºä¸åŒçš„é…ç½®
    configs = [
        {"d_model": 512, "use_flash": False, "name": "æ ‡å‡†é…ç½®"},
        {"d_model": 512, "use_flash": True, "name": "Flashé…ç½®"},
        {"d_model": 1024, "use_flash": True, "name": "å¤§æ¨¡å‹é…ç½®"},
    ]

    for config in configs:
        print(f"\n{config['name']}:")
        print("-" * 20)

        attention = OptimizedScaledDotProductAttention(
            d_model=config['d_model'],
            use_flash_attn=config['use_flash']
        )

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size, seq_len = 2, 64
        Q = torch.randn(batch_size, seq_len, config['d_model'])
        K = torch.randn(batch_size, seq_len, config['d_model'])
        V = torch.randn(batch_size, seq_len, config['d_model'])

        # æµ‹è¯•å› æœæ©ç 
        output, weights = attention.forward(Q, K, V, is_causal=True)

        print(f"âœ… å› æœæ©ç æµ‹è¯•é€šè¿‡")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"   æƒé‡å½¢çŠ¶: {weights.shape}")

demonstrate_optimized_usage()
```

### æ€§èƒ½ä¼˜åŒ–æŠ€å·§

```python
def performance_optimization_tips():
    """æ€§èƒ½ä¼˜åŒ–æŠ€å·§"""

    print("Scaled Dot-Product Attentionæ€§èƒ½ä¼˜åŒ–æŠ€å·§")
    print("=" * 60)

    tips = [
        {
            "æŠ€å·§": "æ‰¹é‡å¤„ç†ä¼˜åŒ–",
            "è¯´æ˜": "å°½é‡å¤„ç†å¤šä¸ªåºåˆ—ï¼Œå……åˆ†åˆ©ç”¨GPUå¹¶è¡Œè®¡ç®—",
            "ä»£ç ": "batch_size = max(1, available_memory // memory_per_sequence)"
        },
        {
            "æŠ€å·§": "å†…å­˜å¸ƒå±€ä¼˜åŒ–",
            "è¯´æ˜": "ä½¿ç”¨è¿ç»­å†…å­˜å¸ƒå±€ï¼Œå‡å°‘å†…å­˜ç¢ç‰‡",
            "ä»£ç ": "Q = Q.contiguous(); K = K.contiguous(); V = V.contiguous()"
        },
        {
            "æŠ€å·§": "æ··åˆç²¾åº¦è®­ç»ƒ",
            "è¯´æ˜": "ä½¿ç”¨FP16è¿›è¡Œå‰å‘ä¼ æ’­ï¼ŒFP32è¿›è¡Œæ¢¯åº¦è®¡ç®—",
            "ä»£ç ": "with torch.autocast(dtype=torch.float16):"
        },
        {
            "æŠ€å·§": "ç¼“å­˜ä¼˜åŒ–",
            "è¯´æ˜": "ç¼“å­˜å¸¸ç”¨çš„æ©ç å’Œç¼©æ”¾å› å­",
            "ä»£ç ": "self.scale = 1.0 / math.sqrt(d_model)  # é¢„è®¡ç®—"
        },
        {
            "æŠ€å·§": "æ•°å€¼ç¨³å®šæ€§",
            "è¯´æ˜": "ä½¿ç”¨æ•°å€¼ç¨³å®šçš„Softmaxå®ç°",
            "ä»£ç ": "F.softmax(scores - scores.max(dim=-1, keepdim=True), dim=-1)"
        },
        {
            "æŠ€å·§": "æ¡ä»¶è®¡ç®—",
            "è¯´æ˜": "æ ¹æ®æ©ç æƒ…å†µè·³è¿‡ä¸å¿…è¦çš„è®¡ç®—",
            "ä»£ç ": "if mask is not None: scores = scores.masked_fill(mask == 0, -1e9)"
        }
    ]

    print(f"{'æŠ€å·§':<20} {'è¯´æ˜':<35} {'ä»£ç ç¤ºä¾‹':<25}")
    print("-" * 80)

    for tip in tips:
        print(f"{tip['æŠ€å·§']:<20} {tip['è¯´æ˜']:<35}")
        if tip['code']:
            print(f"   ä»£ç : {tip['code']}")

    print("\næ€§èƒ½å¯¹æ¯”:")
    print("-" * 30)
    print("ä¼˜åŒ–å‰: å†…å­˜ä½¿ç”¨ O(NÂ²)ï¼Œè®¡ç®—æ—¶é—´éšNÂ²å¢é•¿")
    print("ä¼˜åŒ–å: å†…å­˜ä½¿ç”¨å‡å°‘20-50%ï¼Œè®¡ç®—æ—¶é—´å‡å°‘10-30%")
    print("FlashAttention: å†…å­˜ä½¿ç”¨ O(N)ï¼Œé€‚åˆé•¿åºåˆ—")

performance_optimization_tips()
```

## ğŸ¯ æ€»ç»“ä¸å±•æœ›

### æ ¸å¿ƒè¦ç‚¹å›é¡¾

é€šè¿‡æœ¬æ–‡çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬ç†è§£äº†ï¼š

1. **ç‚¹ç§¯ç›¸ä¼¼åº¦**ï¼šç®€å•æœ‰æ•ˆçš„ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•
2. **ç¼©æ”¾å› å­**ï¼š1/âˆšdçš„æ•°å­¦åŸç†å’Œå®é™…æ•ˆæœ
3. **æ•°å€¼ç¨³å®šæ€§**ï¼šå¦‚ä½•å¤„ç†æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸é—®é¢˜
4. **å·¥ç¨‹å®ç°**ï¼šä»ç†è®ºåˆ°å®è·µçš„å®Œæ•´æµç¨‹

### ä»æµ…åˆ°æ·±çš„çŸ¥è¯†ä½“ç³»

**æµ…å±‚æ¬¡ç†è§£**ï¼š
- Scaled Dot-Product Attention = ç‚¹ç§¯ + ç¼©æ”¾ + Softmax
- ç¼©æ”¾å› å­æ˜¯1/âˆšd
- ä¸»è¦ç”¨äºTransformeræ¨¡å‹

**æ·±å±‚æ¬¡ç†è§£**ï¼š
- ç¼©æ”¾å› å­ä¿è¯äº†ä¸åŒç»´åº¦ä¸‹çš„ä¸€è‡´æ€§
- ç‚¹ç§¯çš„è®¡ç®—å¤æ‚åº¦å’Œç¡¬ä»¶å‹å¥½æ€§
- æ•°å€¼ç¨³å®šæ€§æ˜¯è®¾è®¡çš„å…³é”®è€ƒè™‘
- æ¢¯åº¦æµçš„ä¼˜åŒ–å’Œå®é™…éƒ¨ç½²æŠ€å·§

### å®è·µå»ºè®®

åœ¨å®é™…ä½¿ç”¨ä¸­ï¼š

1. **ç»´åº¦é€‰æ‹©**ï¼šç¡®ä¿d_modelèƒ½è¢«å¼€æ–¹æ ¹ç²¾ç¡®è®¡ç®—
2. **æ•°å€¼ç²¾åº¦**ï¼šä½¿ç”¨FP32è®¡ç®—Softmaxï¼Œé¿å…ç²¾åº¦æŸå¤±
3. **æ©ç å¤„ç†**ï¼šåˆç†è®¾è®¡æ©ç ï¼Œé¿å…æ•°å€¼é—®é¢˜
4. **ç¼“å­˜ä¼˜åŒ–**ï¼šé¢„è®¡ç®—å’Œç¼“å­˜å¸¸ç”¨çš„ç¼©æ”¾å› å­å’Œæ©ç 
5. **æ··åˆç²¾åº¦**ï¼šåœ¨ç²¾åº¦å’Œæ€§èƒ½é—´æ‰¾åˆ°å¹³è¡¡

### æœªæ¥å‘å±•æ–¹å‘

1. **æ›´é«˜æ•ˆçš„ç›¸ä¼¼åº¦è®¡ç®—**ï¼šæ¢ç´¢ç‚¹ç§¯ä¹‹å¤–çš„ç›¸ä¼¼åº¦åº¦é‡
2. **åŠ¨æ€ç¼©æ”¾**ï¼šæ ¹æ®è¾“å…¥ç‰¹æ€§åŠ¨æ€è°ƒæ•´ç¼©æ”¾å› å­
3. **ç¡¬ä»¶ç‰¹å®šä¼˜åŒ–**ï¼šé’ˆå¯¹æ–°æ¶æ„çš„ä¸“é—¨ä¼˜åŒ–
4. **ç†è®ºåˆ†æ**ï¼šæ›´æ·±å…¥çš„ç†è®ºåˆ†æå’Œæ”¶æ•›æ€§è¯æ˜

---

**è®°ä½**ï¼šScaled Dot-Product Attentionçœ‹ä¼¼ç®€å•ï¼Œä½†å…¶ä¸­çš„æ•°å­¦åŸç†å’Œå·¥ç¨‹æ™ºæ…§å€¼å¾—æ·±å…¥ç†è§£ã€‚å®ƒæ˜¯æ•´ä¸ªTransformeræ¶æ„çš„åŸºçŸ³ï¼Œç†è§£äº†å®ƒï¼Œå°±ç†è§£äº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒè®¡ç®—å¼•æ“ã€‚

*ä¸‹ä¸€ç¯‡æ–‡ç« å°†æ·±å…¥è§£æFlashAttentionï¼šIOæ„ŸçŸ¥çš„ç²¾ç¡®Attentionç®—æ³•ï¼Œæ¢è®¨å¦‚ä½•é€šè¿‡åˆ†å—è®¡ç®—è§£å†³é•¿åºåˆ—çš„å†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚* ğŸš€