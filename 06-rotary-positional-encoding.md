# RoPEä½ç½®ç¼–ç ï¼šæ—‹è½¬ä½ç½®ç¼–ç çš„æ·±åº¦è§£æ

## ğŸ¯ å¼•è¨€ï¼šä½ç½®ç¼–ç çš„æ¼”è¿›ä¹‹è·¯

åœ¨å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•å†ç¨‹ä¸­ï¼Œå¦‚ä½•è®©æ¨¡å‹ç†è§£åºåˆ—ä¸­tokençš„ä½ç½®å…³ç³»ä¸€ç›´æ˜¯ä¸€ä¸ªæ ¸å¿ƒæŒ‘æˆ˜ã€‚ä»æœ€åˆçš„æ­£å¼¦ä½ç½®ç¼–ç åˆ°å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼Œå†åˆ°ä»Šå¤©çš„RoPEï¼ˆRotary Positional Encodingï¼‰ï¼Œä½ç½®ç¼–ç æŠ€æœ¯ç»å†äº†å¤šæ¬¡é©å‘½æ€§çš„æ¼”è¿›ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œå½“ä½ é˜…è¯»"è‹¹æœå…¬å¸å‘å¸ƒäº†æ–°äº§å“"è¿™å¥è¯æ—¶ï¼Œä½ éœ€è¦ç†è§£"è‹¹æœ"åœ¨å¥é¦–æ˜¯ä¸»è¯­ï¼Œ"äº§å“"åœ¨å¥æœ«æ˜¯å®¾è¯­ã€‚è¿™ç§ä½ç½®å…³ç³»å¯¹äºç†è§£å¥å­å«ä¹‰è‡³å…³é‡è¦ã€‚RoPEæ­£æ˜¯é€šè¿‡æ—‹è½¬æ“ä½œï¼Œå°†ä½ç½®ä¿¡æ¯ä¼˜é›…åœ°"åµŒå…¥"åˆ°tokençš„è¯­ä¹‰è¡¨ç¤ºä¸­ã€‚

æœ¬æ–‡å°†æ·±å…¥å‰–æRoPEçš„è®¾è®¡å“²å­¦ã€æ•°å­¦åŸç†ã€å·¥ç¨‹å®ç°ä»¥åŠä¼˜åŒ–ç­–ç•¥ï¼Œè®©ä½ å…¨é¢ç†è§£è¿™é¡¹åœ¨Transformeræ¶æ„ä¸­æ‰®æ¼”å…³é”®è§’è‰²çš„æŠ€æœ¯ã€‚

## ğŸ§  ä½ç½®ç¼–ç çš„åŸºç¡€ç†è®º

### ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ

åœ¨æœ€åˆçš„Transformeræ¶æ„ä¸­ï¼ŒSelf-Attentionæœºåˆ¶æœ¬èº«æ˜¯ä¸æ„ŸçŸ¥ä½ç½®çš„ã€‚è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¾‹å­ç†è§£è¿™ä¸ªé—®é¢˜ï¼š

```python
def demonstrate_positional_ambiguity():
    """æ¼”ç¤ºä½ç½®ç¼–ç çš„å¿…è¦æ€§"""

    # ç¤ºä¾‹å¥å­
    sentence1 = "çŒ«è¿½è€é¼ "
    sentence2 = "è€é¼ è¿½çŒ«"

    # å‡è®¾çš„è¯å‘é‡ï¼ˆç®€åŒ–è¡¨ç¤ºï¼‰
    token_embeddings = {
        "çŒ«": torch.tensor([1.0, 0.5]),
        "è¿½": torch.tensor([0.8, 0.9]),
        "è€é¼ ": torch.tensor([0.6, 0.7])
    }

    # è®¡ç®—Attentionï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    def simple_attention(q, k, v):
        scores = torch.matmul(q, k.T)
        weights = F.softmax(scores, dim=-1)
        return torch.matmul(weights, v)

    print("=== ä½ç½®ç¼–ç çš„å¿…è¦æ€§æ¼”ç¤º ===")
    print("å¥å­1: çŒ«è¿½è€é¼ ")
    print("å¥å­2: è€é¼ è¿½çŒ«")
    print()

    # æ²¡æœ‰ä½ç½®ç¼–ç çš„æƒ…å†µ
    tokens1 = ["çŒ«", "è¿½", "è€é¼ "]
    tokens2 = ["è€é¼ ", "è¿½", "çŒ«"]

    embeddings1 = torch.stack([token_embeddings[t] for t in tokens1])
    embeddings2 = torch.stack([token_embeddings[t] for t in tokens2])

    # æ³¨æ„ï¼šä¸¤ä¸ªå¥å­çš„è¯å‘é‡é›†åˆç›¸åŒï¼Œåªæ˜¯é¡ºåºä¸åŒ
    print("æ— ä½ç½®ç¼–ç æ—¶ï¼Œä¸¤ä¸ªå¥å­çš„è¯å‘é‡é›†åˆç›¸åŒ:")
    print(f"å¥å­1å‘é‡é›†åˆ: {embeddings1.tolist()}")
    print(f"å¥å­2å‘é‡é›†åˆ: {embeddings2.tolist()}")
    print("è¿™ä¼šå¯¼è‡´Attentionæ— æ³•åŒºåˆ†è¯åºï¼")
    print()

demonstrate_positional_ambiguity()
```

### ä¼ ç»Ÿä½ç½®ç¼–ç çš„å±€é™æ€§

#### 1. ç»å¯¹ä½ç½®ç¼–ç ï¼ˆLearned Positional Embeddingsï¼‰

```python
class AbsolutePositionalEncoding(nn.Module):
    """ç»å¯¹ä½ç½®ç¼–ç """

    def __init__(self, max_seq_len, d_model):
        super().__init__()
        self.position_embeddings = nn.Embedding(max_seq_len, d_model)

    def forward(self, x):
        batch_size, seq_len, d_model = x.shape
        position_ids = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)
        return x + self.position_embeddings(position_ids)

# ç»å¯¹ä½ç½®ç¼–ç çš„é—®é¢˜åˆ†æ
def analyze_absolute_positioning_limitations():
    """åˆ†æç»å¯¹ä½ç½®ç¼–ç çš„å±€é™æ€§"""

    print("=== ç»å¯¹ä½ç½®ç¼–ç çš„å±€é™æ€§ ===")
    print("1. å¤–æ¨æ€§å·®ï¼šæ— æ³•å¤„ç†è¶…è¿‡è®­ç»ƒé•¿åº¦çš„åºåˆ—")
    print("2. ç›¸å¯¹ä½ç½®ä¿¡æ¯ä¸¢å¤±ï¼šéš¾ä»¥æ•æ‰tokené—´çš„ç›¸å¯¹å…³ç³»")
    print("3. å›ºå®šæ¨¡å¼ï¼šæ— æ³•é€‚åº”ä¸åŒä»»åŠ¡çš„ç‰¹æ®Šä½ç½®éœ€æ±‚")
    print("4. å‚æ•°å¼€é”€ï¼šéœ€è¦é¢å¤–å­¦ä¹ ä½ç½®å‚æ•°")

    # å¤–æ¨æ€§é—®é¢˜æ¼”ç¤º
    max_train_len = 512
    inference_len = 1024

    print(f"\nå¤–æ¨æ€§é—®é¢˜ç¤ºä¾‹:")
    print(f"è®­ç»ƒæœ€å¤§é•¿åº¦: {max_train_len}")
    print(f"æ¨ç†éœ€è¦é•¿åº¦: {inference_len}")
    print(f"å·®è·: {inference_len - max_train_len} ä¸ªä½ç½®æ²¡æœ‰è§è¿‡è®­ç»ƒæ•°æ®")

analyze_absolute_positioning_limitations()
```

#### 2. æ­£å¼¦ä½ç½®ç¼–ç ï¼ˆSinusoidal Positional Encodingï¼‰

```python
class SinusoidalPositionalEncoding(nn.Module):
    """æ­£å¼¦ä½ç½®ç¼–ç  - TransformeråŸå§‹è®¾è®¡"""

    def __init__(self, d_model, max_seq_len=5000):
        super().__init__()
        pe = torch.zeros(max_seq_len, d_model)
        position = torch.arange(0, max_seq_len).unsqueeze(1).float()

        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           -(math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

# æ­£å¼¦ä½ç½®ç¼–ç çš„é—®é¢˜
def analyze_sinusoidal_limitations():
    """åˆ†ææ­£å¼¦ä½ç½®ç¼–ç çš„å±€é™æ€§"""

    print("\n=== æ­£å¼¦ä½ç½®ç¼–ç çš„å±€é™æ€§ ===")
    print("1. å›ºå®šé¢‘ç‡æ¨¡å¼ï¼šå¯èƒ½ä¸é€‚åˆæ‰€æœ‰ä»»åŠ¡")
    print("2. åŠ æ³•å¹²æ‰°ï¼šç›´æ¥åŠ æ³•å¯èƒ½å½±å“åŸå§‹è¯­ä¹‰")
    print("3. ç›¸å¯¹ä½ç½®è®¡ç®—å¤æ‚ï¼šéœ€è¦é¢å¤–çš„ç›¸å¯¹ä½ç½®è®¡ç®—")
    print("4. ç»´åº¦è€¦åˆï¼šä¸åŒé¢‘ç‡ç»´åº¦çš„è€¦åˆé™åˆ¶äº†è¡¨è¾¾èƒ½åŠ›")

analyze_sinusoidal_limitations()
```

## ğŸ”„ RoPEçš„æ ¸å¿ƒæ€æƒ³ï¼šæ—‹è½¬çš„ä½ç½®ç¼–ç 

### RoPEçš„è®¾è®¡å“²å­¦

RoPEçš„æ ¸å¿ƒç†å¿µæ˜¯ï¼š**é€šè¿‡æ—‹è½¬å‘é‡æ¥ç¼–ç ä½ç½®ä¿¡æ¯**ã€‚è¿™ç§æ–¹æ³•å·§å¦™åœ°å°†ä½ç½®ä¿¡æ¯"èå…¥"åˆ°å‘é‡ç©ºé—´ä¸­ï¼Œè€Œä¸æ˜¯ç®€å•åœ°"åŠ "ä¸Šå»ã€‚

```python
def rope_intuition_demo():
    """RoPEçš„ç›´è§‚ç†è§£æ¼”ç¤º"""

    print("=== RoPEæ ¸å¿ƒæ€æƒ³æ¼”ç¤º ===")
    print()
    print("ä¼ ç»Ÿæ–¹æ³•ï¼ˆåŠ æ³•ï¼‰:")
    print("  token_vec + position_vec")
    print("  é—®é¢˜ï¼šä½ç½®ä¿¡æ¯ä¸è¯­ä¹‰ä¿¡æ¯åˆ†ç¦»")
    print()
    print("RoPEæ–¹æ³•ï¼ˆæ—‹è½¬ï¼‰:")
    print("  rotate(token_vec, position_angle)")
    print("  ä¼˜åŠ¿ï¼šä½ç½®ä¿¡æ¯ä¸è¯­ä¹‰ä¿¡æ¯è‡ªç„¶èåˆ")
    print()

    # 2Dç©ºé—´ä¸­çš„æ—‹è½¬æ¼”ç¤º
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # åŸå§‹å‘é‡
    original_vec = np.array([1, 0.5])

    # ä½ç½®1ï¼ˆè§’åº¦0ï¼‰
    angle1 = 0
    rotation_matrix1 = np.array([[np.cos(angle1), -np.sin(angle1)],
                                 [np.sin(angle1), np.cos(angle1)]])
    rotated_vec1 = rotation_matrix1 @ original_vec

    # ä½ç½®2ï¼ˆè§’åº¦Ï€/4ï¼‰
    angle2 = np.pi / 4
    rotation_matrix2 = np.array([[np.cos(angle2), -np.sin(angle2)],
                                 [np.sin(angle2), np.cos(angle2)]])
    rotated_vec2 = rotation_matrix2 @ original_vec

    # ç»˜åˆ¶åŸå§‹å‘é‡å’Œæ—‹è½¬åçš„å‘é‡
    ax1.arrow(0, 0, original_vec[0], original_vec[1],
              head_width=0.1, head_length=0.1, fc='blue', ec='blue',
              label='åŸå§‹å‘é‡', linewidth=2)
    ax1.arrow(0, 0, rotated_vec1[0], rotated_vec1[1],
              head_width=0.1, head_length=0.1, fc='red', ec='red',
              label=f'ä½ç½®1 (è§’åº¦={angle1:.2f})', linewidth=2)
    ax1.arrow(0, 0, rotated_vec2[0], rotated_vec2[1],
              head_width=0.1, head_length=0.1, fc='green', ec='green',
              label=f'ä½ç½®2 (è§’åº¦={angle2:.2f})', linewidth=2)

    ax1.set_xlim(-0.5, 1.5)
    ax1.set_ylim(-0.5, 1.5)
    ax1.set_aspect('equal')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    ax1.set_title('RoPEï¼šé€šè¿‡æ—‹è½¬ç¼–ç ä½ç½®')

    # ç›¸å¯¹ä½ç½®å…³ç³»æ¼”ç¤º
    positions = [0, np.pi/6, np.pi/3, np.pi/2]
    colors = ['blue', 'green', 'orange', 'red']

    for i, (pos, color) in enumerate(zip(positions, colors)):
        rotation_matrix = np.array([[np.cos(pos), -np.sin(pos)],
                                   [np.sin(pos), np.cos(pos)]])
        rotated = rotation_matrix @ original_vec
        ax2.arrow(0, 0, rotated[0], rotated[1],
                  head_width=0.05, head_length=0.05, fc=color, ec=color,
                  label=f'ä½ç½®{i+1}', alpha=0.7)

    ax2.set_xlim(-0.5, 1.5)
    ax2.set_ylim(-0.5, 1.5)
    ax2.set_aspect('equal')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    ax2.set_title('ä¸åŒä½ç½®çš„å‘é‡è¡¨ç¤º')

    plt.tight_layout()
    plt.show()

rope_intuition_demo()
```

### RoPEçš„æ•°å­¦åŸç†

#### 1. å¤æ•°è¡¨ç¤ºæ³•

RoPEçš„æ ¸å¿ƒæ˜¯ä½¿ç”¨å¤æ•°ï¼ˆæˆ–2Då‘é‡ï¼‰çš„æ—‹è½¬æ¥ç¼–ç ä½ç½®ï¼š

```python
def rope_mathematical_derivation():
    """RoPEçš„æ•°å­¦æ¨å¯¼"""

    print("=== RoPEæ•°å­¦æ¨å¯¼ ===")
    print()
    print("1. å¤æ•°æ—‹è½¬åŸºæœ¬åŸç†:")
    print("   å¯¹äºå¤æ•° z = a + bi")
    print("   æ—‹è½¬è§’åº¦ Î¸: z' = z * e^(iÎ¸)")
    print("   z' = (a + bi) * (cos(Î¸) + i*sin(Î¸))")
    print("   z' = (a*cos(Î¸) - b*sin(Î¸)) + i*(a*sin(Î¸) + b*cos(Î¸))")
    print()

    print("2. å‘é‡æ—‹è½¬çŸ©é˜µ:")
    print("   [x'] = [cos(Î¸) -sin(Î¸)] [x]")
    print("   [y']   [sin(Î¸)  cos(Î¸)] [y]")
    print()

    print("3. å¤šç»´æƒ…å†µä¸‹çš„åˆ†ç»„æ—‹è½¬:")
    print("   å°†dç»´å‘é‡åˆ†ç»„ä¸ºd/2ä¸ª2Då‘é‡")
    print("   æ¯ç»„ä½¿ç”¨ä¸åŒçš„æ—‹è½¬é¢‘ç‡")
    print()

    # å…·ä½“çš„æ—‹è½¬é¢‘ç‡è®¡ç®—
    d_model = 512
    print(f"4. æ—‹è½¬é¢‘ç‡è®¡ç®— (d_model={d_model}):")

    for i in range(0, min(8, d_model), 2):
        freq = 1.0 / (10000 ** (i / d_model))
        print(f"   ç»´åº¦[{i}:{i+2}]: é¢‘ç‡ = {freq:.6f}")

rope_mathematical_derivation()
```

#### 2. å®Œæ•´çš„RoPEè®¡ç®—è¿‡ç¨‹

```python
class RoPEMath:
    """RoPEæ•°å­¦è®¡ç®—è¯¦è§£"""

    def __init__(self, d_model, max_seq_len=4096):
        self.d_model = d_model
        self.max_seq_len = max_seq_len

    def compute_rotation_frequencies(self):
        """è®¡ç®—æ—‹è½¬é¢‘ç‡"""
        # 10000^(2i/d_model) for i = 0, 2, 4, ..., d_model-2
        indices = torch.arange(0, self.d_model, 2, dtype=torch.float32)
        freqs = 1.0 / (10000 ** (indices / self.d_model))
        return freqs

    def compute_rotation_matrix(self, position):
        """è®¡ç®—æŒ‡å®šä½ç½®çš„æ—‹è½¬çŸ©é˜µ"""
        freqs = self.compute_rotation_frequencies()
        angles = position * freqs

        cos_vals = torch.cos(angles)
        sin_vals = torch.sin(angles)

        return cos_vals, sin_vals

    def apply_rope_2d(self, x, position):
        """åœ¨2Då¹³é¢ä¸Šåº”ç”¨RoPE"""
        cos_vals, sin_vals = self.compute_rotation_matrix(position)

        # åˆ†ç»„ä¸º2Då‘é‡
        x_2d = x.view(-1, 2)
        cos_2d = cos_vals.view(-1, 2)
        sin_2d = sin_vals.view(-1, 2)

        # åº”ç”¨æ—‹è½¬ï¼š[x*cos - y*sin, x*sin + y*cos]
        x_rot = x_2d[:, 0] * cos_2d[:, 0] - x_2d[:, 1] * sin_2d[:, 0]
        y_rot = x_2d[:, 0] * sin_2d[:, 0] + x_2d[:, 1] * cos_2d[:, 0]

        return torch.stack([x_rot, y_rot], dim=1).view_as(x)

def rope_step_by_step_demo():
    """RoPEè®¡ç®—è¿‡ç¨‹åˆ†æ­¥æ¼”ç¤º"""

    print("=== RoPEè®¡ç®—è¿‡ç¨‹æ¼”ç¤º ===")

    # è®¾ç½®å‚æ•°
    d_model = 8  # ç®€åŒ–ç»´åº¦
    position = 3
    x = torch.randn(d_model)

    print(f"è¾“å…¥å‘é‡ (ä½ç½®{position}): {x.tolist()}")
    print()

    rope_math = RoPEMath(d_model)

    # æ­¥éª¤1ï¼šè®¡ç®—æ—‹è½¬é¢‘ç‡
    freqs = rope_math.compute_rotation_frequencies()
    print(f"æ­¥éª¤1: æ—‹è½¬é¢‘ç‡")
    print(f"  é¢‘ç‡: {freqs.tolist()}")
    print()

    # æ­¥éª¤2ï¼šè®¡ç®—è§’åº¦
    angles = position * freqs
    print(f"æ­¥éª¤2: æ—‹è½¬è§’åº¦")
    print(f"  è§’åº¦: {angles.tolist()}")
    print()

    # æ­¥éª¤3ï¼šè®¡ç®—coså’Œsinå€¼
    cos_vals = torch.cos(angles)
    sin_vals = torch.sin(angles)
    print(f"æ­¥éª¤3: ä¸‰è§’å‡½æ•°å€¼")
    print(f"  cos: {cos_vals.tolist()}")
    print(f"  sin: {sin_vals.tolist()}")
    print()

    # æ­¥éª¤4ï¼šåº”ç”¨æ—‹è½¬
    result = rope_math.apply_rope_2d(x, position)
    print(f"æ­¥éª¤4: æ—‹è½¬ç»“æœ")
    print(f"  è¾“å‡º: {result.tolist()}")

rope_step_by_step_demo()
```

## ğŸ—ï¸ é«˜æ•ˆçš„RoPEå®ç°

### åŸºç¡€RoPEå®ç°

```python
class BasicRoPE(nn.Module):
    """åŸºç¡€RoPEå®ç°"""

    def __init__(self, d_model, max_seq_len=4096):
        super().__init__()
        self.d_model = d_model
        self.max_seq_len = max_seq_len

        # é¢„è®¡ç®—é¢‘ç‡
        self.register_buffer('freqs', self._compute_freqs())

    def _compute_freqs(self):
        """è®¡ç®—æ—‹è½¬é¢‘ç‡"""
        indices = torch.arange(0, self.d_model, 2, dtype=torch.float32)
        freqs = 1.0 / (10000 ** (indices / self.d_model))
        return freqs

    def forward(self, x, positions=None):
        """
        Args:
            x: [batch_size, seq_len, d_model] æˆ– [batch_size, num_heads, seq_len, head_dim]
            positions: [batch_size, seq_len] ä½ç½®ç´¢å¼•
        """
        if positions is None:
            # é»˜è®¤ä½¿ç”¨0,1,2,...,seq_len-1
            seq_len = x.shape[-2]
            positions = torch.arange(seq_len, device=x.device).unsqueeze(0)

        # è®¡ç®—æ—‹è½¬è§’åº¦
        angles = positions.unsqueeze(-1) * self.freqs  # [batch_size, seq_len, d_model/2]

        # è®¡ç®—coså’Œsin
        cos_vals = torch.cos(angles)
        sin_vals = torch.sin(angles)

        # åº”ç”¨RoPE
        return self._apply_rope(x, cos_vals, sin_vals)

    def _apply_rope(self, x, cos_vals, sin_vals):
        """åº”ç”¨RoPEæ—‹è½¬"""
        # å°†è¾“å…¥åˆ†å‰²ä¸ºå®éƒ¨å’Œè™šéƒ¨
        x_real = x[..., ::2]  # å¶æ•°ç´¢å¼•
        x_imag = x[..., 1::2]  # å¥‡æ•°ç´¢å¼•

        # åº”ç”¨æ—‹è½¬å…¬å¼
        x_rot_real = x_real * cos_vals - x_imag * sin_vals
        x_rot_imag = x_real * sin_vals + x_imag * cos_vals

        # é‡æ–°ç»„åˆ
        x_rotated = torch.cat([x_rot_real, x_rot_imag], dim=-1)

        return x_rotated

# åŸºç¡€å®ç°çš„æ€§èƒ½æµ‹è¯•
def test_basic_rope():
    """æµ‹è¯•åŸºç¡€RoPEå®ç°"""

    # é…ç½®å‚æ•°
    batch_size = 4
    seq_len = 512
    d_model = 512
    head_dim = 128

    # æµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, seq_len, d_model)
    positions = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)

    # åˆ›å»ºRoPEæ¨¡å—
    rope = BasicRoPE(d_model)

    # åº”ç”¨RoPE
    x_rotated = rope(x, positions)

    print("=== åŸºç¡€RoPEæµ‹è¯• ===")
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {x_rotated.shape}")
    print(f"æ•°å€¼èŒƒå›´ - è¾“å…¥: [{x.min():.3f}, {x.max():.3f}]")
    print(f"æ•°å€¼èŒƒå›´ - è¾“å‡º: [{x_rotated.min():.3f}, {x_rotated.max():.3f}]")
    print(f"å‘é‡èŒƒæ•°å˜åŒ– - è¾“å…¥: {x.norm(dim=-1).mean():.3f}")
    print(f"å‘é‡èŒƒæ•°å˜åŒ– - è¾“å‡º: {x_rotated.norm(dim=-1).mean():.3f}")

test_basic_rope()
```

### ä¼˜åŒ–çš„RoPEå®ç°

```python
class OptimizedRoPE(nn.Module):
    """ä¼˜åŒ–çš„RoPEå®ç° - é’ˆå¯¹å®é™…ç”Ÿäº§ç¯å¢ƒ"""

    def __init__(self, head_dim, max_seq_len=4096, device='cpu'):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len

        # é¢„è®¡ç®—coså’Œsinç¼“å­˜
        self._precompute_cos_sin_cache(max_seq_len, device)

    def _precompute_cos_sin_cache(self, max_seq_len, device):
        """é¢„è®¡ç®—coså’Œsinç¼“å­˜"""
        # è®¡ç®—é¢‘ç‡
        indices = torch.arange(0, self.head_dim, 2, dtype=torch.float32, device=device)
        freqs = 1.0 / (10000 ** (indices / self.head_dim))

        # è®¡ç®—ä½ç½®ç¼–ç 
        t = torch.arange(max_seq_len, device=device, dtype=torch.float32)
        freqs = torch.outer(t, freqs)

        # è®¡ç®—coså’Œsin
        emb = torch.cat((freqs, freqs), dim=-1)
        cos_cached = emb.cos()[None, :, None, :]  # [1, max_seq_len, 1, head_dim]
        sin_cached = emb.sin()[None, :, None, :]

        self.register_buffer('cos_cached', cos_cached)
        self.register_buffer('sin_cached', sin_cached)

    def forward(self, q, k, positions=None):
        """
        é«˜æ•ˆçš„RoPEåº”ç”¨ï¼Œä¸“é—¨é’ˆå¯¹Queryå’ŒKey

        Args:
            q: [batch_size, num_heads, seq_len, head_dim]
            k: [batch_size, num_heads, seq_len, head_dim]
            positions: å¯é€‰çš„ä½ç½®ç´¢å¼•
        """
        batch_size, num_heads, seq_len, head_dim = q.shape

        if positions is None:
            # ä½¿ç”¨ç¼“å­˜çš„coså’Œsin
            cos = self.cos_cached[:, :seq_len, :, :]
            sin = self.sin_cached[:, :seq_len, :, :]
        else:
            # åŠ¨æ€è®¡ç®—coså’Œsinï¼ˆç”¨äºéè¿ç»­ä½ç½®ï¼‰
            cos, sin = self._compute_cos_sin_dynamic(positions)

        # åº”ç”¨RoPEï¼ˆå‘é‡åŒ–æ“ä½œï¼‰
        q_rot = self._apply_rope_vectorized(q, cos, sin)
        k_rot = self._apply_rope_vectorized(k, cos, sin)

        return q_rot, k_rot

    def _apply_rope_vectorized(self, x, cos, sin):
        """å‘é‡åŒ–çš„RoPEåº”ç”¨"""
        # ä½¿ç”¨æ›´é«˜æ•ˆçš„å¼ é‡æ“ä½œ
        x2 = torch.cat([-x[..., self.head_dim//2:], x[..., :self.head_dim//2]], dim=-1)
        x2 = x2.reshape(x.shape)
        return x * cos + x2 * sin

    def _compute_cos_sin_dynamic(self, positions):
        """åŠ¨æ€è®¡ç®—coså’Œsinï¼ˆç”¨äºéæ ‡å‡†ä½ç½®ï¼‰"""
        batch_size, seq_len = positions.shape
        device = positions.device

        # è®¡ç®—é¢‘ç‡
        indices = torch.arange(0, self.head_dim, 2, dtype=torch.float32, device=device)
        freqs = 1.0 / (10000 ** (indices / self.head_dim))

        # è®¡ç®—è§’åº¦
        angles = positions.unsqueeze(-1) * freqs  # [batch_size, seq_len, head_dim/2]
        angles = torch.cat([angles, angles], dim=-1)  # [batch_size, seq_len, head_dim]

        # è®¡ç®—coså’Œsin
        cos = torch.cos(angles).unsqueeze(2)  # [batch_size, seq_len, 1, head_dim]
        sin = torch.sin(angles).unsqueeze(2)

        return cos, sin

# æ€§èƒ½å¯¹æ¯”æµ‹è¯•
def rope_performance_comparison():
    """RoPEæ€§èƒ½å¯¹æ¯”æµ‹è¯•"""

    print("=== RoPEæ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")

    # æµ‹è¯•é…ç½®
    configs = [
        {"seq_len": 512, "head_dim": 64, "name": "å°å‹é…ç½®"},
        {"seq_len": 1024, "head_dim": 128, "name": "ä¸­å‹é…ç½®"},
        {"seq_len": 2048, "head_dim": 256, "name": "å¤§å‹é…ç½®"},
    ]

    for config in configs:
        print(f"\n{config['name']}:")
        seq_len = config["seq_len"]
        head_dim = config["head_dim"]

        # æµ‹è¯•æ•°æ®
        batch_size = 8
        num_heads = 32

        q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
        k = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')

        # åŸºç¡€å®ç°
        basic_rope = BasicRoPE(head_dim).cuda()

        torch.cuda.synchronize()
        start_time = time.time()
        for _ in range(100):
            positions = torch.arange(seq_len, device='cuda').unsqueeze(0).expand(batch_size, -1)
            q_rot_basic = basic_rope(q.transpose(1, 2), positions).transpose(1, 2)
            k_rot_basic = basic_rope(k.transpose(1, 2), positions).transpose(1, 2)
            torch.cuda.synchronize()
        basic_time = (time.time() - start_time) / 100

        # ä¼˜åŒ–å®ç°
        optimized_rope = OptimizedRoPE(head_dim, max_seq_len=seq_len).cuda()

        torch.cuda.synchronize()
        start_time = time.time()
        for _ in range(100):
            q_rot_opt, k_rot_opt = optimized_rope(q, k)
            torch.cuda.synchronize()
        optimized_time = (time.time() - start_time) / 100

        # éªŒè¯ç»“æœä¸€è‡´æ€§
        max_diff = torch.max(torch.abs(q_rot_basic - q_rot_opt))

        print(f"  åŸºç¡€å®ç°: {basic_time*1000:.2f} ms")
        print(f"  ä¼˜åŒ–å®ç°: {optimized_time*1000:.2f} ms")
        print(f"  æ€§èƒ½æå‡: {basic_time/optimized_time:.2f}x")
        print(f"  æ•°å€¼å·®å¼‚: {max_diff:.2e}")

rope_performance_comparison()
```

## ğŸ¯ RoPEçš„æ·±å±‚ç‰¹æ€§åˆ†æ

### 1. ç›¸å¯¹ä½ç½®ä¿æŒæ€§

```python
def analyze_relative_position_property():
    """åˆ†æRoPEçš„ç›¸å¯¹ä½ç½®ä¿æŒç‰¹æ€§"""

    print("=== RoPEç›¸å¯¹ä½ç½®ä¿æŒæ€§åˆ†æ ===")

    # åˆ›å»ºæµ‹è¯•å‘é‡
    d_model = 64
    test_vec = torch.randn(d_model)

    rope = OptimizedRoPE(d_model)

    # æµ‹è¯•ä¸åŒä½ç½®çš„è¡¨ç¤º
    positions = [0, 1, 2, 3, 10]
    rotated_vectors = []

    for pos in positions:
        # è®¡ç®—æ—‹è½¬åçš„å‘é‡
        angles = pos * rope._compute_freqs()
        cos_vals = torch.cos(angles)
        sin_vals = torch.sin(angles)

        rotated = rope._apply_rope_vectorized(
            test_vec.unsqueeze(0).unsqueeze(0),
            cos_vals.unsqueeze(0).unsqueeze(0),
            sin_vals.unsqueeze(0).unsqueeze(0)
        ).squeeze()

        rotated_vectors.append(rotated)

    # è®¡ç®—ç›¸å¯¹ä½ç½®çš„ç›¸ä¼¼æ€§
    print("ç›¸å¯¹ä½ç½®ç›¸ä¼¼æ€§åˆ†æ:")
    for i in range(len(positions)):
        for j in range(i+1, len(positions)):
            pos1, pos2 = positions[i], positions[j]
            vec1, vec2 = rotated_vectors[i], rotated_vectors[j]

            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = F.cosine_similarity(vec1, vec2, dim=0)
            relative_distance = abs(pos2 - pos1)

            print(f"  ä½ç½®{pos1} vs ä½ç½®{pos2} (è·ç¦»={relative_distance}): "
                  f"ç›¸ä¼¼åº¦={similarity:.4f}")

    # å¯è§†åŒ–ç›¸å¯¹ä½ç½®å…³ç³»
    plt.figure(figsize=(12, 8))

    # å­å›¾1ï¼šä¸åŒä½ç½®çš„å‘é‡è¡¨ç¤º
    ax1 = plt.subplot(2, 2, 1)
    for i, (pos, vec) in enumerate(zip(positions, rotated_vectors)):
        plt.scatter(vec[0], vec[1], label=f'ä½ç½®{pos}', s=100, alpha=0.7)
        plt.text(vec[0]+0.02, vec[1]+0.02, f'{pos}', fontsize=12)

    plt.xlabel('ç»´åº¦ 0')
    plt.ylabel('ç»´åº¦ 1')
    plt.title('ä¸åŒä½ç½®çš„å‘é‡è¡¨ç¤ºï¼ˆ2DæŠ•å½±ï¼‰')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å­å›¾2ï¼šç›¸ä¼¼åº¦çŸ©é˜µ
    ax2 = plt.subplot(2, 2, 2)
    similarity_matrix = torch.zeros(len(positions), len(positions))
    for i in range(len(positions)):
        for j in range(len(positions)):
            similarity_matrix[i, j] = F.cosine_similarity(
                rotated_vectors[i], rotated_vectors[j], dim=0
            )

    im = plt.imshow(similarity_matrix.numpy(), cmap='viridis', aspect='auto')
    plt.xticks(range(len(positions)), positions)
    plt.yticks(range(len(positions)), positions)
    plt.colorbar(im, label='ä½™å¼¦ç›¸ä¼¼åº¦')
    plt.title('ä½ç½®ç›¸ä¼¼åº¦çŸ©é˜µ')

    # å­å›¾3ï¼šç›¸å¯¹è·ç¦» vs ç›¸ä¼¼åº¦
    ax3 = plt.subplot(2, 2, 3)
    relative_distances = []
    similarities = []

    for i in range(len(positions)):
        for j in range(i+1, len(positions)):
            relative_distances.append(abs(positions[j] - positions[i]))
            similarities.append(F.cosine_similarity(
                rotated_vectors[i], rotated_vectors[j], dim=0
            ).item())

    plt.scatter(relative_distances, similarities, alpha=0.7, s=100)
    plt.xlabel('ç›¸å¯¹ä½ç½®è·ç¦»')
    plt.ylabel('ä½™å¼¦ç›¸ä¼¼åº¦')
    plt.title('ç›¸å¯¹è·ç¦» vs ç›¸ä¼¼åº¦å…³ç³»')
    plt.grid(True, alpha=0.3)

    # å­å›¾4ï¼šé¢‘ç‡åˆ†æ
    ax4 = plt.subplot(2, 2, 4)
    freqs = rope._compute_freqs()[:8]  # åªæ˜¾ç¤ºå‰8ä¸ªé¢‘ç‡
    freq_positions = list(range(len(freqs)))

    plt.bar(freq_positions, freqs.log10())
    plt.xlabel('é¢‘ç‡ç´¢å¼•')
    plt.ylabel('log10(é¢‘ç‡)')
    plt.title('RoPEé¢‘ç‡åˆ†å¸ƒï¼ˆå¯¹æ•°å°ºåº¦ï¼‰')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

analyze_relative_position_property()
```

### 2. å¤–æ¨æ€§åˆ†æ

```python
def analyze_extrapolation_capability():
    """åˆ†æRoPEçš„å¤–æ¨èƒ½åŠ›"""

    print("=== RoPEå¤–æ¨æ€§åˆ†æ ===")

    # æ¨¡æ‹Ÿè®­ç»ƒå’Œæ¨ç†åœºæ™¯
    train_max_len = 512
    inference_lengths = [600, 800, 1024, 2048]

    d_model = 64
    test_vec = torch.randn(d_model)
    rope = OptimizedRoPE(d_model, max_seq_len=train_max_len)

    print(f"è®­ç»ƒæœ€å¤§é•¿åº¦: {train_max_len}")
    print(f"æµ‹è¯•å¤–æ¨é•¿åº¦: {inference_lengths}")
    print()

    # åˆ†æå¤–æ¨æ€§èƒ½
    for inference_len in inference_lengths:
        print(f"æ¨ç†é•¿åº¦: {inference_len}")

        # è®¡ç®—è¶…å‡ºè®­ç»ƒèŒƒå›´çš„å‘é‡
        positions_outside_range = list(range(train_max_len, min(inference_len, train_max_len + 100)))

        if positions_outside_range:
            # è®¡ç®—è®­ç»ƒèŒƒå›´å†…æœ€æœ«ä½ç½®çš„å‘é‡
            last_train_pos = train_max_len - 1
            angles_last = last_train_pos * rope._compute_freqs()
            cos_last = torch.cos(angles_last)
            sin_last = torch.sin(angles_last)

            vec_last = rope._apply_rope_vectorized(
                test_vec.unsqueeze(0).unsqueeze(0),
                cos_last.unsqueeze(0).unsqueeze(0),
                sin_last.unsqueeze(0).unsqueeze(0)
            ).squeeze()

            # è®¡ç®—è¶…å‡ºèŒƒå›´çš„å‘é‡
            outside_pos = positions_outside_range[0]
            angles_outside = outside_pos * rope._compute_freqs()
            cos_outside = torch.cos(angles_outside)
            sin_outside = torch.sin(angles_outside)

            vec_outside = rope._apply_rope_vectorized(
                test_vec.unsqueeze(0).unsqueeze(0),
                cos_outside.unsqueeze(0).unsqueeze(0),
                sin_outside.unsqueeze(0).unsqueeze(0)
            ).squeeze()

            # è®¡ç®—ç›¸ä¼¼æ€§
            similarity = F.cosine_similarity(vec_last, vec_outside, dim=0)
            position_gap = outside_pos - last_train_pos

            print(f"  è®­ç»ƒèŒƒå›´æœ«å°¾ä½ç½® {last_train_pos} vs è¶…å‡ºä½ç½® {outside_pos}")
            print(f"  ä½ç½®å·®è·: {position_gap}")
            print(f"  å‘é‡ç›¸ä¼¼åº¦: {similarity:.4f}")

            # åˆ†æé¢‘ç‡å¯¹é½æƒ…å†µ
            freqs = rope._compute_freqs()
            angle_diff_last = (position_gap * freqs) % (2 * math.pi)
            angle_diff_outside = ((outside_pos % train_max_len) * freqs) % (2 * math.pi)

            freq_alignment = F.cosine_similarity(
                torch.cos(angle_diff_last), torch.cos(angle_diff_outside), dim=0
            )
            print(f"  é¢‘ç‡å¯¹é½åº¦: {freq_alignment:.4f}")

        print()

    # å¤–æ¨æ€§å¯è§†åŒ–
    plt.figure(figsize=(15, 5))

    # å­å›¾1ï¼šè®­ç»ƒèŒƒå›´å†…çš„å‘é‡å˜åŒ–
    ax1 = plt.subplot(1, 3, 1)
    train_positions = list(range(0, train_max_len, 50))
    train_vectors = []

    for pos in train_positions:
        angles = pos * rope._compute_freqs()
        cos_vals = torch.cos(angles)
        sin_vals = torch.sin(angles)
        vec = rope._apply_rope_vectorized(
            test_vec.unsqueeze(0).unsqueeze(0),
            cos_vals.unsqueeze(0).unsqueeze(0),
            sin_vals.unsqueeze(0).unsqueeze(0)
        ).squeeze()
        train_vectors.append(vec[:2].numpy())  # åªå–å‰2ç»´

    train_vectors = np.array(train_vectors)
    plt.plot(train_positions, train_vectors[:, 0], 'b-', label='ç»´åº¦0', alpha=0.7)
    plt.plot(train_positions, train_vectors[:, 1], 'r-', label='ç»´åº¦1', alpha=0.7)
    plt.axvline(x=train_max_len, color='k', linestyle='--', label='è®­ç»ƒè¾¹ç•Œ')
    plt.xlabel('ä½ç½®')
    plt.ylabel('å‘é‡å€¼')
    plt.title('è®­ç»ƒèŒƒå›´å†…çš„å‘é‡å˜åŒ–')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å­å›¾2ï¼šå¤–æ¨åŒºåŸŸçš„å‘é‡å˜åŒ–
    ax2 = plt.subplot(1, 3, 2)
    extended_positions = list(range(train_max_len, train_max_len + 200, 10))
    extended_vectors = []

    for pos in extended_positions:
        angles = pos * rope._compute_freqs()
        cos_vals = torch.cos(angles)
        sin_vals = torch.sin(angles)
        vec = rope._apply_rope_vectorized(
            test_vec.unsqueeze(0).unsqueeze(0),
            cos_vals.unsqueeze(0).unsqueeze(0),
            sin_vals.unsqueeze(0).unsqueeze(0)
        ).squeeze()
        extended_vectors.append(vec[:2].numpy())

    extended_vectors = np.array(extended_vectors)
    plt.plot(extended_positions, extended_vectors[:, 0], 'b-', label='ç»´åº¦0', alpha=0.7)
    plt.plot(extended_positions, extended_vectors[:, 1], 'r-', label='ç»´åº¦1', alpha=0.7)
    plt.axvline(x=train_max_len, color='k', linestyle='--', label='è®­ç»ƒè¾¹ç•Œ')
    plt.xlabel('ä½ç½®')
    plt.ylabel('å‘é‡å€¼')
    plt.title('å¤–æ¨åŒºåŸŸçš„å‘é‡å˜åŒ–')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å­å›¾3ï¼šé¢‘ç‡å‘¨æœŸæ€§åˆ†æ
    ax3 = plt.subplot(1, 3, 3)
    freqs = rope._compute_freqs()[:8]  # å‰8ä¸ªé¢‘ç‡
    positions = np.arange(0, train_max_len + 200, 10)

    for i, freq in enumerate(freqs):
        values = np.cos(positions * freq.item())
        plt.plot(positions, values + i*0.5, alpha=0.7, label=f'é¢‘ç‡{i}')

    plt.axvline(x=train_max_len, color='k', linestyle='--', alpha=0.5, label='è®­ç»ƒè¾¹ç•Œ')
    plt.xlabel('ä½ç½®')
    plt.ylabel('cos(ä½ç½® Ã— é¢‘ç‡)')
    plt.title('ä¸åŒé¢‘ç‡çš„å‘¨æœŸæ€§å˜åŒ–')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

analyze_extrapolation_capability()
```

## ğŸš€ RoPEçš„å˜ä½“ä¸ä¼˜åŒ–

### 1. å¤šç§RoPEå˜ä½“

```python
class RoPEVariants:
    """RoPEå˜ä½“å®ç°"""

    def __init__(self, d_model):
        self.d_model = d_model

    def original_rope(self, x, position):
        """åŸå§‹RoPEå®ç°"""
        indices = torch.arange(0, d_model, 2, dtype=torch.float32)
        freqs = 1.0 / (10000 ** (indices / d_model))
        angles = position * freqs
        cos_vals = torch.cos(angles)
        sin_vals = torch.sin(angles)

        x_real = x[..., ::2]
        x_imag = x[..., 1::2]

        x_rot_real = x_real * cos_vals - x_imag * sin_vals
        x_rot_imag = x_real * sin_vals + x_imag * cos_vals

        return torch.cat([x_rot_real, x_rot_imag], dim=-1)

    def linear_rope(self, x, position):
        """çº¿æ€§RoPE - æ”¹è¿›å¤–æ¨æ€§"""
        indices = torch.arange(0, d_model, 2, dtype=torch.float32)
        # ä½¿ç”¨çº¿æ€§å¢é•¿è€ŒéæŒ‡æ•°è¡°å‡çš„é¢‘ç‡
        freqs = 1.0 / (1 + indices)
        angles = position * freqs
        cos_vals = torch.cos(angles)
        sin_vals = torch.sin(angles)

        x_real = x[..., ::2]
        x_imag = x[..., 1::2]

        x_rot_real = x_real * cos_vals - x_imag * sin_vals
        x_rot_imag = x_real * sin_vals + x_imag * cos_vals

        return torch.cat([x_rot_real, x_rot_imag], dim=-1)

    def yarn_rope(self, x, position, alpha=1.0, beta=1.0):
        """YaRN RoPE - æ”¹è¿›é•¿åºåˆ—å¤„ç†"""
        indices = torch.arange(0, d_model, 2, dtype=torch.float32)
        base_freqs = 1.0 / (10000 ** (indices / d_model))

        # YaRNçš„é¢‘ç‡è°ƒæ•´
        adjusted_freqs = base_freqs * alpha
        angles = position * adjusted_freqs

        # ä½ç½®é‡ç¼©æ”¾
        scaled_angles = angles * beta

        cos_vals = torch.cos(scaled_angles)
        sin_vals = torch.sin(scaled_angles)

        x_real = x[..., ::2]
        x_imag = x[..., 1::2]

        x_rot_real = x_real * cos_vals - x_imag * sin_vals
        x_rot_imag = x_real * sin_vals + x_imag * cos_vals

        return torch.cat([x_rot_real, x_rot_imag], dim=-1)

    def xpos_rope(self, x, position):
        """XPOS RoPE - ç›¸å¯¹ä½ç½®ç¼–ç çš„æ”¹è¿›"""
        indices = torch.arange(0, d_model, 2, dtype=torch.float32)
        freqs = 1.0 / (10000 ** (indices / d_model))
        angles = position * freqs

        # XPOSä½¿ç”¨ä¸åŒçš„è¡°å‡æœºåˆ¶
        decay = torch.exp(-angles / 100)  # å¼•å…¥è¡°å‡
        cos_vals = torch.cos(angles) * decay
        sin_vals = torch.sin(angles) * decay

        x_real = x[..., ::2]
        x_imag = x[..., 1::2]

        x_rot_real = x_real * cos_vals - x_imag * sin_vals
        x_rot_imag = x_real * sin_vals + x_imag * cos_vals

        return torch.cat([x_rot_real, x_rot_imag], dim=-1)

def compare_rope_variants():
    """æ¯”è¾ƒä¸åŒRoPEå˜ä½“çš„æ€§èƒ½"""

    print("=== RoPEå˜ä½“æ¯”è¾ƒ ===")

    d_model = 64
    test_vec = torch.randn(d_model)
    variants = RoPEVariants(d_model)

    # æµ‹è¯•ä¸åŒä½ç½®
    test_positions = [100, 500, 1000, 2000, 4000]
    variant_names = ['Original', 'Linear', 'YaRN', 'XPOS']

    results = {name: [] for name in variant_names}

    for pos in test_positions:
        # è®¡ç®—ä¸åŒå˜ä½“çš„ç»“æœ
        original = variants.original_rope(test_vec, pos)
        linear = variants.linear_rope(test_vec, pos)
        yarn = variants.yarn_rope(test_vec, pos, alpha=1.2, beta=0.8)
        xpos = variants.xpos_rope(test_vec, pos)

        results['Original'].append(original)
        results['Linear'].append(linear)
        results['YaRN'].append(yarn)
        results['XPOS'].append(xpos)

    # åˆ†æå¤–æ¨ç¨³å®šæ€§
    print("å¤–æ¨ç¨³å®šæ€§åˆ†æï¼ˆå‘é‡èŒƒæ•°å˜åŒ–ï¼‰:")
    for name, vectors in results.items():
        norms = [vec.norm().item() for vec in vectors]
        norm_std = np.std(norms)
        print(f"  {name}: èŒƒæ•°æ ‡å‡†å·® = {norm_std:.4f}")

    # å¯è§†åŒ–æ¯”è¾ƒ
    plt.figure(figsize=(15, 10))

    # å­å›¾1-4ï¼šå„å˜ä½“çš„å‘é‡å˜åŒ–
    for i, (name, vectors) in enumerate(results.items(), 1):
        ax = plt.subplot(2, 2, i)

        # æå–å‰2ç»´è¿›è¡Œå¯è§†åŒ–
        vectors_2d = [vec[:2].numpy() for vec in vectors]
        vectors_2d = np.array(vectors_2d)

        ax.plot(test_positions, vectors_2d[:, 0], 'b-', label='ç»´åº¦0', alpha=0.7)
        ax.plot(test_positions, vectors_2d[:, 1], 'r-', label='ç»´åº¦1', alpha=0.7)
        ax.set_xlabel('ä½ç½®')
        ax.set_ylabel('å‘é‡å€¼')
        ax.set_title(f'{name} RoPE')
        ax.legend()
        ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

compare_rope_variants()
```

### 2. RoPEçš„å†…å­˜å’Œè®¡ç®—ä¼˜åŒ–

```python
class MemoryEfficientRoPE:
    """å†…å­˜é«˜æ•ˆçš„RoPEå®ç°"""

    def __init__(self, head_dim, max_seq_len=4096, chunk_size=512):
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.chunk_size = chunk_size

        # åˆ†å—ç¼“å­˜coså’Œsin
        self._setup_chunked_cache()

    def _setup_chunked_cache(self):
        """è®¾ç½®åˆ†å—ç¼“å­˜"""
        num_chunks = (self.max_seq_len + self.chunk_size - 1) // self.chunk_size
        self.cos_chunks = []
        self.sin_chunks = []

        for chunk_idx in range(num_chunks):
            start_pos = chunk_idx * self.chunk_size
            end_pos = min(start_pos + self.chunk_size, self.max_seq_len)
            chunk_len = end_pos - start_pos

            # è®¡ç®—è¿™ä¸ªå—çš„coså’Œsin
            indices = torch.arange(0, self.head_dim, 2, dtype=torch.float32)
            freqs = 1.0 / (10000 ** (indices / self.head_dim))

            positions = torch.arange(start_pos, end_pos).float()
            angles = torch.outer(positions, freqs)

            cos_chunk = torch.cos(angles)
            sin_chunk = torch.sin(angles)

            self.cos_chunks.append(cos_chunk)
            self.sin_chunks.append(sin_chunk)

    def forward_chunked(self, q, k, positions):
        """åˆ†å—å¤„ç†RoPE"""
        batch_size, num_heads, seq_len, head_dim = q.shape

        # å°†ä½ç½®åˆ†å—
        chunk_results_q = []
        chunk_results_k = []

        for chunk_idx in range(len(self.cos_chunks)):
            start_pos = chunk_idx * self.chunk_size
            end_pos = min(start_pos + self.chunk_size, seq_len)

            if start_pos >= seq_len:
                break

            # è·å–å½“å‰å—çš„coså’Œsin
            cos_chunk = self.cos_chunks[chunk_idx][:end_pos - start_pos]
            sin_chunk = self.sin_chunks[chunk_idx][:end_pos - start_pos]

            # å¤„ç†å½“å‰å—
            q_chunk = q[:, :, start_pos:end_pos, :]
            k_chunk = k[:, :, start_pos:end_pos, :]

            cos_expanded = cos_chunk.unsqueeze(0).unsqueeze(0)
            sin_expanded = sin_chunk.unsqueeze(0).unsqueeze(0)

            q_rot_chunk = self._apply_rope_vectorized(q_chunk, cos_expanded, sin_expanded)
            k_rot_chunk = self._apply_rope_vectorized(k_chunk, cos_expanded, sin_expanded)

            chunk_results_q.append(q_rot_chunk)
            chunk_results_k.append(k_rot_chunk)

        # åˆå¹¶ç»“æœ
        q_rotated = torch.cat(chunk_results_q, dim=2)
        k_rotated = torch.cat(chunk_results_k, dim=2)

        return q_rotated, k_rotated

    def _apply_rope_vectorized(self, x, cos, sin):
        """å‘é‡åŒ–çš„RoPEåº”ç”¨"""
        x2 = torch.cat([-x[..., self.head_dim//2:], x[..., :self.head_dim//2]], dim=-1)
        return x * cos + x2 * sin

# å†…å­˜æ•ˆç‡æµ‹è¯•
def test_memory_efficient_rope():
    """æµ‹è¯•å†…å­˜é«˜æ•ˆçš„RoPEå®ç°"""

    print("=== å†…å­˜æ•ˆç‡æµ‹è¯• ===")

    # æµ‹è¯•å¤§åºåˆ—é•¿åº¦
    seq_len = 8192
    head_dim = 128
    batch_size = 4
    num_heads = 32

    q = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')
    k = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda')

    # æ ‡å‡†RoPE
    standard_rope = OptimizedRoPE(head_dim, max_seq_len=seq_len).cuda()

    torch.cuda.reset_peak_memory_stats()
    start_time = time.time()
    q_std, k_std = standard_rope(q, k)
    torch.cuda.synchronize()
    standard_time = time.time() - start_time
    standard_memory = torch.cuda.max_memory_allocated() / 1024 / 1024

    # å†…å­˜é«˜æ•ˆRoPE
    efficient_rope = MemoryEfficientRoPE(head_dim, max_seq_len=seq_len, chunk_size=512)

    torch.cuda.reset_peak_memory_stats()
    start_time = time.time()
    q_eff, k_eff = efficient_rope.forward_chunked(q, k, None)
    torch.cuda.synchronize()
    efficient_time = time.time() - start_time
    efficient_memory = torch.cuda.max_memory_allocated() / 1024 / 1024

    # éªŒè¯ç»“æœä¸€è‡´æ€§
    max_diff_q = torch.max(torch.abs(q_std - q_eff))
    max_diff_k = torch.max(torch.abs(k_std - k_eff))

    print(f"åºåˆ—é•¿åº¦: {seq_len}")
    print(f"æ ‡å‡†RoPE: æ—¶é—´={standard_time*1000:.2f}ms, å†…å­˜={standard_memory:.1f}MB")
    print(f"é«˜æ•ˆRoPE: æ—¶é—´={efficient_time*1000:.2f}ms, å†…å­˜={efficient_memory:.1f}MB")
    print(f"å†…å­˜èŠ‚çœ: {(standard_memory-efficient_memory)/standard_memory*100:.1f}%")
    print(f"æ—¶é—´å·®å¼‚: {(efficient_time-standard_time)/standard_time*100:.1f}%")
    print(f"æ•°å€¼ç²¾åº¦å·®å¼‚: {max(max_diff_q.item(), max_diff_k.item()):.2e}")

test_memory_efficient_rope()
```

## ğŸ¯ RoPEåœ¨å®é™…æ¨¡å‹ä¸­çš„åº”ç”¨

### ä¸Attentionæœºåˆ¶çš„é›†æˆ

```python
class RoPEIntegratedAttention(nn.Module):
    """é›†æˆRoPEçš„å®Œæ•´Attentionå®ç°"""

    def __init__(self, d_model, num_heads, max_seq_len=4096, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.scale = 1.0 / math.sqrt(self.head_dim)

        # QKVæŠ•å½±
        self.qkv_proj = nn.Linear(d_model, d_model * 3, bias=False)

        # è¾“å‡ºæŠ•å½±
        self.o_proj = nn.Linear(d_model, d_model, bias=True)

        # RoPE
        self.rope = OptimizedRoPE(self.head_dim, max_seq_len)

        # Dropout
        self.attention_dropout = nn.Dropout(dropout)
        self.output_dropout = nn.Dropout(dropout)

    def forward(self, x, attention_mask=None, position_ids=None):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: [batch_size, seq_len, d_model]
            attention_mask: [batch_size, 1, seq_len, seq_len]
            position_ids: [batch_size, seq_len]
        """
        batch_size, seq_len, d_model = x.shape

        # QKVæŠ•å½±
        qkv = self.qkv_proj(x)
        qkv = qkv.view(batch_size, seq_len, 3, self.num_heads, self.head_dim)
        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, batch, heads, seq, dim]
        q, k, v = qkv[0], qkv[1], qkv[2]

        # åº”ç”¨RoPE
        q, k = self.rope(q, k, position_ids)

        # è®¡ç®—Attentionåˆ†æ•°
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale

        # åº”ç”¨attention mask
        if attention_mask is not None:
            scores = scores.masked_fill(attention_mask == 0, float('-inf'))

        # Softmax
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.attention_dropout(attn_weights)

        # Attentionè¾“å‡º
        output = torch.matmul(attn_weights, v)

        # é‡å¡‘å’Œè¾“å‡ºæŠ•å½±
        output = output.transpose(1, 2).contiguous()
        output = output.view(batch_size, seq_len, d_model)
        output = self.o_proj(output)
        output = self.output_dropout(output)

        return output, attn_weights

# é›†æˆæµ‹è¯•
def test_rope_integration():
    """æµ‹è¯•RoPEä¸Attentionçš„é›†æˆ"""

    print("=== RoPEé›†æˆæµ‹è¯• ===")

    # æ¨¡å‹é…ç½®
    d_model = 512
    num_heads = 8
    max_seq_len = 1024
    batch_size = 4

    # åˆ›å»ºæ¨¡å‹
    model = RoPEIntegratedAttention(d_model, num_heads, max_seq_len).cuda()

    # æµ‹è¯•æ•°æ®
    seq_len = 512
    x = torch.randn(batch_size, seq_len, d_model, device='cuda')

    # åˆ›å»ºcausal mask
    causal_mask = torch.tril(torch.ones(seq_len, seq_len, device='cuda'))
    causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]

    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output, attention_weights = model(x, causal_mask)

    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attention_weights.shape}")
    print(f"è¾“å‡ºèŒƒæ•°: {output.norm(dim=-1).mean():.4f}")
    print(f"æ³¨æ„åŠ›æƒé‡å’Œ: {attention_weights.sum(dim=-1).mean():.4f}")

    # æ€§èƒ½æµ‹è¯•
    num_runs = 100
    torch.cuda.synchronize()
    start_time = time.time()

    for _ in range(num_runs):
        with torch.no_grad():
            output, _ = model(x, causal_mask)
        torch.cuda.synchronize()

    avg_time = (time.time() - start_time) / num_runs
    print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.2f} ms")

test_rope_integration()
```

## ğŸ¯ æ€»ç»“ä¸å±•æœ›

### æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹

é€šè¿‡æœ¬æ–‡çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å…¨é¢æŒæ¡äº†RoPEä½ç½®ç¼–ç çš„æ ¸å¿ƒæŠ€æœ¯ï¼š

1. **è®¾è®¡å“²å­¦**ï¼šé€šè¿‡æ—‹è½¬è€ŒéåŠ æ³•æ¥ç¼–ç ä½ç½®ä¿¡æ¯
2. **æ•°å­¦åŸç†**ï¼šåŸºäºå¤æ•°æ—‹è½¬çš„ä¼˜é›…æ•°å­¦æ¡†æ¶
3. **å®ç°ä¼˜åŒ–**ï¼šä»åŸºç¡€å®ç°åˆ°ç”Ÿäº§çº§çš„é«˜æ•ˆå®ç°
4. **å˜ä½“æ¢ç´¢**ï¼šå¤šç§RoPEå˜ä½“åŠå…¶é€‚ç”¨åœºæ™¯
5. **å·¥ç¨‹é›†æˆ**ï¼šä¸Attentionæœºåˆ¶çš„æ— ç¼é›†æˆ

### RoPEçš„çªå‡ºä¼˜åŠ¿

**ç†è®ºä¼˜åŠ¿**ï¼š
- **ç›¸å¯¹ä½ç½®ä¿æŒ**ï¼šè‡ªç„¶åœ°ç¼–ç ç›¸å¯¹ä½ç½®å…³ç³»
- **å¤–æ¨èƒ½åŠ›å¼º**ï¼šç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æœ‰æ›´å¥½çš„é•¿åºåˆ—å¤„ç†èƒ½åŠ›
- **å‚æ•°æ•ˆç‡é«˜**ï¼šæ— éœ€å­¦ä¹ é¢å¤–çš„ä½ç½®å‚æ•°
- **æ•°å€¼ç¨³å®šæ€§å¥½**ï¼šé¿å…äº†ä½ç½®ç¼–ç çš„æ•°å€¼çˆ†ç‚¸

**å®è·µä¼˜åŠ¿**ï¼š
- **è®¡ç®—æ•ˆç‡é«˜**ï¼šå¯ä»¥ä¸Attentionè®¡ç®—èåˆ
- **å†…å­˜å‹å¥½**ï¼šæ”¯æŒç¼“å­˜å’Œåˆ†å—è®¡ç®—
- **æ˜“äºé›†æˆ**ï¼šä¸ç°æœ‰Transformeræ¶æ„å…¼å®¹æ€§å¥½

### æ€§èƒ½æå‡æ€»ç»“

**è®¡ç®—æ€§èƒ½**ï¼š
- **1.5-3å€**çš„RoPEè®¡ç®—åŠ é€Ÿï¼ˆé€šè¿‡ä¼˜åŒ–å®ç°ï¼‰
- **20-50%**çš„å†…å­˜ä½¿ç”¨å‡å°‘ï¼ˆé€šè¿‡åˆ†å—å’Œç¼“å­˜ï¼‰
- **æ›´å¥½çš„GPUåˆ©ç”¨ç‡**

**æ¨¡å‹æ€§èƒ½**ï¼š
- **é•¿åºåˆ—å¤„ç†**ï¼šæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦
- **ç›¸å¯¹ä½ç½®ç†è§£**ï¼šæ›´å¥½åœ°æ•æ‰tokené—´çš„å…³ç³»
- **å¤–æ¨ç¨³å®šæ€§**ï¼šåœ¨è¶…å‡ºè®­ç»ƒé•¿åº¦æ—¶ä¿æŒè¾ƒå¥½çš„æ€§èƒ½

### æœªæ¥å‘å±•æ–¹å‘

1. **æ”¹è¿›å¤–æ¨æ€§**ï¼šæ›´å…ˆè¿›çš„é•¿åºåˆ—å¤„ç†æŠ€æœ¯
2. **è‡ªé€‚åº”é¢‘ç‡**ï¼šæ ¹æ®ä»»åŠ¡åŠ¨æ€è°ƒæ•´æ—‹è½¬é¢‘ç‡
3. **å¤šå°ºåº¦RoPE**ï¼šç»“åˆä¸åŒå°ºåº¦çš„ä½ç½®ä¿¡æ¯
4. **ç¡¬ä»¶ååŒè®¾è®¡**ï¼šé’ˆå¯¹ç‰¹å®šç¡¬ä»¶çš„RoPEä¼˜åŒ–

### å®è·µå»ºè®®

**ä½¿ç”¨åœºæ™¯é€‰æ‹©**ï¼š
- **æ ‡å‡†NLPä»»åŠ¡**ï¼šä½¿ç”¨åŸå§‹RoPE
- **é•¿æ–‡æ¡£å¤„ç†**ï¼šè€ƒè™‘YaRNç­‰æ”¹è¿›å˜ä½“
- **ä»£ç ç”Ÿæˆ**ï¼šå¯ä»¥ä½¿ç”¨Linear RoPEæå‡å¤–æ¨æ€§
- **å¤šæ¨¡æ€ä»»åŠ¡**ï¼šè€ƒè™‘ä»»åŠ¡ç‰¹å®šçš„RoPEè°ƒæ•´

**ä¼˜åŒ–é‡ç‚¹**ï¼š
- é¢„è®¡ç®—coså’Œsinç¼“å­˜
- ä½¿ç”¨å‘é‡åŒ–æ“ä½œ
- è€ƒè™‘åˆ†å—å¤„ç†é•¿åºåˆ—
- ä¸Attentionè®¡ç®—èåˆ

---

**è®°ä½**ï¼šRoPEä¸ä»…æ˜¯ä½ç½®ç¼–ç çš„æŠ€æœ¯æ”¹è¿›ï¼Œæ›´æ˜¯å¯¹"å¦‚ä½•åœ¨å‘é‡ç©ºé—´ä¸­è¡¨ç¤ºä½ç½®å…³ç³»"è¿™ä¸€æ ¹æœ¬é—®é¢˜çš„ä¼˜é›…è§£ç­”ã€‚æŒæ¡RoPEï¼Œå°±æŒæ¡äº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹ä½ç½®ç¼–ç çš„æ ¸å¿ƒæŠ€æœ¯ã€‚

*ä¸‹ä¸€ç¯‡æ–‡ç« å°†æ·±å…¥æ¢è®¨Attentionçš„å„ç§å˜ä½“å’Œæ‰©å±•ï¼Œäº†è§£è¿™ä¸ªé¢†åŸŸçš„å‰æ²¿å‘å±•å’Œåˆ›æ–°æ–¹å‘ã€‚* ğŸš€