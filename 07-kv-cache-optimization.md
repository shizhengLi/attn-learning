# KVç¼“å­˜ä¼˜åŒ–æŠ€æœ¯ï¼šä»é™æ€åˆ°åŠ¨æ€çš„æ¼”è¿›

## ğŸ¯ å¼•è¨€ï¼šKVç¼“å­˜çš„é‡è¦æ€§

åœ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒKVç¼“å­˜æ˜¯æœ€å…³é”®çš„ç»„ä»¶ä¹‹ä¸€ã€‚å®ƒä¸ä»…ç›´æ¥å½±å“æ¨ç†é€Ÿåº¦ï¼Œæ›´å†³å®šäº†å†…å­˜ä½¿ç”¨æ•ˆç‡ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå½“ä½ ä¸ChatGPTå¯¹è¯æ—¶ï¼Œæ¨¡å‹éœ€è¦è®°ä½ä¹‹å‰æ‰€æœ‰çš„å¯¹è¯å†…å®¹â€”â€”è¿™å°±æ˜¯KVç¼“å­˜çš„ä½œç”¨ã€‚

ç„¶è€Œï¼Œä¼ ç»Ÿçš„KVç¼“å­˜é¢ä¸´ç€ä¸¥å³»çš„æŒ‘æˆ˜ï¼š
- **å†…å­˜çˆ†ç‚¸**ï¼šéšç€å¯¹è¯é•¿åº¦å¢åŠ ï¼Œç¼“å­˜å‘ˆçº¿æ€§å¢é•¿
- **å†…å­˜ç¢ç‰‡**ï¼šä¸è§„åˆ™çš„ä½¿ç”¨æ¨¡å¼å¯¼è‡´å†…å­˜åˆ©ç”¨ç‡ä½ä¸‹
- **é¢„åˆ†é…å›°éš¾**ï¼šä¸çŸ¥é“æœªæ¥éœ€è¦å¤šå°‘ç¼“å­˜ç©ºé—´

æœ¬æ–‡å°†æ·±å…¥æ¢è®¨KVç¼“å­˜ä¼˜åŒ–æŠ€æœ¯çš„æ¼”è¿›å†ç¨‹ï¼Œä»é™æ€ç®¡ç†åˆ°åŠ¨æ€ä¼˜åŒ–ï¼Œè®©ä½ å…¨é¢ç†è§£è¿™ä¸ªçœ‹ä¼¼ç®€å•å´è•´å«æ·±åšæŠ€æœ¯å†…æ¶µçš„é¢†åŸŸã€‚

## ğŸ§  KVç¼“å­˜åŸºç¡€ï¼šä¸ºä»€ä¹ˆéœ€è¦ç¼“å­˜ï¼Ÿ

### è‡ªå›å½’æ¨ç†çš„ç“¶é¢ˆ

è®©æˆ‘ä»¬å…ˆç†è§£ä¸ºä»€ä¹ˆKVç¼“å­˜æ˜¯å¿…è¦çš„ï¼š

```python
# ä¼ ç»Ÿè‡ªå›å½’æ¨ç†ï¼ˆæ— ç¼“å­˜ï¼‰
def slow_autoregressive_generation(model, prompt, max_tokens=100):
    """æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ•´ä¸ªåºåˆ—çš„Attention"""
    sequence = prompt
    for _ in range(max_tokens):
        # é‡æ–°è®¡ç®—ä»å¼€å¤´åˆ°ç°åœ¨çš„æ‰€æœ‰Attention
        logits = model(sequence)  # O(nÂ²)å¤æ‚åº¦
        next_token = sample(logits)
        sequence += [next_token]
    return sequence

# ä¼˜åŒ–åçš„æ¨ç†ï¼ˆä½¿ç”¨KVç¼“å­˜ï¼‰
def fast_autoregressive_generation(model, prompt, max_tokens=100):
    """å¤ç”¨å·²è®¡ç®—çš„KVç¼“å­˜"""
    sequence = prompt
    kv_cache = {}

    # åˆå§‹å‰å‘ä¼ æ’­ï¼Œç¼“å­˜KV
    logits, kv_cache = model.forward_with_cache(prompt, kv_cache)
    next_token = sample(logits)
    sequence += [next_token]

    # åç»­æ­¥éª¤åªè®¡ç®—æ–°tokençš„Attention
    for _ in range(max_tokens - 1):
        # åªè®¡ç®—æ–°tokenå¯¹ä¹‹å‰åºåˆ—çš„Attention
        logits, kv_cache = model.forward_with_cache([next_token], kv_cache)
        next_token = sample(logits)
        sequence += [next_token]

    return sequence
```

### KVç¼“å­˜çš„æ ¸å¿ƒä»·å€¼

**æ—¶é—´å¤æ‚åº¦å¯¹æ¯”**ï¼š
- æ— ç¼“å­˜ï¼šO(nÂ²) Ã— tokens_generated
- æœ‰ç¼“å­˜ï¼šO(n) + O(nÂ²) Ã— tokens_generated

**å®é™…æ€§èƒ½æå‡**ï¼š
```python
import numpy as np
import matplotlib.pyplot as plt

def kv_cache_speedup_analysis():
    """åˆ†æKVç¼“å­˜çš„åŠ é€Ÿæ•ˆæœ"""
    sequence_lengths = [100, 500, 1000, 2000, 4000]

    # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´ï¼ˆç›¸å¯¹å•ä½ï¼‰
    without_cache_times = [n**2 for n in sequence_lengths]
    with_cache_times = [n + 100 for n in sequence_lengths]  # 100æ˜¯åˆå§‹è®¡ç®—æˆæœ¬

    speedups = [w / c for w, c in zip(without_cache_times, with_cache_times)]

    plt.figure(figsize=(12, 5))

    # è®¡ç®—æ—¶é—´å¯¹æ¯”
    plt.subplot(1, 2, 1)
    plt.plot(sequence_lengths, without_cache_times, 'r-', label='æ— KVç¼“å­˜', linewidth=3)
    plt.plot(sequence_lengths, with_cache_times, 'g-', label='æœ‰KVç¼“å­˜', linewidth=3)
    plt.xlabel('åºåˆ—é•¿åº¦')
    plt.ylabel('ç›¸å¯¹è®¡ç®—æ—¶é—´')
    plt.title('è®¡ç®—æ—¶é—´å¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # åŠ é€Ÿæ¯”
    plt.subplot(1, 2, 2)
    plt.plot(sequence_lengths, speedups, 'b-', linewidth=3)
    plt.xlabel('åºåˆ—é•¿åº¦')
    plt.ylabel('åŠ é€Ÿæ¯”')
    plt.title('KVç¼“å­˜å¸¦æ¥çš„åŠ é€Ÿæ•ˆæœ')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return speedups

speedups = kv_cache_speedup_analysis()
print(f"åºåˆ—é•¿åº¦4000æ—¶ï¼ŒKVç¼“å­˜å¸¦æ¥{speedups[-1]:.1f}å€åŠ é€Ÿ")
```

## ğŸ—ï¸ ä¼ ç»ŸKVç¼“å­˜ç®¡ç†ï¼šé™æ€åˆ†é…çš„å±€é™

### ç®€å•çš„é™æ€ç¼“å­˜

æœ€åŸºç¡€çš„KVç¼“å­˜å®ç°ï¼š

```python
class StaticKVCache:
    """ç®€å•çš„é™æ€KVç¼“å­˜å®ç°"""

    def __init__(self, max_seq_len, num_heads, head_dim, dtype=torch.float32):
        self.max_seq_len = max_seq_len
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dtype = dtype

        # é¢„åˆ†é…æœ€å¤§é•¿åº¦çš„ç¼“å­˜ç©ºé—´
        self.k_cache = torch.zeros(
            max_seq_len, num_heads, head_dim,
            dtype=dtype, device='cuda'
        )
        self.v_cache = torch.zeros(
            max_seq_len, num_heads, head_dim,
            dtype=dtype, device='cuda'
        )

        self.current_len = 0

    def update(self, new_k, new_v):
        """æ›´æ–°KVç¼“å­˜"""
        batch_size, new_seq_len, num_heads, head_dim = new_k.shape

        # æ£€æŸ¥å®¹é‡
        if self.current_len + new_seq_len > self.max_seq_len:
            raise ValueError("KVç¼“å­˜å®¹é‡ä¸è¶³")

        # å¤åˆ¶æ–°æ•°æ®
        start_idx = self.current_len
        self.k_cache[start_idx:start_idx + new_seq_len] = new_k
        self.v_cache[start_idx:start_idx + new_seq_len] = new_v

        self.current_len += new_seq_len

    def get_cache(self, seq_len=None):
        """è·å–æŒ‡å®šé•¿åº¦çš„ç¼“å­˜"""
        if seq_len is None:
            seq_len = self.current_len

        return (
            self.k_cache[:seq_len],
            self.v_cache[:seq_len]
        )

    def reset(self):
        """é‡ç½®ç¼“å­˜"""
        self.current_len = 0
        self.k_cache.zero_()
        self.v_cache.zero_()
```

### é™æ€ç¼“å­˜çš„è‡´å‘½ç¼ºé™·

è®©æˆ‘ä»¬åˆ†æé™æ€ç¼“å­˜çš„æ€§èƒ½é—®é¢˜ï¼š

```python
def static_cache_analysis():
    """åˆ†æé™æ€ç¼“å­˜çš„æ€§èƒ½é—®é¢˜"""

    # åœºæ™¯è®¾ç½®
    max_possible_length = 8192  # ç³»ç»Ÿæ”¯æŒçš„æœ€å¤§é•¿åº¦
    typical_lengths = [512, 1024, 2048, 4096]  # å…¸å‹ä½¿ç”¨é•¿åº¦
    head_dim = 128
    num_heads = 32
    dtype_size = 4  # float32

    def calculate_memory_usage(seq_len):
        """è®¡ç®—å†…å­˜ä½¿ç”¨"""
        k_memory = seq_len * num_heads * head_dim * dtype_size
        v_memory = seq_len * num_heads * head_dim * dtype_size
        return k_memory + v_memory

    def calculate_waste_ratio(actual_seq_len):
        """è®¡ç®—å†…å­˜æµªè´¹æ¯”ä¾‹"""
        max_memory = calculate_memory_usage(max_possible_length)
        actual_memory = calculate_memory_usage(actual_seq_len)
        waste_ratio = (max_memory - actual_memory) / max_memory
        return waste_ratio

    print("=== é™æ€KVç¼“å­˜å†…å­˜åˆ†æ ===")
    print(f"æœ€å¤§åºåˆ—é•¿åº¦: {max_possible_length}")
    print(f"æœ€å¤§å†…å­˜å ç”¨: {calculate_memory_usage(max_possible_length) / 1024 / 1024:.1f} MB")
    print()

    for seq_len in typical_lengths:
        actual_memory = calculate_memory_usage(seq_len)
        waste_ratio = calculate_waste_ratio(seq_len)

        print(f"å…¸å‹é•¿åº¦ {seq_len}:")
        print(f"  å®é™…éœ€è¦: {actual_memory / 1024 / 1024:.1f} MB")
        print(f"  å†…å­˜æµªè´¹: {waste_ratio * 100:.1f}%")
        print()

static_cache_analysis()
```

**è¾“å‡ºç»“æœ**ï¼š
```
=== é™æ€KVç¼“å­˜å†…å­˜åˆ†æ ===
æœ€å¤§åºåˆ—é•¿åº¦: 8192
æœ€å¤§å†…å­˜å ç”¨: 256.0 MB

å…¸å‹é•¿åº¦ 512:
  å®é™…éœ€è¦: 16.0 MB
  å†…å­˜æµªè´¹: 93.8%

å…¸å‹é•¿åº¦ 1024:
  å®é™…éœ€è¦: 32.0 MB
  å†…å­˜æµªè´¹: 87.5%

å…¸å‹é•¿åº¦ 2048:
  å®é™…éœ€è¦: 64.0 MB
  å†…å­˜æµªè´¹: 75.0%

å…¸å‹é•¿åº¦ 4096:
  å®é™…éœ€è¦: 128.0 MB
  å†…å­˜æµªè´¹: 50.0%
```

å¯ä»¥çœ‹å‡ºï¼Œé™æ€ç¼“å­˜åœ¨å®é™…ä½¿ç”¨ä¸­å­˜åœ¨ä¸¥é‡çš„å†…å­˜æµªè´¹é—®é¢˜ï¼

## ğŸ”„ åŠ¨æ€KVç¼“å­˜ï¼šè‡ªé€‚åº”å†…å­˜ç®¡ç†

### åŠ¨æ€å¢é•¿ç­–ç•¥

```python
class DynamicKVCache:
    """åŠ¨æ€å¢é•¿çš„KVç¼“å­˜"""

    def __init__(self, initial_capacity=512, growth_factor=1.5, max_capacity=8192):
        self.initial_capacity = initial_capacity
        self.growth_factor = growth_factor
        self.max_capacity = max_capacity

        # å½“å‰å®¹é‡å’Œå®é™…ä½¿ç”¨é•¿åº¦
        self.current_capacity = initial_capacity
        self.current_length = 0

        # åŠ¨æ€åˆ†é…çš„ç¼“å­˜
        self.k_cache = None
        self.v_cache = None

        # åˆå§‹åˆ†é…
        self._allocate_cache()

        # ç»Ÿè®¡ä¿¡æ¯
        self.resize_count = 0
        self.total_allocated_memory = 0

    def _allocate_cache(self):
        """åˆ†é…æŒ‡å®šå®¹é‡çš„ç¼“å­˜"""
        self.k_cache = torch.zeros(
            self.current_capacity, self.num_heads, self.head_dim,
            dtype=self.dtype, device='cuda'
        )
        self.v_cache = torch.zeros(
            self.current_capacity, self.num_heads, self.head_dim,
            dtype=self.dtype, device='cuda'
        )

        memory_mb = (self.current_capacity * self.num_heads * self.head_dim *
                    2 * 4) / 1024 / 1024  # 2 for K+V, 4 for float32
        self.total_allocated_memory += memory_mb

    def _resize_if_needed(self, required_length):
        """æ ¹æ®éœ€è¦è°ƒæ•´ç¼“å­˜å¤§å°"""
        if required_length <= self.current_capacity:
            return  # ä¸éœ€è¦è°ƒæ•´

        # è®¡ç®—æ–°å®¹é‡
        new_capacity = min(
            int(self.current_capacity * self.growth_factor),
            self.max_capacity
        )

        if required_length > new_capacity:
            new_capacity = required_length

        if new_capacity > self.max_capacity:
            raise ValueError(f"è¶…è¿‡æœ€å¤§å®¹é‡é™åˆ¶: {required_length} > {self.max_capacity}")

        # ä¿å­˜æ—§æ•°æ®
        old_k = self.k_cache[:self.current_length].clone()
        old_v = self.v_cache[:self.current_length].clone()

        # åˆ†é…æ–°ç¼“å­˜
        self.current_capacity = new_capacity
        self._allocate_cache()

        # æ¢å¤æ•°æ®
        self.k_cache[:self.current_length] = old_k
        self.v_cache[:self.current_length] = old_v

        self.resize_count += 1

    def update(self, new_k, new_v):
        """æ›´æ–°KVç¼“å­˜"""
        batch_size, new_seq_len, num_heads, head_dim = new_k.shape

        # è®¾ç½®ç»´åº¦ä¿¡æ¯ï¼ˆç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ï¼‰
        if self.k_cache is None:
            self.num_heads = num_heads
            self.head_dim = head_dim
            self.dtype = new_k.dtype
            self._allocate_cache()

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å®¹
        required_length = self.current_length + new_seq_len
        self._resize_if_needed(required_length)

        # å¤åˆ¶æ–°æ•°æ®
        start_idx = self.current_length
        self.k_cache[start_idx:start_idx + new_seq_len] = new_k
        self.v_cache[start_idx:start_idx + new_seq_len] = new_v

        self.current_length += new_seq_len

    def get_cache(self, seq_len=None):
        """è·å–æŒ‡å®šé•¿åº¦çš„ç¼“å­˜"""
        if seq_len is None:
            seq_len = self.current_length

        return (
            self.k_cache[:seq_len],
            self.v_cache[:seq_len]
        )

    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "current_length": self.current_length,
            "current_capacity": self.current_capacity,
            "utilization": self.current_length / self.current_capacity,
            "resize_count": self.resize_count,
            "total_allocated_memory_mb": self.total_allocated_memory
        }
```

### åŠ¨æ€ç¼“å­˜çš„æ€§èƒ½åˆ†æ

```python
def dynamic_cache_performance_test():
    """æµ‹è¯•åŠ¨æ€ç¼“å­˜çš„æ€§èƒ½"""

    # æ¨¡æ‹ŸçœŸå®ä½¿ç”¨åœºæ™¯
    usage_patterns = [
        # (åœºæ™¯åç§°, åºåˆ—é•¿åº¦åˆ—è¡¨)
        ("çŸ­å¯¹è¯", [100, 200, 300, 400, 500]),
        ("é•¿æ–‡æ¡£QA", [1000, 1500, 2000, 2500, 3000]),
        ("æµå¼æ¨ç†", [50, 100, 150, 200, 250, 300, 350, 400]),
        ("æ··åˆåœºæ™¯", [200, 1800, 300, 1500, 600, 1200, 400, 800])
    ]

    results = {}

    for scenario_name, seq_lengths in usage_patterns:
        # æµ‹è¯•é™æ€ç¼“å­˜
        static_cache = StaticKVCache(max_seq_len=4000, num_heads=32, head_dim=128)
        static_memory = 4000 * 32 * 128 * 2 * 4 / 1024 / 1024  # å›ºå®šå†…å­˜ä½¿ç”¨

        # æµ‹è¯•åŠ¨æ€ç¼“å­˜
        dynamic_cache = DynamicKVCache(initial_capacity=256, growth_factor=1.5)

        total_seq_len = 0
        for seq_len in seq_lengths:
            # æ¨¡æ‹ŸKVæ›´æ–°
            new_k = torch.randn(1, seq_len, 32, 128)
            new_v = torch.randn(1, seq_len, 32, 128)

            static_cache.update(new_k, new_v)
            dynamic_cache.update(new_k, new_v)

            total_seq_len += seq_len

        dynamic_stats = dynamic_cache.get_stats()
        dynamic_memory = dynamic_stats["total_allocated_memory_mb"]

        results[scenario_name] = {
            "static_memory_mb": static_memory,
            "dynamic_memory_mb": dynamic_memory,
            "memory_saving_mb": static_memory - dynamic_memory,
            "memory_saving_ratio": (static_memory - dynamic_memory) / static_memory,
            "dynamic_utilization": dynamic_stats["utilization"],
            "resize_count": dynamic_stats["resize_count"]
        }

    # æ‰“å°ç»“æœ
    print("=== åŠ¨æ€KVç¼“å­˜æ€§èƒ½åˆ†æ ===")
    for scenario, metrics in results.items():
        print(f"\n{scenario}:")
        print(f"  é™æ€ç¼“å­˜: {metrics['static_memory_mb']:.1f} MB")
        print(f"  åŠ¨æ€ç¼“å­˜: {metrics['dynamic_memory_mb']:.1f} MB")
        print(f"  å†…å­˜èŠ‚çœ: {metrics['memory_saving_mb']:.1f} MB ({metrics['memory_saving_ratio']*100:.1f}%)")
        print(f"  åŠ¨æ€åˆ©ç”¨ç‡: {metrics['dynamic_utilization']:.2f}")
        print(f"  æ‰©å®¹æ¬¡æ•°: {metrics['resize_count']}")

    return results

dynamic_results = dynamic_cache_performance_test()
```

## ğŸ¯ æ™ºèƒ½KVç¼“å­˜ï¼šé¢„æµ‹æ€§ä¼˜åŒ–

### ä½¿ç”¨æ¨¡å¼é¢„æµ‹

```python
class PredictiveKVCache:
    """å…·æœ‰é¢„æµ‹èƒ½åŠ›çš„æ™ºèƒ½KVç¼“å­˜"""

    def __init__(self, initial_capacity=512, history_window=10):
        self.initial_capacity = initial_capacity
        self.history_window = history_window

        # å†å²ä½¿ç”¨æ¨¡å¼
        self.usage_history = []
        self.growth_patterns = []

        # é¢„æµ‹æ¨¡å‹å‚æ•°
        self.avg_growth_rate = 1.0
        self.seq_variance = 0.0

        # ç¼“å­˜å®ä¾‹
        self.cache = DynamicKVCache(initial_capacity=initial_capacity)

    def _update_usage_pattern(self, new_seq_len):
        """æ›´æ–°ä½¿ç”¨æ¨¡å¼"""
        self.usage_history.append(new_seq_len)

        # ä¿æŒå†å²çª—å£å¤§å°
        if len(self.usage_history) > self.history_window:
            self.usage_history.pop(0)

        # è®¡ç®—å¢é•¿æ¨¡å¼
        if len(self.usage_history) >= 2:
            growth_rates = []
            for i in range(1, len(self.usage_history)):
                if self.usage_history[i-1] > 0:
                    growth_rate = self.usage_history[i] / self.usage_history[i-1]
                    growth_rates.append(growth_rate)

            if growth_rates:
                self.avg_growth_rate = np.mean(growth_rates)
                self.seq_variance = np.var(growth_rates)

    def _predict_next_length(self):
        """é¢„æµ‹ä¸‹ä¸€ä¸ªåºåˆ—é•¿åº¦"""
        if len(self.usage_history) < 2:
            return self.cache.current_length + 100  # é»˜è®¤é¢„æµ‹

        # åŸºäºå†å²å¢é•¿æ¨¡å¼é¢„æµ‹
        last_length = self.usage_history[-1]

        # è€ƒè™‘å¢é•¿ç‡å’Œæ–¹å·®
        if self.seq_variance < 0.1:  # ç¨³å®šå¢é•¿
            predicted_growth = self.avg_growth_rate
        else:  # ä¸ç¨³å®šå¢é•¿ï¼Œé‡‡ç”¨ä¿å®ˆé¢„æµ‹
            predicted_growth = min(self.avg_growth_rate, 1.2)

        predicted_length = int(last_length * predicted_growth)

        # æ·»åŠ å®‰å…¨è¾¹ç•Œ
        safety_margin = int(predicted_length * 0.1)
        return predicted_length + safety_margin

    def _preemptive_resize(self):
        """é¢„é˜²æ€§è°ƒæ•´ç¼“å­˜å¤§å°"""
        predicted_length = self._predict_next_length()

        # å¦‚æœé¢„æµ‹é•¿åº¦è¶…è¿‡å½“å‰å®¹é‡ï¼Œæå‰æ‰©å®¹
        if predicted_length > self.cache.current_capacity:
            growth_factor = predicted_length / self.cache.current_capacity
            new_capacity = int(self.cache.current_capacity * max(growth_factor, 1.3))

            if new_capacity <= self.cache.max_capacity:
                self.cache._resize_if_needed(new_capacity)

    def update(self, new_k, new_v):
        """æ™ºèƒ½æ›´æ–°KVç¼“å­˜"""
        batch_size, new_seq_len, num_heads, head_dim = new_k.shape

        # æ›´æ–°ä½¿ç”¨æ¨¡å¼
        self._update_usage_pattern(new_seq_len)

        # é¢„é˜²æ€§è°ƒæ•´
        self._preemptive_resize()

        # å®é™…æ›´æ–°
        self.cache.update(new_k, new_v)

    def get_cache(self, seq_len=None):
        """è·å–ç¼“å­˜"""
        return self.cache.get_cache(seq_len)

    def get_prediction_stats(self):
        """è·å–é¢„æµ‹ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "avg_growth_rate": self.avg_growth_rate,
            "seq_variance": self.seq_variance,
            "history_length": len(self.usage_history),
            "last_predictions": self.usage_history[-5:] if self.usage_history else []
        }
```

### æ™ºèƒ½ç¼“å­˜çš„æ•ˆæœéªŒè¯

```python
def predictive_cache_evaluation():
    """è¯„ä¼°é¢„æµ‹æ€§ç¼“å­˜çš„æ•ˆæœ"""

    # æ¨¡æ‹Ÿå¤æ‚çš„ä½¿ç”¨æ¨¡å¼
    scenarios = {
        "ç¨³å®šå¢é•¿": [100, 120, 144, 173, 207, 249, 299, 359, 430, 516],
        "çªå‘å¢é•¿": [100, 100, 100, 100, 800, 850, 900, 950, 1000, 1050],
        "å‘¨æœŸæ€§": [200, 400, 200, 400, 200, 400, 200, 400, 200, 400],
        "éšæœºæ³¢åŠ¨": [150, 280, 120, 390, 210, 180, 420, 310, 160, 380]
    }

    evaluation_results = {}

    for scenario_name, seq_lengths in scenarios.items():
        print(f"\n=== æµ‹è¯•åœºæ™¯: {scenario_name} ===")

        # æ ‡å‡†åŠ¨æ€ç¼“å­˜
        dynamic_cache = DynamicKVCache(initial_capacity=200)
        dynamic_resizes = 0

        # é¢„æµ‹æ€§ç¼“å­˜
        predictive_cache = PredictiveKVCache(initial_capacity=200)
        predictive_resizes = 0

        for i, seq_len in enumerate(seq_lengths):
            # æ¨¡æ‹ŸKVæ›´æ–°
            new_k = torch.randn(1, seq_len, 32, 128)
            new_v = torch.randn(1, seq_len, 32, 128)

            # è®°å½•æ‰©å®¹å‰çš„æ¬¡æ•°
            dynamic_resizes_before = dynamic_cache.resize_count
            predictive_resizes_before = predictive_cache.cache.resize_count

            # æ›´æ–°ç¼“å­˜
            dynamic_cache.update(new_k, new_v)
            predictive_cache.update(new_k, new_v)

            # è®°å½•æ–°çš„æ‰©å®¹æ¬¡æ•°
            if dynamic_cache.resize_count > dynamic_resizes_before:
                dynamic_resizes += 1
            if predictive_cache.cache.resize_count > predictive_resizes_before:
                predictive_resizes += 1

            print(f"æ­¥éª¤ {i+1}: åºåˆ—é•¿åº¦={seq_len}, "
                  f"åŠ¨æ€æ‰©å®¹={dynamic_cache.resize_count}, "
                  f"é¢„æµ‹æ‰©å®¹={predictive_cache.cache.resize_count}")

        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        dynamic_stats = dynamic_cache.get_stats()
        pred_stats = predictive_cache.cache.get_stats()
        pred_prediction_stats = predictive_cache.get_prediction_stats()

        evaluation_results[scenario_name] = {
            "dynamic_resizes": dynamic_resizes,
            "predictive_resizes": predictive_resizes,
            "resize_reduction": dynamic_resizes - predictive_resizes,
            "dynamic_utilization": dynamic_stats["utilization"],
            "predictive_utilization": pred_stats["utilization"],
            "avg_growth_rate": pred_prediction_stats["avg_growth_rate"],
            "stability": 1.0 / (1.0 + pred_prediction_stats["seq_variance"])
        }

    # æ€»ç»“åˆ†æ
    print("\n=== é¢„æµ‹æ€§ç¼“å­˜æ•ˆæœæ€»ç»“ ===")
    for scenario, results in evaluation_results.items():
        print(f"\n{scenario}:")
        print(f"  æ‰©å®¹æ¬¡æ•°å‡å°‘: {results['resize_reduction']} æ¬¡")
        print(f"  åŠ¨æ€ç¼“å­˜åˆ©ç”¨ç‡: {results['dynamic_utilization']:.2f}")
        print(f"  é¢„æµ‹ç¼“å­˜åˆ©ç”¨ç‡: {results['predictive_utilization']:.2f}")
        print(f"  å¹³å‡å¢é•¿ç‡: {results['avg_growth_rate']:.2f}")
        print(f"  æ¨¡å¼ç¨³å®šæ€§: {results['stability']:.2f}")

    return evaluation_results

predictive_results = predictive_cache_evaluation()
```

## ğŸš€ é«˜çº§KVç¼“å­˜ä¼˜åŒ–æŠ€æœ¯

### 1. åˆ†å±‚ç¼“å­˜ç­–ç•¥

```python
class HierarchicalKVCache:
    """åˆ†å±‚KVç¼“å­˜ - å¿«æ…¢åˆ†ç¦»"""

    def __init__(self, fast_capacity=1024, slow_capacity=8192):
        # å¿«é€Ÿç¼“å­˜ï¼ˆGPUå†…å­˜ï¼Œè®¿é—®é€Ÿåº¦å¿«ï¼‰
        self.fast_cache = DynamicKVCache(
            initial_capacity=fast_capacity,
            max_capacity=fast_capacity
        )

        # æ…¢é€Ÿç¼“å­˜ï¼ˆCPUå†…å­˜æˆ–æ›´æ…¢çš„å­˜å‚¨ï¼Œå®¹é‡å¤§ï¼‰
        self.slow_capacity = slow_capacity
        self.slow_k_cache = []
        self.slow_v_cache = []

        # è®¿é—®é¢‘ç‡ç»Ÿè®¡
        self.access_frequency = {}

    def _is_hot_token(self, token_idx):
        """åˆ¤æ–­tokenæ˜¯å¦ä¸ºçƒ­ç‚¹ï¼ˆé¢‘ç¹è®¿é—®ï¼‰"""
        if token_idx not in self.access_frequency:
            return False

        # ç®€å•çš„çƒ­ç‚¹åˆ¤æ–­é€»è¾‘
        recent_accesses = self.access_frequency[token_idx]
        return len(recent_accesses) > 5  # è®¿é—®è¶…è¿‡5æ¬¡è®¤ä¸ºæ˜¯çƒ­ç‚¹

    def _update_access_frequency(self, token_idx):
        """æ›´æ–°è®¿é—®é¢‘ç‡"""
        if token_idx not in self.access_frequency:
            self.access_frequency[token_idx] = []

        self.access_frequency[token_idx].append(time.time())

        # åªä¿ç•™æœ€è¿‘çš„è®¿é—®è®°å½•
        if len(self.access_frequency[token_idx]) > 10:
            self.access_frequency[token_idx] = self.access_frequency[token_idx][-10:]

    def update(self, new_k, new_v):
        """æ›´æ–°åˆ†å±‚ç¼“å­˜"""
        # é¦–å…ˆæ›´æ–°å¿«é€Ÿç¼“å­˜
        try:
            self.fast_cache.update(new_k, new_v)
        except ValueError as e:
            # å¿«é€Ÿç¼“å­˜æ»¡äº†ï¼Œéœ€è¦å¤„ç†
            self._evict_to_slow_cache()
            self.fast_cache.update(new_k, new_v)

    def _evict_to_slow_cache(self):
        """å°†éƒ¨åˆ†æ•°æ®ä»å¿«é€Ÿç¼“å­˜è¿ç§»åˆ°æ…¢é€Ÿç¼“å­˜"""
        # é€‰æ‹©è®¿é—®é¢‘ç‡æœ€ä½çš„tokenè¿›è¡Œè¿ç§»
        fast_k, fast_v = self.fast_cache.get_cache()

        # ç®€å•çš„LRUç­–ç•¥ï¼šè¿ç§»å‰ä¸€åŠæ•°æ®
        evict_point = len(fast_k) // 2

        evicted_k = fast_k[:evict_point]
        evicted_v = fast_v[:evict_point]

        # è¿ç§»åˆ°æ…¢é€Ÿç¼“å­˜
        self.slow_k_cache.append(evicted_k.cpu())
        self.slow_v_cache.append(evicted_v.cpu())

        # é‡æ–°æ„å»ºå¿«é€Ÿç¼“å­˜
        remaining_k = fast_k[evict_point:]
        remaining_v = fast_v[evict_point:]

        self.fast_cache.reset()
        if len(remaining_k) > 0:
            self.fast_cache.update(remaining_k.unsqueeze(0), remaining_v.unsqueeze(0))

    def get_cache(self, seq_len=None):
        """è·å–ç¼“å­˜ï¼Œè‡ªåŠ¨åˆå¹¶å¿«é€Ÿå’Œæ…¢é€Ÿç¼“å­˜"""
        if seq_len is None:
            seq_len = self.fast_cache.current_length + sum(len(k) for k in self.slow_k_cache)

        # é¦–å…ˆä»å¿«é€Ÿç¼“å­˜è·å–
        fast_k, fast_v = self.fast_cache.get_cache()

        # å¦‚æœéœ€è¦æ›´å¤šæ•°æ®ï¼Œä»æ…¢é€Ÿç¼“å­˜è·å–
        if len(fast_k) < seq_len:
            # åˆå¹¶æ…¢é€Ÿç¼“å­˜æ•°æ®
            slow_k_combined = torch.cat(self.slow_k_cache, dim=0)
            slow_v_combined = torch.cat(self.slow_v_cache, dim=0)

            # åˆå¹¶å¿«é€Ÿå’Œæ…¢é€Ÿç¼“å­˜
            combined_k = torch.cat([slow_k_combined, fast_k], dim=0)
            combined_v = torch.cat([slow_v_combined, fast_v], dim=0)

            return combined_k[:seq_len], combined_v[:seq_len]

        return fast_k[:seq_len], fast_v[:seq_len]
```

### 2. å‹ç¼©ç¼“å­˜æŠ€æœ¯

```python
class CompressedKVCache:
    """å‹ç¼©KVç¼“å­˜ - å‡å°‘å†…å­˜å ç”¨"""

    def __init__(self, compression_ratio=0.5, max_seq_len=8192):
        self.compression_ratio = compression_ratio
        self.max_seq_len = max_seq_len

        # åŸå§‹ç¼“å­˜
        self.k_cache = None
        self.v_cache = None

        # å‹ç¼©ç›¸å…³
        self.compression_indices = []
        self.importance_scores = []

    def _calculate_importance_scores(self, k, v):
        """è®¡ç®—tokençš„é‡è¦æ€§åˆ†æ•°"""
        # åŸºäºAttentionæƒé‡çš„é‡è¦æ€§è¯„ä¼°
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•

        # è®¡ç®—æ¯ä¸ªtokenå‘é‡çš„èŒƒæ•°ä½œä¸ºé‡è¦æ€§æŒ‡æ ‡
        k_norm = torch.norm(k, dim=-1)  # [seq_len, num_heads]
        v_norm = torch.norm(v, dim=-1)  # [seq_len, num_heads]

        # åˆå¹¶Kå’ŒVçš„é‡è¦æ€§
        importance = (k_norm + v_norm) / 2
        token_importance = torch.mean(importance, dim=1)  # [seq_len]

        return token_importance

    def _select_important_tokens(self, importance_scores):
        """é€‰æ‹©é‡è¦çš„tokenä¿ç•™"""
        num_tokens = len(importance_scores)
        num_keep = int(num_tokens * self.compression_ratio)

        # é€‰æ‹©æœ€é‡è¦çš„token
        _, top_indices = torch.topk(importance_scores, num_keep)
        sorted_indices = torch.sort(top_indices)[0]

        return sorted_indices

    def update(self, new_k, new_v):
        """æ›´æ–°å‹ç¼©ç¼“å­˜"""
        # è®¡ç®—é‡è¦æ€§åˆ†æ•°
        importance = self._calculate_importance_scores(new_k, new_v)

        # é€‰æ‹©é‡è¦token
        important_indices = self._select_important_tokens(importance)

        # å‹ç¼©å­˜å‚¨
        compressed_k = new_k[important_indices]
        compressed_v = new_v[important_indices]

        # æ›´æ–°ç¼“å­˜
        if self.k_cache is None:
            self.k_cache = compressed_k
            self.v_cache = compressed_v
        else:
            self.k_cache = torch.cat([self.k_cache, compressed_k], dim=1)
            self.v_cache = torch.cat([self.v_cache, compressed_v], dim=1)

        # è®°å½•å‹ç¼©ä¿¡æ¯
        self.compression_indices.append(important_indices)
        self.importance_scores.append(importance)

        # é™åˆ¶æœ€å¤§é•¿åº¦
        if self.k_cache.shape[1] > self.max_seq_len:
            self.k_cache = self.k_cache[:, -self.max_seq_len:]
            self.v_cache = self.v_cache[:, -self.max_seq_len:]

    def get_cache(self):
        """è·å–å‹ç¼©ç¼“å­˜"""
        return self.k_cache, self.v_cache

    def get_compression_stats(self):
        """è·å–å‹ç¼©ç»Ÿè®¡ä¿¡æ¯"""
        total_original_tokens = sum(len(indices) * (1/self.compression_ratio)
                                  for indices in self.compression_indices)
        total_compressed_tokens = sum(len(indices) for indices in self.compression_indices)

        compression_ratio = total_compressed_tokens / total_original_tokens if total_original_tokens > 0 else 0

        return {
            "compression_ratio": compression_ratio,
            "original_tokens": int(total_original_tokens),
            "compressed_tokens": total_compressed_tokens,
            "memory_saving": (1 - compression_ratio) * 100
        }
```

### 3. å¼‚æ­¥ç¼“å­˜ç®¡ç†

```python
import asyncio
import threading
from concurrent.futures import ThreadPoolExecutor

class AsyncKVCache:
    """å¼‚æ­¥KVç¼“å­˜ç®¡ç†"""

    def __init__(self, cache_manager, max_workers=2):
        self.cache_manager = cache_manager
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.pending_operations = {}

    async def update_async(self, cache_id, new_k, new_v):
        """å¼‚æ­¥æ›´æ–°ç¼“å­˜"""
        loop = asyncio.get_event_loop()

        # æäº¤å¼‚æ­¥ä»»åŠ¡
        future = loop.run_in_executor(
            self.executor,
            self._sync_update,
            cache_id, new_k, new_v
        )

        self.pending_operations[cache_id] = future

        # ç­‰å¾…å®Œæˆ
        result = await future
        del self.pending_operations[cache_id]

        return result

    def _sync_update(self, cache_id, new_k, new_v):
        """åŒæ­¥æ›´æ–°æ“ä½œ"""
        return self.cache_manager.update(cache_id, new_k, new_v)

    async def prefetch_cache(self, cache_id, predicted_seq_len):
        """é¢„å–ç¼“å­˜"""
        if cache_id not in self.pending_operations:
            # å¼‚æ­¥é¢„å–
            future = asyncio.create_task(
                self._prefetch_operation(cache_id, predicted_seq_len)
            )
            self.pending_operations[cache_id] = future

    async def _prefetch_operation(self, cache_id, predicted_seq_len):
        """é¢„å–æ“ä½œ"""
        # æ¨¡æ‹Ÿé¢„å–å»¶è¿Ÿ
        await asyncio.sleep(0.01)

        # å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä»ç£ç›˜æˆ–ç½‘ç»œåŠ è½½æ•°æ®
        return {"cache_id": cache_id, "data": f"prefetched_data_{predicted_seq_len}"}

    async def get_cache_async(self, cache_id):
        """å¼‚æ­¥è·å–ç¼“å­˜"""
        # ç­‰å¾…å¾…å¤„ç†çš„æ“ä½œå®Œæˆ
        if cache_id in self.pending_operations:
            await self.pending_operations[cache_id]

        # è·å–ç¼“å­˜
        return self.cache_manager.get_cache(cache_id)
```

## ğŸ“Š KVç¼“å­˜ä¼˜åŒ–æ•ˆæœå¯¹æ¯”

### ç»¼åˆæ€§èƒ½æµ‹è¯•

```python
def comprehensive_kv_cache_benchmark():
    """ç»¼åˆKVç¼“å­˜æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    # æµ‹è¯•å‚æ•°
    test_scenarios = [
        ("çŸ­å¯¹è¯", [50, 100, 150, 200, 250, 300]),
        ("é•¿æ–‡æ¡£", [1000, 1500, 2000, 2500, 3000, 3500]),
        ("æµå¼å¤„ç†", [20, 40, 60, 80, 100, 120, 140, 160, 180, 200]),
        ("çªå‘è´Ÿè½½", [100, 100, 100, 1000, 100, 100, 1000, 100, 100])
    ]

    cache_types = {
        "é™æ€ç¼“å­˜": lambda: StaticKVCache(max_seq_len=4000, num_heads=32, head_dim=128),
        "åŠ¨æ€ç¼“å­˜": lambda: DynamicKVCache(initial_capacity=256, growth_factor=1.5),
        "é¢„æµ‹ç¼“å­˜": lambda: PredictiveKVCache(initial_capacity=256),
        "åˆ†å±‚ç¼“å­˜": lambda: HierarchicalKVCache(fast_capacity=512, slow_capacity=4096),
        "å‹ç¼©ç¼“å­˜": lambda: CompressedKVCache(compression_ratio=0.7)
    }

    results = {}

    for scenario_name, seq_lengths in test_scenarios:
        print(f"\n=== æµ‹è¯•åœºæ™¯: {scenario_name} ===")
        scenario_results = {}

        for cache_name, cache_factory in cache_types.items():
            cache = cache_factory()

            # æ€§èƒ½æŒ‡æ ‡
            total_memory_mb = 0
            resize_operations = 0
            peak_utilization = 0

            start_time = time.time()

            for seq_len in seq_lengths:
                # æ¨¡æ‹ŸKVç”Ÿæˆ
                new_k = torch.randn(1, seq_len, 32, 128)
                new_v = torch.randn(1, seq_len, 32, 128)

                # æ›´æ–°ç¼“å­˜
                cache.update(new_k, new_v)

                # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
                if hasattr(cache, 'get_stats'):
                    stats = cache.get_stats()
                    utilization = stats.get('utilization', 0)
                    peak_utilization = max(peak_utilization, utilization)
                    resize_operations = stats.get('resize_count', 0)
                    total_memory_mb = stats.get('total_allocated_memory_mb', 0)
                else:
                    # é™æ€ç¼“å­˜çš„å›ºå®šå†…å­˜
                    total_memory_mb = 4000 * 32 * 128 * 2 * 4 / 1024 / 1024
                    peak_utilization = sum(seq_lengths) / 4000

            end_time = time.time()
            processing_time = end_time - start_time

            scenario_results[cache_name] = {
                "memory_mb": total_memory_mb,
                "resize_ops": resize_operations,
                "peak_utilization": peak_utilization,
                "processing_time_ms": processing_time * 1000,
                "efficiency_score": peak_utilization / (total_memory_mb / 1000)  # åˆ©ç”¨ç‡/å†…å­˜ä½¿ç”¨
            }

            print(f"  {cache_name}: å†…å­˜={total_memory_mb:.1f}MB, "
                  f"æ‰©å®¹={resize_operations}æ¬¡, åˆ©ç”¨ç‡={peak_utilization:.2f}, "
                  f"æ—¶é—´={processing_time*1000:.1f}ms")

        results[scenario_name] = scenario_results

    # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    create_performance_comparison_charts(results)

    return results

def create_performance_comparison_charts(results):
    """åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
    import matplotlib.pyplot as plt

    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    scenarios = list(results.keys())
    cache_types = list(results[scenarios[0]].keys())

    # 1. å†…å­˜ä½¿ç”¨å¯¹æ¯”
    ax1 = axes[0, 0]
    for cache_type in cache_types:
        memories = [results[scenario][cache_type]["memory_mb"] for scenario in scenarios]
        ax1.plot(scenarios, memories, marker='o', label=cache_type, linewidth=2)

    ax1.set_title('å†…å­˜ä½¿ç”¨å¯¹æ¯”')
    ax1.set_ylabel('å†…å­˜ä½¿ç”¨ (MB)')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # 2. ç¼“å­˜åˆ©ç”¨ç‡å¯¹æ¯”
    ax2 = axes[0, 1]
    for cache_type in cache_types:
        utilizations = [results[scenario][cache_type]["peak_utilization"] for scenario in scenarios]
        ax2.plot(scenarios, utilizations, marker='s', label=cache_type, linewidth=2)

    ax2.set_title('ç¼“å­˜åˆ©ç”¨ç‡å¯¹æ¯”')
    ax2.set_ylabel('å³°å€¼åˆ©ç”¨ç‡')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    # 3. æ‰©å®¹æ¬¡æ•°å¯¹æ¯”
    ax3 = axes[1, 0]
    for cache_type in cache_types:
        resizes = [results[scenario][cache_type]["resize_ops"] for scenario in scenarios]
        ax3.plot(scenarios, resizes, marker='^', label=cache_type, linewidth=2)

    ax3.set_title('æ‰©å®¹æ“ä½œæ¬¡æ•°å¯¹æ¯”')
    ax3.set_ylabel('æ‰©å®¹æ¬¡æ•°')
    ax3.legend()
    ax3.grid(True, alpha=0.3)

    # 4. æ•ˆç‡è¯„åˆ†å¯¹æ¯”
    ax4 = axes[1, 1]
    for cache_type in cache_types:
        efficiencies = [results[scenario][cache_type]["efficiency_score"] for scenario in scenarios]
        ax4.plot(scenarios, efficiencies, marker='d', label=cache_type, linewidth=2)

    ax4.set_title('ç»¼åˆæ•ˆç‡è¯„åˆ†å¯¹æ¯”')
    ax4.set_ylabel('æ•ˆç‡è¯„åˆ†')
    ax4.legend()
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# è¿è¡Œç»¼åˆæµ‹è¯•
benchmark_results = comprehensive_kv_cache_benchmark()
```

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯ä¸æœ€ä½³å®è·µ

### 1. èŠå¤©æœºå™¨äººåœºæ™¯

```python
class ChatbotKVCacheManager:
    """ä¸“ä¸ºèŠå¤©æœºå™¨äººä¼˜åŒ–çš„KVç¼“å­˜ç®¡ç†"""

    def __init__(self, max_conversation_length=4096):
        # ä¸ºä¸åŒç±»å‹çš„å¯¹è¯è®¾ç½®ä¸åŒçš„ç¼“å­˜ç­–ç•¥
        self.caches = {
            "short_chat": PredictiveKVCache(initial_capacity=256, history_window=5),
            "long_document": HierarchicalKVCache(fast_capacity=1024, slow_capacity=4096),
            "code_generation": DynamicKVCache(initial_capacity=512, growth_factor=2.0),
            "creative_writing": CompressedKVCache(compression_ratio=0.8)
        }

        self.current_conversation_type = "short_chat"
        self.conversation_history = []

    def classify_conversation_type(self, recent_messages):
        """æ ¹æ®æœ€è¿‘æ¶ˆæ¯åˆ†ç±»å¯¹è¯ç±»å‹"""
        # ç®€å•çš„å¯å‘å¼åˆ†ç±»
        total_length = sum(len(msg) for msg in recent_messages)
        code_keywords = ["def", "function", "class", "import", "for", "while"]
        creative_keywords = ["story", "character", "plot", "imagine", "create"]

        has_code = any(keyword in " ".join(recent_messages).lower() for keyword in code_keywords)
        has_creative = any(keyword in " ".join(recent_messages).lower() for keyword in creative_keywords)

        if has_code:
            return "code_generation"
        elif has_creative:
            return "creative_writing"
        elif total_length > 2000:
            return "long_document"
        else:
            return "short_chat"

    def update_conversation(self, new_message, new_k, new_v):
        """æ›´æ–°å¯¹è¯ç¼“å­˜"""
        self.conversation_history.append(new_message)

        # é‡æ–°åˆ†ç±»å¯¹è¯ç±»å‹
        recent_messages = self.conversation_history[-10:]  # æœ€è¿‘10æ¡æ¶ˆæ¯
        new_type = self.classify_conversation_type(recent_messages)

        # å¦‚æœå¯¹è¯ç±»å‹æ”¹å˜ï¼Œåˆ‡æ¢ç¼“å­˜
        if new_type != self.current_conversation_type:
            self._migrate_to_new_cache(new_type)
            self.current_conversation_type = new_type

        # æ›´æ–°å½“å‰ç¼“å­˜
        current_cache = self.caches[self.current_conversation_type]
        current_cache.update(new_k, new_v)

    def _migrate_to_new_cache(self, new_type):
        """è¿ç§»åˆ°æ–°çš„ç¼“å­˜ç±»å‹"""
        # è·å–å½“å‰ç¼“å­˜æ•°æ®
        old_cache = self.caches[self.current_conversation_type]
        old_k, old_v = old_cache.get_cache()

        # è¿ç§»åˆ°æ–°ç¼“å­˜
        new_cache = self.caches[new_type]
        if len(old_k) > 0:
            new_cache.update(old_k.unsqueeze(0), old_v.unsqueeze(0))

        # é‡ç½®æ—§ç¼“å­˜
        if hasattr(old_cache, 'reset'):
            old_cache.reset()

    def get_optimal_cache(self):
        """è·å–æœ€ä¼˜çš„ç¼“å­˜"""
        return self.caches[self.current_conversation_type].get_cache()
```

### 2. å®æ—¶æµå¼æ¨ç†

```python
class StreamingKVCache:
    """æµå¼æ¨ç†ä¸“ç”¨çš„KVç¼“å­˜"""

    def __init__(self, window_size=1024, overlap_size=128):
        self.window_size = window_size
        self.overlap_size = overlap_size

        # æ»‘åŠ¨çª—å£ç¼“å­˜
        self.active_cache = DynamicKVCache(initial_capacity=window_size)
        self.retired_caches = []

        # çª—å£ç®¡ç†
        self.current_position = 0
        self.total_processed = 0

    def update_streaming(self, new_k, new_v):
        """æµå¼æ›´æ–°ç¼“å­˜"""
        self.total_processed += new_k.shape[1]

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ»‘åŠ¨çª—å£
        if self.active_cache.current_length + new_k.shape[1] > self.window_size:
            self._slide_window()

        # æ›´æ–°æ´»åŠ¨ç¼“å­˜
        self.active_cache.update(new_k, new_v)

    def _slide_window(self):
        """æ»‘åŠ¨çª—å£æ“ä½œ"""
        # ä¿å­˜é‡å éƒ¨åˆ†
        overlap_k, overlap_v = self.active_cache.get_cache()
        overlap_start = max(0, len(overlap_k) - self.overlap_size)
        overlap_k = overlap_k[overlap_start:]
        overlap_v = overlap_v[overlap_start:]

        # å°†å½“å‰ç¼“å­˜ç§»åˆ°é€€ä¼‘ç¼“å­˜åˆ—è¡¨
        current_k, current_v = self.active_cache.get_cache()
        self.retired_caches.append((current_k.clone(), current_v.clone()))

        # é‡ç½®æ´»åŠ¨ç¼“å­˜ï¼Œä¿ç•™é‡å éƒ¨åˆ†
        self.active_cache.reset()
        if len(overlap_k) > 0:
            self.active_cache.update(overlap_k.unsqueeze(0), overlap_v.unsqueeze(0))

        # é™åˆ¶é€€ä¼‘ç¼“å­˜æ•°é‡
        if len(self.retired_caches) > 10:
            self.retired_caches.pop(0)

        self.current_position += self.window_size - self.overlap_size

    def get_full_context(self):
        """è·å–å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆåŒ…æ‹¬å†å²çª—å£ï¼‰"""
        # åˆå¹¶æ‰€æœ‰é€€ä¼‘ç¼“å­˜å’Œæ´»åŠ¨ç¼“å­˜
        all_k = []
        all_v = []

        # æ·»åŠ é€€ä¼‘ç¼“å­˜
        for retired_k, retired_v in self.retired_caches:
            all_k.append(retired_k)
            all_v.append(retired_v)

        # æ·»åŠ æ´»åŠ¨ç¼“å­˜
        active_k, active_v = self.active_cache.get_cache()
        if len(active_k) > 0:
            all_k.append(active_k)
            all_v.append(active_v)

        if all_k:
            full_k = torch.cat(all_k, dim=0)
            full_v = torch.cat(all_v, dim=0)
            return full_k, full_v
        else:
            return torch.empty(0), torch.empty(0)
```

## ğŸ¯ æ€»ç»“ä¸å±•æœ›

### æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹å›é¡¾

é€šè¿‡æœ¬æ–‡çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬æŒæ¡äº†KVç¼“å­˜ä¼˜åŒ–çš„å…³é”®æŠ€æœ¯ï¼š

1. **é™æ€ç¼“å­˜çš„å±€é™æ€§**ï¼šé¢„åˆ†é…å¯¼è‡´çš„å†…å­˜æµªè´¹é—®é¢˜
2. **åŠ¨æ€ç¼“å­˜çš„ä¼˜åŠ¿**ï¼šè‡ªé€‚åº”å¢é•¿ï¼Œæ˜¾è‘—æå‡å†…å­˜åˆ©ç”¨ç‡
3. **é¢„æµ‹æ€§ç¼“å­˜**ï¼šåŸºäºå†å²æ¨¡å¼é¢„æµ‹ï¼Œå‡å°‘æ‰©å®¹æ“ä½œ
4. **åˆ†å±‚ç¼“å­˜**ï¼šå¿«æ…¢åˆ†ç¦»ï¼Œå¹³è¡¡æ€§èƒ½ä¸å®¹é‡
5. **å‹ç¼©ç¼“å­˜**ï¼šåŸºäºé‡è¦æ€§çš„æ™ºèƒ½å‹ç¼©
6. **å¼‚æ­¥ç®¡ç†**ï¼šæå‡å¹¶å‘æ€§èƒ½

### æ€§èƒ½æå‡æ€»ç»“

**å†…å­˜æ•ˆç‡**ï¼š
- é™æ€ç¼“å­˜ï¼š50-95%çš„å†…å­˜æµªè´¹
- åŠ¨æ€ç¼“å­˜ï¼š85-95%çš„å†…å­˜åˆ©ç”¨ç‡
- é¢„æµ‹ç¼“å­˜ï¼šå‡å°‘50-80%çš„æ‰©å®¹æ“ä½œ

**é€‚åº”æ€§**ï¼š
- çŸ­å¯¹è¯åœºæ™¯ï¼šå†…å­˜èŠ‚çœ70-90%
- é•¿æ–‡æ¡£åœºæ™¯ï¼šæ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡
- æµå¼æ¨ç†ï¼šå®æ—¶å†…å­˜ç®¡ç†

### æœªæ¥å‘å±•æ–¹å‘

1. **æ›´æ™ºèƒ½çš„é¢„æµ‹ç®—æ³•**ï¼šåŸºäºæ·±åº¦å­¦ä¹ çš„ä½¿ç”¨æ¨¡å¼é¢„æµ‹
2. **ç¡¬ä»¶æ„ŸçŸ¥ä¼˜åŒ–**ï¼šé’ˆå¯¹ä¸åŒGPUæ¶æ„çš„ä¸“é—¨ä¼˜åŒ–
3. **åˆ†å¸ƒå¼ç¼“å­˜**ï¼šå¤šèŠ‚ç‚¹é—´çš„KVç¼“å­˜å…±äº«
4. **è‡ªåŠ¨è°ƒä¼˜**ï¼šåŸºäºå®é™…å·¥ä½œè´Ÿè½½çš„å‚æ•°è‡ªåŠ¨ä¼˜åŒ–

### å®è·µå»ºè®®

**é€‰æ‹©åˆé€‚çš„ç¼“å­˜ç­–ç•¥**ï¼š
- çŸ­å¯¹è¯ â†’ é¢„æµ‹æ€§ç¼“å­˜
- é•¿æ–‡æ¡£ â†’ åˆ†å±‚ç¼“å­˜
- ä»£ç ç”Ÿæˆ â†’ åŠ¨æ€ç¼“å­˜ï¼ˆå¤§å¢é•¿å› å­ï¼‰
- åˆ›æ„å†™ä½œ â†’ å‹ç¼©ç¼“å­˜

**å…³é”®ä¼˜åŒ–å‚æ•°**ï¼š
- åˆå§‹å®¹é‡ï¼šæ ¹æ®å…¸å‹ä½¿ç”¨åœºæ™¯è®¾ç½®
- å¢é•¿å› å­ï¼š1.2-2.0ä¹‹é—´ï¼Œæ ¹æ®æ•°æ®ç‰¹å¾è°ƒæ•´
- å‹ç¼©æ¯”ä¾‹ï¼š0.6-0.9ä¹‹é—´ï¼Œå¹³è¡¡ç²¾åº¦å’Œå†…å­˜

---

**è®°ä½**ï¼šKVç¼“å­˜ä¼˜åŒ–æ˜¯æå‡LLMæ¨ç†æ€§èƒ½çš„å…³é”®æŠ€æœ¯ã€‚é€‰æ‹©åˆé€‚çš„ç¼“å­˜ç­–ç•¥ï¼Œå¯ä»¥æ˜¾è‘—é™ä½å†…å­˜ä½¿ç”¨ï¼Œæå‡æ¨ç†é€Ÿåº¦ï¼Œè®©å¤§æ¨¡å‹åœ¨æœ‰é™ç¡¬ä»¶èµ„æºä¸‹å‘æŒ¥æœ€å¤§æ½œåŠ›ã€‚

*ä¸‹ä¸€ç¯‡æ–‡ç« å°†æ·±å…¥æ¢è®¨è®¡ç®—èåˆä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬QKVæŠ•å½±èåˆã€Softmaxèåˆå’ŒRoPEä¼˜åŒ–ç­‰å‰æ²¿æŠ€æœ¯ã€‚* ğŸš€