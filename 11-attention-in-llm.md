# Attentionåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨ï¼šæ¶æ„è®¾è®¡çš„æ ¸å¿ƒè€ƒé‡

## ğŸ¯ å¼•è¨€ï¼šAttentionå¦‚ä½•é©±åŠ¨ç°ä»£LLM

ä»GPTåˆ°LLaMAï¼Œä»PaLMåˆ°Claudeï¼Œç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„é©å‘½æ€§æˆåŠŸç¦»ä¸å¼€Attentionæœºåˆ¶çš„åˆ›æ–°åº”ç”¨ã€‚Attentionä¸ä»…æ˜¯è¿™äº›æ¨¡å‹çš„è®¡ç®—æ ¸å¿ƒï¼Œæ›´æ˜¯å†³å®šæ¨¡å‹æ€§èƒ½ã€æ•ˆç‡å’Œå¯æ‰©å±•æ€§çš„å…³é”®æ¶æ„è¦ç´ ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œå½“ä½ å‘ChatGPTæé—®æ—¶ï¼Œæ¨¡å‹éœ€è¦åœ¨ç†è§£ä½ çš„é—®é¢˜ã€å›å¿†ç›¸å…³çŸ¥è¯†ã€ç”Ÿæˆè¿è´¯å›ç­”çš„æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œä¸æ–­è¿›è¡Œå¤æ‚çš„æ³¨æ„åŠ›è®¡ç®—ã€‚è¿™èƒŒåæ¶‰åŠäº†ä»åº•å±‚ç¡¬ä»¶ä¼˜åŒ–åˆ°é«˜å±‚æ¶æ„è®¾è®¡çš„å…¨æ–¹ä½æŠ€æœ¯æŒ‘æˆ˜ã€‚

æœ¬æ–‡å°†æ·±å…¥å‰–æAttentionåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å®é™…åº”ç”¨ï¼Œä»æ¶æ„è®¾è®¡çš„æ ¸å¿ƒè€ƒé‡åˆ°æ¨ç†ä¼˜åŒ–çš„å®è·µæŠ€å·§ï¼Œä»è®­ç»ƒç­–ç•¥åˆ°éƒ¨ç½²æ–¹æ¡ˆï¼Œè®©ä½ å…¨é¢ç†è§£AttentionæŠ€æœ¯å¦‚ä½•æ”¯æ’‘èµ·ç°ä»£AIçš„å®ä¼Ÿå·¥ç¨‹ã€‚

## ğŸ—ï¸ LLMä¸­çš„Attentionæ¶æ„è®¾è®¡

### å…¸å‹LLMçš„Attentionå±‚å¸ƒå±€

```python
class TransformerBlock(nn.Module):
    """æ ‡å‡†çš„Transformerå— - LLMçš„åŸºç¡€æ„å»ºå•å…ƒ"""

    def __init__(self, d_model, n_heads, d_ff, dropout=0.1, attention_type='mha'):
        super().__init__()

        # Multi-Head Attention (æˆ–å…¶å˜ä½“)
        if attention_type == 'mha':
            self.attention = MultiHeadAttention(d_model, n_heads, dropout)
        elif attention_type == 'mqa':
            self.attention = MultiQueryAttention(d_model, n_heads, dropout)
        elif attention_type.startswith('gqa'):
            num_kv_heads = int(attention_type.split('-')[1]) if '-' in attention_type else n_heads // 4
            self.attention = GroupedQueryAttention(d_model, n_heads, num_kv_heads, dropout)
        else:
            raise ValueError(f"Unsupported attention type: {attention_type}")

        # Feed Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )

        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

        # Dropout for residual connections
        self.residual_dropout = nn.Dropout(dropout)

    def forward(self, x, attention_mask=None, use_cache=False, past_key_value=None):
        """
        Args:
            x: [batch_size, seq_len, d_model] è¾“å…¥
            attention_mask: [batch_size, 1, seq_len, seq_len] æ³¨æ„åŠ›æ©ç 
            use_cache: æ˜¯å¦ä½¿ç”¨KVç¼“å­˜ï¼ˆæ¨ç†æ—¶ï¼‰
            past_key_value: ä¹‹å‰çš„KVç¼“å­˜
        """
        # Self-Attention with residual connection
        residual = x
        x = self.norm1(x)

        if use_cache:
            attn_output, present_key_value = self.attention(
                x, x, x,
                attention_mask=attention_mask,
                past_key_value=past_key_value,
                use_cache=True
            )
        else:
            attn_output, _ = self.attention(x, x, x, attention_mask=attention_mask)
            present_key_value = None

        x = residual + self.residual_dropout(attn_output)

        # Feed Forward with residual connection
        residual = x
        x = self.norm2(x)
        x = residual + self.residual_dropout(self.ffn(x))

        return x, present_key_value

class LLMArchitecture(nn.Module):
    """å®Œæ•´çš„å¤§è¯­è¨€æ¨¡å‹æ¶æ„"""

    def __init__(self, vocab_size, d_model=768, n_layers=12, n_heads=12,
                 d_ff=3072, max_seq_len=2048, attention_type='mha'):
        super().__init__()

        self.d_model = d_model
        self.n_layers = n_layers
        self.max_seq_len = max_seq_len

        # Token and Position Embeddings
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_embedding = nn.Embedding(max_seq_len, d_model)

        # Transformer Blocks
        self.blocks = nn.ModuleList([
            TransformerBlock(d_model, n_heads, d_ff, attention_type=attention_type)
            for _ in range(n_layers)
        ])

        # Final Layer Norm
        self.final_norm = nn.LayerNorm(d_model)

        # Output Projection
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)

        # æƒé‡å…±äº«
        self.lm_head.weight = self.token_embedding.weight

    def forward(self, input_ids, attention_mask=None, use_cache=False, past_key_values=None):
        """
        Args:
            input_ids: [batch_size, seq_len] token IDs
            attention_mask: [batch_size, seq_len] padding mask
            use_cache: æ˜¯å¦ä½¿ç”¨KVç¼“å­˜
            past_key_values: ä¹‹å‰çš„KVç¼“å­˜åˆ—è¡¨
        """
        batch_size, seq_len = input_ids.shape

        # Token embeddings
        token_embeds = self.token_embedding(input_ids)

        # Position embeddings
        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
        position_embeds = self.position_embedding(position_ids)

        # Combine embeddings
        x = token_embeds + position_embeds

        # Create attention mask if not provided
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids)

        # Convert to causal mask
        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=input_ids.device))
        attention_mask = attention_mask.unsqueeze(1).unsqueeze(2) * causal_mask.unsqueeze(0).unsqueeze(0)

        # Pass through transformer blocks
        present_key_values = [] if use_cache else None

        for i, block in enumerate(self.blocks):
            past_key_value = past_key_values[i] if past_key_values else None

            x, present = block(
                x,
                attention_mask=attention_mask,
                use_cache=use_cache,
                past_key_value=past_key_value
            )

            if use_cache:
                present_key_values.append(present)

        # Final normalization and output
        x = self.final_norm(x)
        logits = self.lm_head(x)

        return {
            'logits': logits,
            'past_key_values': present_key_values
        }

# LLMæ¶æ„åˆ†æ
def analyze_llm_architecture():
    """åˆ†æLLMæ¶æ„ä¸­çš„Attentionè®¾è®¡"""

    print("=== LLMæ¶æ„ä¸­çš„Attentionè®¾è®¡åˆ†æ ===")
    print()

    # ä¸åŒè§„æ¨¡æ¨¡å‹çš„å…¸å‹é…ç½®
    model_configs = [
        {
            'name': 'å°å‹ (GPT-2 Small)',
            'd_model': 768,
            'n_layers': 12,
            'n_heads': 12,
            'd_ff': 3072,
            'vocab_size': 50257,
            'attention_type': 'mha'
        },
        {
            'name': 'ä¸­å‹ (LLaMA-7B)',
            'd_model': 4096,
            'n_layers': 32,
            'n_heads': 32,
            'd_ff': 11008,
            'vocab_size': 32000,
            'attention_type': 'mqa'
        },
        {
            'name': 'å¤§å‹ (LLaMA-65B)',
            'd_model': 8192,
            'n_layers': 80,
            'n_heads': 64,
            'd_ff': 22016,
            'vocab_size': 32000,
            'attention_type': 'gqa-8'
        }
    ]

    print("æ¨¡å‹è§„æ¨¡\tå‚æ•°é‡\t\tAttentionç±»å‹\tå†…å­˜/å±‚(MB)\tè®¡ç®—/å±‚(GFLOPs)")
    print("-" * 80)

    for config in model_configs:
        # åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼ˆä»…ç”¨äºåˆ†æï¼Œä¸åŠ è½½æƒé‡ï¼‰
        model = LLMArchitecture(
            vocab_size=config['vocab_size'],
            d_model=config['d_model'],
            n_layers=config['n_layers'],
            n_heads=config['n_heads'],
            d_ff=config['d_ff'],
            attention_type=config['attention_type']
        )

        # è®¡ç®—æ€»å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters()) / 1e9  # Billion

        # è®¡ç®—å•å±‚Attentionçš„å†…å­˜ä½¿ç”¨
        batch_size, seq_len = 1, 2048
        d_model, n_heads = config['d_model'], config['n_heads']
        head_dim = d_model // n_heads

        # KVç¼“å­˜å†…å­˜
        if config['attention_type'] == 'mha':
            kv_memory = batch_size * seq_len * n_heads * head_dim * 2 * 4 / (1024**2)  # MB
        elif config['attention_type'] == 'mqa':
            kv_memory = batch_size * seq_len * head_dim * 2 * 4 / (1024**2)  # MB
        else:  # GQA
            num_kv_heads = int(config['attention_type'].split('-')[1])
            kv_memory = batch_size * seq_len * num_kv_heads * (d_model // num_kv_heads) * 2 * 4 / (1024**2)  # MB

        # è®¡ç®—å•å±‚è®¡ç®—é‡
        attention_flops = batch_size * n_heads * seq_len * seq_len * head_dim * 2  # QK^T + AV

        print(f"{config['name']:12s}\t{total_params:8.2f}B\t\t{config['attention_type']:12s}\t"
              f"{kv_memory:10.1f}\t{attention_flops/1e9:10.1f}")

    print()
    print("æ¶æ„è®¾è®¡è¶‹åŠ¿:")
    print("1. å°å‹æ¨¡å‹: ä½¿ç”¨æ ‡å‡†MHAï¼Œè¿½æ±‚æœ€ä½³æ€§èƒ½")
    print("2. ä¸­å‹æ¨¡å‹: å¼€å§‹é‡‡ç”¨MQAï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡")
    print("3. å¤§å‹æ¨¡å‹: ä½¿ç”¨GQAï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¹…é™ä½å†…å­˜éœ€æ±‚")
    print("4. è¶…å¤§æ¨¡å‹: å¯èƒ½ä¼šé‡‡ç”¨æ›´æ¿€è¿›çš„ä¼˜åŒ–ç­–ç•¥")

analyze_llm_architecture()
```

### Attentionå±‚çš„ä¼˜åŒ–ç­–ç•¥

```python
class OptimizedAttentionBlock(nn.Module):
    """ä¼˜åŒ–çš„Attentionå— - åŒ…å«å¤šç§ä¼˜åŒ–æŠ€æœ¯"""

    def __init__(self, d_model, n_heads, d_ff, dropout=0.1,
                 attention_type='mha', use_rope=True, use_flash_attn=False):
        super().__init__()

        self.d_model = d_model
        self.n_heads = n_heads
        self.head_dim = d_model // n_heads
        self.use_rope = use_rope
        self.use_flash_attn = use_flash_attn

        # Attentionå±‚
        if use_flash_attn:
            # ä½¿ç”¨FlashAttentionï¼ˆéœ€è¦ç‰¹æ®Šå®ç°ï¼‰
            self.attention = FlashAttention(d_model, n_heads, dropout)
        else:
            # æ ‡å‡†Attentionæˆ–å…¶å˜ä½“
            if attention_type == 'mha':
                self.attention = MultiHeadAttention(d_model, n_heads, dropout)
            elif attention_type == 'mqa':
                self.attention = MultiQueryAttention(d_model, n_heads, dropout)
            else:
                self.attention = GroupedQueryAttention(d_model, n_heads,
                                                     int(attention_type.split('-')[1]), dropout)

        # RoPEä½ç½®ç¼–ç 
        if use_rope:
            self.rope = OptimizedRoPE(self.head_dim)

        # Feed Forward Network - ä½¿ç”¨æ›´é«˜æ•ˆçš„æ¿€æ´»å‡½æ•°
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),  # GELUé€šå¸¸æ¯”ReLUæ•ˆæœæ›´å¥½
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )

        # RMSNormæ›¿ä»£LayerNormï¼ˆæ›´ç¨³å®šï¼Œè®¡ç®—æ›´å¿«ï¼‰
        self.norm1 = RMSNorm(d_model)
        self.norm2 = RMSNorm(d_model)

        # Pre-normalizationï¼ˆæ›´ç¨³å®šçš„è®­ç»ƒï¼‰
        self.pre_norm = True

        # Dropout
        self.residual_dropout = nn.Dropout(dropout)

    def forward(self, x, attention_mask=None, position_ids=None, use_cache=False):
        """ä¼˜åŒ–çš„å‰å‘ä¼ æ’­"""

        if self.pre_norm:
            # Pre-norm: å…ˆnormå†attention
            normed_x = self.norm1(x)

            if self.use_rope and position_ids is not None:
                # åº”ç”¨RoPEä½ç½®ç¼–ç 
                normed_x = self._apply_rope(normed_x, position_ids)

            if use_cache:
                attn_output, cache = self.attention(normed_x, normed_x, normed_x,
                                                  attention_mask=attention_mask,
                                                  use_cache=True)
            else:
                attn_output, _ = self.attention(normed_x, normed_x, normed_x,
                                              attention_mask=attention_mask)
                cache = None

            # æ®‹å·®è¿æ¥
            x = x + self.residual_dropout(attn_output)

            # Feed Forward
            normed_x = self.norm2(x)
            ffn_output = self.ffn(normed_x)
            x = x + self.residual_dropout(ffn_output)

        else:
            # Post-norm: å…ˆattentionå†norm
            if self.use_rope and position_ids is not None:
                x = self._apply_rope(x, position_ids)

            if use_cache:
                attn_output, cache = self.attention(x, x, x,
                                                  attention_mask=attention_mask,
                                                  use_cache=True)
            else:
                attn_output, _ = self.attention(x, x, x, attention_mask=attention_mask)
                cache = None

            x = self.norm1(x + attn_output)
            x = self.norm2(x + self.ffn(x))

        return x, cache

    def _apply_rope(self, x, position_ids):
        """åº”ç”¨RoPEä½ç½®ç¼–ç """
        batch_size, seq_len, d_model = x.shape
        x = x.view(batch_size, seq_len, self.n_heads, self.head_dim)

        # å°†position_idsæ‰©å±•åˆ°å¤šå¤´ç»´åº¦
        cos, sin = self.rope(position_ids)

        # åº”ç”¨RoPE
        x_rotated = self._rope_apply(x, cos, sin)

        return x_rotated.view(batch_size, seq_len, d_model)

class RMSNorm(nn.Module):
    """RMS Normalization - æ¯”LayerNormæ›´é«˜æ•ˆ"""

    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(d_model))

    def forward(self, x):
        # RMS: sqrt(mean(x^2))
        rms = torch.sqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.eps)
        return self.weight * x / rms

# ä¼˜åŒ–ç­–ç•¥æ•ˆæœå¯¹æ¯”
def compare_optimization_strategies():
    """å¯¹æ¯”ä¸åŒä¼˜åŒ–ç­–ç•¥çš„æ•ˆæœ"""

    print("=== Attentionä¼˜åŒ–ç­–ç•¥å¯¹æ¯” ===")
    print()

    # åŸºç¡€é…ç½®
    d_model = 2048
    n_heads = 32
    seq_len = 2048
    batch_size = 1

    optimization_configs = [
        {
            'name': 'åŸºç¡€MHA',
            'use_rope': False,
            'use_flash_attn': False,
            'attention_type': 'mha',
            'norm_type': 'layer_norm',
            'norm_position': 'post'
        },
        {
            'name': 'MHA + RoPE',
            'use_rope': True,
            'use_flash_attn': False,
            'attention_type': 'mha',
            'norm_type': 'layer_norm',
            'norm_position': 'post'
        },
        {
            'name': 'MQA + RoPE',
            'use_rope': True,
            'use_flash_attn': False,
            'attention_type': 'mqa',
            'norm_type': 'layer_norm',
            'norm_position': 'post'
        },
        {
            'name': 'MQA + RoPE + PreNorm',
            'use_rope': True,
            'use_flash_attn': False,
            'attention_type': 'mqa',
            'norm_type': 'layer_norm',
            'norm_position': 'pre'
        },
        {
            'name': 'MQA + RoPE + RMSNorm',
            'use_rope': True,
            'use_flash_attn': False,
            'attention_type': 'mqa',
            'norm_type': 'rms_norm',
            'norm_position': 'pre'
        },
        {
            'name': 'å…¨ä¼˜åŒ– (MQA+RoPE+Flash)',
            'use_rope': True,
            'use_flash_attn': True,
            'attention_type': 'mqa',
            'norm_type': 'rms_norm',
            'norm_position': 'pre'
        }
    ]

    print("é…ç½®\t\t\tæ¨ç†æ—¶é—´(ms)\tå†…å­˜ä½¿ç”¨(MB)\tç›¸å¯¹æ€§èƒ½")
    print("-" * 70)

    baseline_time = None
    baseline_memory = None

    for config in optimization_configs:
        # æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®ï¼ˆå®é™…ä¸­éœ€è¦çœŸå®æµ‹è¯•ï¼‰
        if config['name'] == 'åŸºç¡€MHA':
            inference_time = 100.0  # åŸºå‡†
            memory_usage = 2048.0   # åŸºå‡†
            baseline_time = inference_time
            baseline_memory = memory_usage
        else:
            # ä¼°ç®—ä¼˜åŒ–æ•ˆæœ
            time_reduction = 0.0
            memory_reduction = 0.0

            if 'mqa' in config['attention_type']:
                time_reduction += 0.2
                memory_reduction += 0.9
            if 'flash_attn' in config['name'].lower():
                time_reduction += 0.4
                memory_reduction += 0.3
            if config['norm_type'] == 'rms_norm':
                time_reduction += 0.05
            if config['norm_position'] == 'pre':
                time_reduction += 0.1

            inference_time = baseline_time * (1 - time_reduction)
            memory_usage = baseline_memory * (1 - memory_reduction)

        relative_performance = baseline_time / inference_time

        print(f"{config['name']:<20s}\t{inference_time:10.2f}\t{memory_use:10.1f}\t{relative_performance:10.2f}x")

    print()
    print("ä¼˜åŒ–ç­–ç•¥åˆ†æ:")
    print("1. MQA: æœ€å¤§å¹…åº¦çš„å†…å­˜èŠ‚çœï¼Œæ˜¾è‘—çš„æ—¶é—´åŠ é€Ÿ")
    print("2. FlashAttention: ä¸»è¦èŠ‚çœæ—¶é—´ï¼Œä¸­ç­‰å†…å­˜èŠ‚çœ")
    print("3. RoPE: ä½ç½®ç¼–ç ä¼˜åŒ–ï¼Œå°å¹…æ€§èƒ½æå‡")
    print("4. RMSNorm: æ›¿ä»£LayerNormï¼Œå°å¹…æ€§èƒ½æå‡")
    print("5. Pre-normalization: æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œå°å¹…é€Ÿåº¦æå‡")

compare_optimization_strategies()
```

## ğŸš€ æ¨ç†ä¼˜åŒ–ä¸KVç¼“å­˜

### é«˜æ•ˆçš„KVç¼“å­˜ç®¡ç†

```python
class AdvancedKVCache:
    """é«˜çº§KVç¼“å­˜ç®¡ç† - æ”¯æŒå¤šç§ä¼˜åŒ–ç­–ç•¥"""

    def __init__(self, max_seq_len, num_heads, head_dim, dtype=torch.float16):
        self.max_seq_len = max_seq_len
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dtype = dtype

        # åˆ†å±‚ç¼“å­˜è®¾è®¡
        self.hot_cache_size = max_seq_len // 4  # çƒ­ç¼“å­˜ï¼šæœ€è¿‘25%
        self.warm_cache_size = max_seq_len // 4  # æ¸©ç¼“å­˜ï¼šä¸­é—´50%
        self.cold_cache_size = max_seq_len // 2  # å†·ç¼“å­˜ï¼šæœ€æ—©25%

        # é¢„åˆ†é…ç¼“å­˜
        self._allocate_caches()

        # ç¼“å­˜çŠ¶æ€è·Ÿè¸ª
        self.current_length = 0
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0
        }

    def _allocate_caches(self):
        """åˆ†é…åˆ†å±‚ç¼“å­˜"""
        # çƒ­ç¼“å­˜ - GPUå†…å­˜
        self.hot_k_cache = torch.zeros(
            self.hot_cache_size, self.num_heads, self.head_dim,
            dtype=self.dtype, device='cuda'
        )
        self.hot_v_cache = torch.zeros(
            self.hot_cache_size, self.num_heads, self.head_dim,
            dtype=self.dtype, device='cuda'
        )

        # æ¸©ç¼“å­˜ - GPUå†…å­˜ï¼ˆå¯è¢«æ¢å‡ºï¼‰
        self.warm_k_cache = torch.zeros(
            self.warm_cache_size, self.num_heads, self.head_dim,
            dtype=self.dtype, device='cuda'
        )
        self.warm_v_cache = torch.zeros(
            self.warm_cache_size, self.num_heads, self.head_dim,
            dtype=self.dtype, device='cuda'
        )

        # å†·ç¼“å­˜ - CPUå†…å­˜
        self.cold_k_cache = torch.zeros(
            self.cold_cache_size, self.num_heads, self.head_dim,
            dtype=self.dtype, device='cpu'
        )
        self.cold_v_cache = torch.zeros(
            self.cold_cache_size, self.num_heads, self.head_dim,
            dtype=self.dtype, device='cpu'
        )

    def update(self, new_k, new_v):
        """æ›´æ–°ç¼“å­˜"""
        batch_size, seq_len, num_heads, head_dim = new_k.shape

        assert batch_size == 1, "AdvancedKVCacheåªæ”¯æŒbatch_size=1"
        assert num_heads == self.num_heads
        assert head_dim == self.head_dim

        # å°†æ–°KVæ·»åŠ åˆ°ç¼“å­˜
        for i in range(seq_len):
            self._add_single_kv(new_k[0, i], new_v[0, i])
            self.current_length += 1

    def _add_single_kv(self, k_slice, v_slice):
        """æ·»åŠ å•ä¸ªKVå¯¹"""
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç§»åŠ¨ç¼“å­˜
        if self.current_length >= self.max_seq_len:
            self._evict_oldest()

        position = self.current_length % self.max_seq_len

        if position < self.hot_cache_size:
            # æ·»åŠ åˆ°çƒ­ç¼“å­˜
            idx = position
            self.hot_k_cache[idx] = k_slice
            self.hot_v_cache[idx] = v_slice
        elif position < self.hot_cache_size + self.warm_cache_size:
            # æ·»åŠ åˆ°æ¸©ç¼“å­˜
            idx = position - self.hot_cache_size
            self.warm_k_cache[idx] = k_slice
            self.warm_v_cache[idx] = v_slice
        else:
            # æ·»åŠ åˆ°å†·ç¼“å­˜
            idx = position - self.hot_cache_size - self.warm_cache_size
            self.cold_k_cache[idx] = k_slice.to('cpu')
            self.cold_v_cache[idx] = v_slice.to('cpu')

    def _evict_oldest(self):
        """æ·˜æ±°æœ€æ—§çš„KV"""
        # å°†å†·ç¼“å­˜çš„æœ€æ—§éƒ¨åˆ†ç§»é™¤
        # å®é™…å®ç°ä¸­éœ€è¦æ›´å¤æ‚çš„æ·˜æ±°ç­–ç•¥
        self.cache_stats['evictions'] += 1

    def get_cache(self, start_pos=0, end_pos=None):
        """è·å–æŒ‡å®šèŒƒå›´çš„ç¼“å­˜"""
        if end_pos is None:
            end_pos = self.current_length

        # æ”¶é›†æ¥è‡ªä¸åŒå±‚çš„ç¼“å­˜
        k_list = []
        v_list = []

        for pos in range(start_pos, min(end_pos, self.current_length)):
            actual_pos = pos % self.max_seq_len

            if actual_pos < self.hot_cache_size:
                # ä»çƒ­ç¼“å­˜è·å–
                k_list.append(self.hot_k_cache[actual_pos])
                v_list.append(self.hot_v_cache[actual_pos])
                self.cache_stats['hits'] += 1
            elif actual_pos < self.hot_cache_size + self.warm_cache_size:
                # ä»æ¸©ç¼“å­˜è·å–
                idx = actual_pos - self.hot_cache_size
                k_list.append(self.warm_k_cache[idx])
                v_list.append(self.warm_v_cache[idx])
                self.cache_stats['hits'] += 1
            else:
                # ä»å†·ç¼“å­˜è·å–ï¼ˆéœ€è¦ä¼ è¾“åˆ°GPUï¼‰
                idx = actual_pos - self.hot_cache_size - self.warm_cache_size
                k_list.append(self.cold_k_cache[idx].cuda())
                v_list.append(self.cold_v_cache[idx].cuda())
                self.cache_stats['misses'] += 1

        if k_list:
            k = torch.stack(k_list).unsqueeze(0)  # [1, seq_len, num_heads, head_dim]
            v = torch.stack(v_list).unsqueeze(0)
            return k, v
        else:
            return None, None

    def get_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_access = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = self.cache_stats['hits'] / total_access if total_access > 0 else 0

        return {
            'current_length': self.current_length,
            'max_length': self.max_seq_len,
            'hit_rate': hit_rate,
            'total_accesses': total_access,
            'evictions': self.cache_stats['evictions']
        }

# KVç¼“å­˜ä¼˜åŒ–æµ‹è¯•
def test_kv_cache_optimization():
    """æµ‹è¯•KVç¼“å­˜ä¼˜åŒ–ç­–ç•¥"""

    print("=== KVç¼“å­˜ä¼˜åŒ–æµ‹è¯• ===")

    # é…ç½®
    num_heads = 32
    head_dim = 128
    max_seq_len = 8192
    total_tokens = 16384  # è¶…è¿‡æœ€å¤§ç¼“å­˜é•¿åº¦

    # åˆ›å»ºé«˜çº§ç¼“å­˜
    cache = AdvancedKVCache(max_seq_len, num_heads, head_dim)

    # æ¨¡æ‹Ÿé•¿åºåˆ—å¤„ç†
    chunk_size = 256
    total_chunks = total_tokens // chunk_size

    print(f"å¤„ç† {total_tokens} ä¸ªtokenï¼Œæœ€å¤§ç¼“å­˜ {max_seq_len}")
    print(f"åˆ†å—å¤§å°: {chunk_size}, æ€»åˆ†å—æ•°: {total_chunks}")
    print()

    processing_times = []

    for chunk_idx in range(total_chunks):
        start_time = time.time()

        # ç”Ÿæˆæ–°çš„KVæ•°æ®
        new_k = torch.randn(1, chunk_size, num_heads, head_dim, dtype=torch.float16, device='cuda')
        new_v = torch.randn(1, chunk_size, num_heads, head_dim, dtype=torch.float16, device='cuda')

        # æ›´æ–°ç¼“å­˜
        cache.update(new_k, new_v)

        # éšæœºè·å–éƒ¨åˆ†ç¼“å­˜ï¼ˆæ¨¡æ‹Ÿæ¨ç†åœºæ™¯ï¼‰
        if chunk_idx % 10 == 0:
            start_pos = max(0, cache.current_length - 1024)
            end_pos = cache.current_length
            k_cached, v_cached = cache.get_cache(start_pos, end_pos)

        processing_time = time.time() - start_time
        processing_times.append(processing_time)

        if chunk_idx % 20 == 0:
            stats = cache.get_stats()
            print(f"Chunk {chunk_idx:3d}/{total_chunks}: "
                  f"ç¼“å­˜é•¿åº¦={stats['current_length']:4d}, "
                  f"å‘½ä¸­ç‡={stats['hit_rate']:.3f}, "
                  f"å¤„ç†æ—¶é—´={processing_time*1000:6.2f}ms")

    # æœ€ç»ˆç»Ÿè®¡
    final_stats = cache.get_stats()
    avg_processing_time = sum(processing_times) / len(processing_times)

    print()
    print("æœ€ç»ˆç»Ÿè®¡:")
    print(f"  æ€»å¤„ç†tokenæ•°: {total_tokens}")
    print(f"  å½“å‰ç¼“å­˜é•¿åº¦: {final_stats['current_length']}")
    print(f"  ç¼“å­˜å‘½ä¸­ç‡: {final_stats['hit_rate']:.3f}")
    print(f"  å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time*1000:.2f}ms")
    print(f"  ååé‡: {chunk_size/avg_processing_time:.1f} tokens/ç§’")

    # å†…å­˜ä½¿ç”¨åˆ†æ
    hot_memory = cache.hot_cache_size * num_heads * head_dim * 2 * 2 / (1024**2)  # MB
    warm_memory = cache.warm_cache_size * num_heads * head_dim * 2 * 2 / (1024**2)  # MB
    cold_memory = cache.cold_cache_size * num_heads * head_dim * 2 * 2 / (1024**2)  # MB

    print(f"  çƒ­ç¼“å­˜å†…å­˜: {hot_memory:.1f} MB")
    print(f"  æ¸©ç¼“å­˜å†…å­˜: {warm_memory:.1f} MB")
    print(f"  å†·ç¼“å­˜å†…å­˜: {cold_memory:.1f} MB")
    print(f"  æ€»å†…å­˜ä½¿ç”¨: {hot_memory + warm_memory:.1f} MB (GPU)")

test_kv_cache_optimization()
```

### åŠ¨æ€æ‰¹å¤„ç†ä¼˜åŒ–

```python
class DynamicBatchProcessor:
    """åŠ¨æ€æ‰¹å¤„ç†å¤„ç†å™¨ - ä¼˜åŒ–æ¨ç†ååé‡"""

    def __init__(self, model, max_batch_size=8, max_wait_time_ms=50):
        self.model = model
        self.max_batch_size = max_batch_size
        self.max_wait_time_ms = max_wait_time_ms

        # è¯·æ±‚é˜Ÿåˆ—
        self.request_queue = []
        self.processing_queue = []

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'total_requests': 0,
            'total_tokens': 0,
            'total_time': 0.0,
            'avg_batch_size': 0.0,
            'avg_wait_time': 0.0
        }

    def add_request(self, request_id, input_ids, max_new_tokens=100):
        """æ·»åŠ æ–°çš„æ¨ç†è¯·æ±‚"""
        request = {
            'id': request_id,
            'input_ids': input_ids,
            'max_new_tokens': max_new_tokens,
            'generated_tokens': [],
            'finished': False,
            'arrival_time': time.time(),
            'start_time': None,
            'kv_cache': None
        }

        self.request_queue.append(request)
        self.stats['total_requests'] += 1

        # è§¦å‘æ‰¹å¤„ç†
        return self._try_process_batch()

    def _try_process_batch(self):
        """å°è¯•å¤„ç†æ‰¹å¤„ç†"""
        if not self.request_queue:
            return []

        current_time = time.time()

        # é€‰æ‹©å¯æ‰¹å¤„ç†çš„è¯·æ±‚
        batch_requests = []
        remaining_requests = []

        for request in self.request_queue:
            if len(batch_requests) < self.max_batch_size:
                wait_time = (current_time - request['arrival_time']) * 1000

                # å¦‚æœé˜Ÿåˆ—ä¸ºç©ºæˆ–ç­‰å¾…æ—¶é—´è¶…è¿‡é˜ˆå€¼ï¼ŒåŠ å…¥æ‰¹å¤„ç†
                if not batch_requests or wait_time >= self.max_wait_time_ms:
                    batch_requests.append(request)
                    request['start_time'] = current_time
                else:
                    remaining_requests.append(request)
            else:
                remaining_requests.append(request)

        # æ›´æ–°é˜Ÿåˆ—
        self.request_queue = remaining_requests

        if batch_requests:
            results = self._process_batch(batch_requests)
            self._update_stats(batch_requests, results)
            return results

        return []

    def _process_batch(self, batch_requests):
        """å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„è¯·æ±‚"""
        if not batch_requests:
            return []

        start_time = time.time()

        # å‡†å¤‡æ‰¹å¤„ç†æ•°æ®
        batch_input_ids = []
        batch_attention_masks = []
        batch_kv_caches = []

        max_seq_len = 0
        for request in batch_requests:
            input_ids = request['input_ids']
            batch_input_ids.append(input_ids)
            batch_kv_caches.append(request['kv_cache'])
            max_seq_len = max(max_seq_len, len(input_ids))

        # Paddingåˆ°ç›¸åŒé•¿åº¦
        padded_inputs = []
        attention_masks = []

        for input_ids in batch_input_ids:
            # å·¦paddingï¼ˆcausalæ¨¡å‹é€šå¸¸éœ€è¦ï¼‰
            padding_len = max_seq_len - len(input_ids)
            if padding_len > 0:
                padded = torch.cat([
                    torch.full((padding_len,), 0, dtype=input_ids.dtype),
                    input_ids
                ])
                attention_mask = torch.cat([
                    torch.zeros(padding_len),
                    torch.ones(len(input_ids))
                ])
            else:
                padded = input_ids
                attention_mask = torch.ones(len(input_ids))

            padded_inputs.append(padded)
            attention_masks.append(attention_mask)

        # æ‰¹å¤„ç†å¼ é‡
        batch_input_ids = torch.stack(padded_inputs)
        batch_attention_mask = torch.stack(attention_masks)

        # æ¨¡å‹æ¨ç†ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        with torch.no_grad():
            outputs = self.model(
                input_ids=batch_input_ids,
                attention_mask=batch_attention_mask,
                use_cache=True,
                past_key_values=None
            )

        # å¤„ç†è¾“å‡º
        results = []
        processing_time = time.time() - start_time

        for i, request in enumerate(batch_requests):
            # è·å–å¯¹åº”ä½ç½®çš„è¾“å‡º
            original_len = len(request['input_ids'])
            start_idx = max_seq_len - original_len

            logits = outputs['logits'][i, start_idx:]
            new_kv_cache = outputs['past_key_values']

            # ç”Ÿæˆä¸‹ä¸€ä¸ªtokenï¼ˆç®€åŒ–ï¼‰
            next_token = torch.argmax(logits[-1:], dim=-1)

            result = {
                'request_id': request['id'],
                'next_token': next_token.item(),
                'processing_time': processing_time,
                'new_kv_cache': new_kv_cache
            }

            results.append(result)

            # æ›´æ–°è¯·æ±‚çŠ¶æ€
            request['generated_tokens'].append(next_token.item())
            request['kv_cache'] = new_kv_cache

            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            if len(request['generated_tokens']) >= request['max_new_tokens']:
                request['finished'] = True
            else:
                # æ·»åŠ å›é˜Ÿåˆ—ç»§ç»­å¤„ç†
                request['input_ids'] = torch.cat([
                    request['input_ids'],
                    next_token.unsqueeze(0)
                ])
                self.request_queue.append(request)

        return results

    def _update_stats(self, batch_requests, results):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        if not batch_requests:
            return

        batch_size = len(batch_requests)
        processing_time = results[0]['processing_time']

        self.stats['total_time'] += processing_time
        self.stats['avg_batch_size'] = (
            self.stats['avg_batch_size'] * (self.stats['total_requests'] - batch_size) +
            batch_size
        ) / self.stats['total_requests']

        # è®¡ç®—å¹³å‡ç­‰å¾…æ—¶é—´
        total_wait_time = sum(
            req['start_time'] - req['arrival_time']
            for req in batch_requests if req['start_time'] is not None
        )
        avg_wait_time = total_wait_time / batch_size

        self.stats['avg_wait_time'] = (
            self.stats['avg_wait_time'] * (self.stats['total_requests'] - batch_size) +
            avg_wait_time
        ) / self.stats['total_requests']

    def get_stats(self):
        """è·å–å¤„ç†ç»Ÿè®¡"""
        if self.stats['total_time'] > 0:
            throughput = self.stats['total_requests'] / self.stats['total_time']
        else:
            throughput = 0

        return {
            **self.stats,
            'throughput': throughput,
            'queue_length': len(self.request_queue)
        }

# åŠ¨æ€æ‰¹å¤„ç†æ¼”ç¤º
def demo_dynamic_batching():
    """æ¼”ç¤ºåŠ¨æ€æ‰¹å¤„ç†çš„æ•ˆæœ"""

    print("=== åŠ¨æ€æ‰¹å¤„ç†æ¼”ç¤º ===")

    # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹
    model = LLMArchitecture(vocab_size=1000, d_model=512, n_layers=4, n_heads=8)

    # åˆ›å»ºæ‰¹å¤„ç†å™¨
    processor = DynamicBatchProcessor(model, max_batch_size=4, max_wait_time_ms=100)

    # æ¨¡æ‹Ÿè¯·æ±‚åˆ°è¾¾
    import random

    def simulate_requests():
        """æ¨¡æ‹Ÿè¯·æ±‚æµ"""
        request_id = 0

        # æ¨¡æ‹Ÿä¸åŒçš„è¯·æ±‚æ¨¡å¼
        for wave in range(5):
            print(f"\nè¯·æ±‚æ³¢æ¬¡ {wave + 1}:")

            # æ¯ä¸ªæ³¢æ¬¡éšæœºæ•°é‡çš„è¯·æ±‚
            num_requests = random.randint(1, 8)

            for _ in range(num_requests):
                # éšæœºè¾“å…¥é•¿åº¦
                input_length = random.randint(10, 100)
                input_ids = torch.randint(1, 1000, (input_length,))

                results = processor.add_request(request_id, input_ids)

                print(f"  è¯·æ±‚ {request_id}: è¾“å…¥é•¿åº¦={input_length}, "
                      f"æ‰¹å¤„ç†å¤§å°={len(results) if results else 0}")

                request_id += 1

            # å¤„ç†å‰©ä½™è¯·æ±‚
            while processor.request_queue:
                results = processor._try_process_batch()
                if results:
                    print(f"    å¤„ç†äº† {len(results)} ä¸ªè¯·æ±‚")
                time.sleep(0.01)  # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ

    # è¿è¡Œæ¨¡æ‹Ÿ
    simulate_requests()

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    stats = processor.get_stats()

    print(f"\n=== æ‰¹å¤„ç†ç»Ÿè®¡ ===")
    print(f"æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
    print(f"å¹³å‡æ‰¹å¤§å°: {stats['avg_batch_size']:.2f}")
    print(f"å¹³å‡ç­‰å¾…æ—¶é—´: {stats['avg_wait_time']*1000:.2f}ms")
    print(f"ååé‡: {stats['throughput']:.2f} requests/sec")
    print(f"å‰©ä½™é˜Ÿåˆ—é•¿åº¦: {stats['queue_length']}")

demo_dynamic_batching()
```

## ğŸ¯ è®­ç»ƒä¼˜åŒ–ç­–ç•¥

### æ¢¯åº¦æ£€æŸ¥ç‚¹ä¸å†…å­˜ä¼˜åŒ–

```python
class MemoryEfficientTraining:
    """å†…å­˜é«˜æ•ˆçš„è®­ç»ƒç­–ç•¥"""

    def __init__(self, model, use_gradient_checkpointing=True,
                 use_mixed_precision=True, use_offload=False):
        self.model = model
        self.use_gradient_checkpointing = use_gradient_checkpointing
        self.use_mixed_precision = use_mixed_precision
        self.use_offload = use_offload

        # æ··åˆç²¾åº¦è®­ç»ƒ
        if use_mixed_precision:
            self.scaler = torch.cuda.amp.GradScaler()

        # æ¢¯åº¦æ£€æŸ¥ç‚¹
        if use_gradient_checkpointing:
            self._enable_gradient_checkpointing()

        # å†…å­˜ä¼˜åŒ–è®¾ç½®
        self.memory_stats = {
            'peak_memory': 0,
            'avg_memory': 0,
            'checkpoint_savings': 0
        }

    def _enable_gradient_checkpointing(self):
        """å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹"""
        def make_checkpointed_forward(module):
            """åˆ›å»ºæ£€æŸ¥ç‚¹ç‰ˆæœ¬çš„forwardæ–¹æ³•"""
            def checkpointed_forward(*args, **kwargs):
                return torch.utils.checkpoint.checkpoint(
                    module.__class__.forward, module, *args, **kwargs
                )
            return checkpointed_forward

        # ä¸ºæ¯ä¸ªTransformerå—å¯ç”¨æ£€æŸ¥ç‚¹
        for name, module in self.model.named_modules():
            if isinstance(module, TransformerBlock):
                # ä¿å­˜åŸå§‹forwardæ–¹æ³•
                original_forward = module.forward
                # è®¾ç½®æ£€æŸ¥ç‚¹ç‰ˆæœ¬
                module.forward = make_checkpointed_forward(module)
                # ä¿å­˜åŸå§‹æ–¹æ³•ä»¥ä¾¿æ¢å¤
                module._original_forward = original_forward

    def training_step(self, batch, optimizer):
        """å•æ­¥è®­ç»ƒ"""
        # è®°å½•å†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()

        start_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0

        # å‰å‘ä¼ æ’­
        if self.use_mixed_precision:
            with torch.cuda.amp.autocast():
                outputs = self._forward_pass(batch)
                loss = self._compute_loss(outputs, batch)
        else:
            outputs = self._forward_pass(batch)
            loss = self._compute_loss(outputs, batch)

        # è®°å½•å³°å€¼å†…å­˜
        if torch.cuda.is_available():
            peak_memory = torch.cuda.max_memory_allocated()
            self.memory_stats['peak_memory'] = max(
                self.memory_stats['peak_memory'], peak_memory
            )

        # åå‘ä¼ æ’­
        if self.use_mixed_precision:
            self.scaler.scale(loss).backward()
            self.scaler.step(optimizer)
            self.scaler.update()
        else:
            loss.backward()
            optimizer.step()

        optimizer.zero_grad()

        # æ›´æ–°ç»Ÿè®¡
        end_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        self.memory_stats['avg_memory'] = (
            self.memory_stats['avg_memory'] + (end_memory - start_memory)
        ) / 2

        return {
            'loss': loss.item(),
            'peak_memory_mb': peak_memory / (1024**2) if torch.cuda.is_available() else 0
        }

    def _forward_pass(self, batch):
        """å‰å‘ä¼ æ’­ï¼ˆæ”¯æŒæ£€æŸ¥ç‚¹ï¼‰"""
        input_ids = batch['input_ids']
        attention_mask = batch.get('attention_mask')

        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            use_cache=False  # è®­ç»ƒæ—¶ä¸ä½¿ç”¨ç¼“å­˜
        )

        return outputs

    def _compute_loss(self, outputs, batch):
        """è®¡ç®—æŸå¤±"""
        logits = outputs['logits']
        labels = batch['labels']

        # ç®€åŒ–çš„äº¤å‰ç†µæŸå¤±
        loss_fct = nn.CrossEntropyLoss()
        loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))

        return loss

    def get_memory_stats(self):
        """è·å–å†…å­˜ä½¿ç”¨ç»Ÿè®¡"""
        return self.memory_stats.copy()

# è®­ç»ƒä¼˜åŒ–å¯¹æ¯”
def compare_training_optimizations():
    """å¯¹æ¯”ä¸åŒè®­ç»ƒä¼˜åŒ–ç­–ç•¥"""

    print("=== è®­ç»ƒä¼˜åŒ–ç­–ç•¥å¯¹æ¯” ===")

    # é…ç½®
    model_configs = [
        {
            'name': 'åŸºçº¿è®­ç»ƒ',
            'use_checkpointing': False,
            'use_mixed_precision': False,
            'use_offload': False
        },
        {
            'name': 'æ¢¯åº¦æ£€æŸ¥ç‚¹',
            'use_checkpointing': True,
            'use_mixed_precision': False,
            'use_offload': False
        },
        {
            'name': 'æ··åˆç²¾åº¦',
            'use_checkpointing': False,
            'use_mixed_precision': True,
            'use_offload': False
        },
        {
            'name': 'æ£€æŸ¥ç‚¹+æ··åˆç²¾åº¦',
            'use_checkpointing': True,
            'use_mixed_precision': True,
            'use_offload': False
        },
        {
            'name': 'å…¨ä¼˜åŒ–',
            'use_checkpointing': True,
            'use_mixed_precision': True,
            'use_offload': True
        }
    ]

    print("é…ç½®\t\t\tå³°å€¼å†…å­˜(GB)\tè®­ç»ƒæ—¶é—´(s/step)\tå†…å­˜èŠ‚çœ\té€Ÿåº¦æå‡")
    print("-" * 70)

    baseline_memory = None
    baseline_time = None

    for config in model_configs:
        # æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®ï¼ˆå®é™…ä¸­éœ€è¦çœŸå®æµ‹è¯•ï¼‰
        if config['name'] == 'åŸºçº¿è®­ç»ƒ':
            peak_memory = 24.0  # GB
            training_time = 2.5  # seconds per step
            baseline_memory = peak_memory
            baseline_time = training_time
        else:
            # ä¼°ç®—ä¼˜åŒ–æ•ˆæœ
            memory_reduction = 0.0
            time_change = 0.0

            if config['use_checkpointing']:
                memory_reduction += 0.3  # 30%å†…å­˜èŠ‚çœ
                time_change += 0.4       # 40%æ—¶é—´å¢åŠ 

            if config['use_mixed_precision']:
                memory_reduction += 0.5  # 50%å†…å­˜èŠ‚çœ
                time_change += -0.2      # 20%æ—¶é—´å‡å°‘

            if config['use_offload']:
                memory_reduction += 0.6  # 60%å†…å­˜èŠ‚çœ
                time_change += 0.8       # 80%æ—¶é—´å¢åŠ 

            peak_memory = baseline_memory * (1 - memory_reduction)
            training_time = baseline_time * (1 + time_change)

        memory_savings = (baseline_memory - peak_memory) / baseline_memory * 100
        speedup = baseline_time / training_time

        print(f"{config['name']:<20s}\t{peak_memory:10.2f}\t{training_time:12.3f}\t"
              f"{memory_savings:8.1f}%\t{speedup:8.2f}x")

    print()
    print("è®­ç»ƒä¼˜åŒ–å»ºè®®:")
    print("1. æ¢¯åº¦æ£€æŸ¥ç‚¹: å¤§å¹…å‡å°‘å†…å­˜ï¼Œä½†å¢åŠ è®¡ç®—æ—¶é—´")
    print("2. æ··åˆç²¾åº¦: åŒæ—¶èŠ‚çœå†…å­˜å’Œæ—¶é—´ï¼Œæ¨èä½¿ç”¨")
    print("3. å†…å­˜å¸è½½: æç«¯å†…å­˜çº¦æŸæ—¶ä½¿ç”¨ï¼Œä½†æ˜¾è‘—å½±å“é€Ÿåº¦")
    print("4. ç»„åˆç­–ç•¥: æ ¹æ®ç¡¬ä»¶èµ„æºé€‰æ‹©åˆé€‚ç»„åˆ")

compare_training_optimizations()
```

## ğŸ¯ æ€»ç»“ä¸æœ€ä½³å®è·µ

### LLMä¸­Attentionçš„æ ¸å¿ƒè€ƒé‡

é€šè¿‡æœ¬æ–‡çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å…¨é¢æŒæ¡äº†Attentionåœ¨å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å…³é”®åº”ç”¨ï¼š

1. **æ¶æ„è®¾è®¡**ï¼šä»æ ‡å‡†MHAåˆ°MQA/GQAçš„æ¼”è¿›è·¯å¾„
2. **æ¨ç†ä¼˜åŒ–**ï¼šKVç¼“å­˜ã€åŠ¨æ€æ‰¹å¤„ç†ç­‰å…³é”®æŠ€æœ¯
3. **è®­ç»ƒä¼˜åŒ–**ï¼šæ¢¯åº¦æ£€æŸ¥ç‚¹ã€æ··åˆç²¾åº¦ç­‰ç­–ç•¥
4. **å·¥ç¨‹å®è·µ**ï¼šä»ç®—æ³•åˆ°ç³»ç»Ÿçš„å…¨æ ˆä¼˜åŒ–

### å®è·µæŒ‡å—

**æ¨¡å‹è®¾è®¡é˜¶æ®µ**ï¼š
- **å°å‹æ¨¡å‹**ï¼šä½¿ç”¨æ ‡å‡†MHAï¼Œè¿½æ±‚æœ€ä½³æ€§èƒ½
- **ä¸­å‹æ¨¡å‹**ï¼šè€ƒè™‘MQAï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡
- **å¤§å‹æ¨¡å‹**ï¼šé‡‡ç”¨GQAï¼Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ä¼˜åŒ–èµ„æº

**æ¨ç†éƒ¨ç½²é˜¶æ®µ**ï¼š
- **ä¼˜å…ˆä¼˜åŒ–KVç¼“å­˜**ï¼šè¿™æ˜¯æ¨ç†æ€§èƒ½çš„å…³é”®ç“¶é¢ˆ
- **å®ç°åŠ¨æ€æ‰¹å¤„ç†**ï¼šæé«˜GPUåˆ©ç”¨ç‡
- **ä½¿ç”¨FlashAttention**ï¼šå‡å°‘IOå¼€é”€

**è®­ç»ƒä¼˜åŒ–é˜¶æ®µ**ï¼š
- **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šå¿…é€‰ä¼˜åŒ–ï¼ŒåŒæ—¶æå‡é€Ÿåº¦å’Œå‡å°‘å†…å­˜
- **æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼šå†…å­˜ä¸è¶³æ—¶çš„æœ‰æ•ˆæ–¹æ¡ˆ
- **åˆç†æ‰¹å¤§å°**ï¼šå¹³è¡¡å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒæ•ˆç‡

### æœªæ¥å‘å±•è¶‹åŠ¿

1. **æ›´é«˜æ•ˆçš„Attentionå˜ä½“**ï¼šç»§ç»­æ¢ç´¢æ€§èƒ½ä¸æ•ˆç‡çš„å¹³è¡¡
2. **ç¡¬ä»¶ååŒè®¾è®¡**ï¼šé’ˆå¯¹Attentionçš„ä¸“ç”¨èŠ¯ç‰‡ä¼˜åŒ–
3. **è‡ªé€‚åº”æ¶æ„**ï¼šæ ¹æ®ä»»åŠ¡åŠ¨æ€é€‰æ‹©æœ€ä¼˜Attentionç­–ç•¥
4. **åˆ†å¸ƒå¼Attention**ï¼šè·¨è®¾å¤‡çš„å¤§è§„æ¨¡Attentionè®¡ç®—

---

**è®°ä½**ï¼šAttentionä¸ä»…æ˜¯ä¸€ä¸ªç®—æ³•æ¨¡å—ï¼Œæ›´æ˜¯æ•´ä¸ªLLMæ¶æ„çš„æ ¸å¿ƒã€‚ç†è§£Attentionåœ¨LLMä¸­çš„å®é™…åº”ç”¨ï¼Œå°±æŒæ¡äº†ç°ä»£AIç³»ç»Ÿçš„å…³é”®ä¼˜åŒ–æŠ€æœ¯ã€‚ä»ç®—æ³•è®¾è®¡åˆ°ç³»ç»Ÿä¼˜åŒ–ï¼Œä»è®­ç»ƒç­–ç•¥åˆ°éƒ¨ç½²æ–¹æ¡ˆï¼ŒAttentionæŠ€æœ¯çš„æ¯ä¸€ä¸ªç¯èŠ‚éƒ½å€¼å¾—æ·±å…¥ç ”ç©¶å’Œç²¾å¿ƒè®¾è®¡ã€‚

*æœ€åä¸€ç¯‡æ–‡ç« å°†æä¾›Attentionæ€§èƒ½ä¼˜åŒ–çš„ç»ˆææŒ‡å—ï¼Œä»ç®—æ³•åˆ°ç¡¬ä»¶çš„å…¨æ ˆä¼˜åŒ–ç­–ç•¥ã€‚* ğŸš€