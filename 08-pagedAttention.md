# PagedAttentionï¼šè§£å†³é•¿åºåˆ—å†…å­˜ç“¶é¢ˆçš„é©å‘½æ€§æ–¹æ¡ˆ

## ğŸ¯ å¼•è¨€

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ è¦å¤„ç†ä¸€æœ¬1ä¸‡é¡µçš„ä¹¦ç±ï¼Œéœ€è¦åœ¨æ¯ä¸€é¡µä¸Šè®°å½•ä¸å…¶ä»–æ‰€æœ‰é¡µé¢çš„å…³è”åº¦ã€‚ä¼ ç»Ÿæ–¹æ³•æ˜¯ï¼š

1. **ç®€å•åšæ³•**ï¼šæŠŠæ‰€æœ‰1ä¸‡é¡µçš„å†…å®¹éƒ½è®°åœ¨è„‘å­é‡Œ â†’ å¤§è„‘å®¹é‡çˆ†ç‚¸ ğŸ˜µ
2. **åˆ†é¡µåšæ³•**ï¼šæŠŠä¹¦åˆ†æˆç« èŠ‚ï¼Œåªè®°ä½å½“å‰ç« èŠ‚çš„å†…å®¹ â†’ ä½†æ— æ³•è·¨ç« èŠ‚å…³è” ğŸ“š

PagedAttentionæä¾›äº†ä¸€ä¸ªå·§å¦™çš„è§£å†³æ–¹æ¡ˆï¼š**æŠŠé•¿åºåˆ—åˆ†é¡µå­˜å‚¨ï¼ŒæŒ‰éœ€åŠ è½½**ã€‚å°±åƒå›¾ä¹¦é¦†çš„ç´¢å¼•ç³»ç»Ÿâ€”â€”ä½ ä¸éœ€è¦æŠŠæ‰€æœ‰ä¹¦éƒ½å¸¦åœ¨èº«ä¸Šï¼Œåªéœ€è¦åœ¨éœ€è¦æ—¶æŸ¥æ‰¾ç‰¹å®šçš„é¡µé¢ã€‚

PagedAttentionæ˜¯vLLMï¼ˆVirtual Large Language Modelsï¼‰æ¡†æ¶çš„æ ¸å¿ƒåˆ›æ–°ï¼Œå®ƒå°†æ“ä½œç³»ç»Ÿä¸­çš„åˆ†é¡µè™šæ‹Ÿå†…å­˜æ¦‚å¿µå¼•å…¥åˆ°Attentionæœºåˆ¶ä¸­ï¼Œä½¿å¾—é•¿åºåˆ—æ¨ç†å˜å¾—å¯è¡Œã€‚æœ¬æ–‡å°†æ·±å…¥è§£æPagedAttentionçš„åŸç†ã€å®ç°å’Œå®é™…åº”ç”¨ã€‚

## ğŸ§  é•¿åºåˆ—çš„å†…å­˜æŒ‘æˆ˜

### å†…å­˜å¤æ‚åº¦åˆ†æ

è®©æˆ‘ä»¬å…ˆç†è§£ä¸ºä»€ä¹ˆé•¿åºåˆ—ä¼šå¸¦æ¥å†…å­˜é—®é¢˜ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt
import torch

def analyze_memory_scaling():
    """åˆ†æAttentionæœºåˆ¶çš„å†…å­˜æ‰©å±•æ€§"""

    print("Attentionæœºåˆ¶å†…å­˜æ‰©å±•æ€§åˆ†æ")
    print("=" * 60)

    def memory_usage_mb(seq_len, d_model, batch_size=1):
        """è®¡ç®—ä¸åŒåºåˆ—é•¿åº¦çš„å†…å­˜ä½¿ç”¨é‡"""
        # KVç¼“å­˜ï¼š2 * seq_len * d_model * batch_size * 4 bytes
        kv_cache = 2 * seq_len * d_model * 4 / 1024 / 1024
        # æ³¨æ„åŠ›çŸ©é˜µï¼šseq_len * seq_len * batch_size * 4 bytes
        attention_matrix = seq_len * seq_len * 4 / 1024 / 1024
        # æ€»å†…å­˜
        total = kv_cache + attention_matrix
        return {
            'kv_cache': kv_cache,
            'attention_matrix': attention_matrix,
            'total': total
        }

    # åˆ†æä¸åŒåºåˆ—é•¿åº¦
    seq_lengths = [512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]
    d_model = 4096  # 7Bæ¨¡å‹çš„æ ‡å‡†é…ç½®

    print(f"æ¨¡å‹ç»´åº¦: {d_model}")
    print(f"{'åºåˆ—é•¿åº¦':<10} {'KVç¼“å­˜(MB)':<12} {'æ³¨æ„åŠ›çŸ©é˜µ(MB)':<15} {'æ€»å†…å­˜(MB)':<12}")
    print("-" * 60)

    memory_data = []
    for seq_len in seq_lengths:
        usage = memory_usage_mb(seq_len, d_model)
        memory_data.append(usage)
        print(f"{seq_len:<10} {usage['kv_cache']:<12.2f} {usage['attention_matrix']:<15.2f} {usage['total']:<12.2f}")

    # å¯è§†åŒ–å†…å­˜å¢é•¿
    plt.figure(figsize=(15, 10))

    # å­å›¾1: æ€»å†…å­˜ä½¿ç”¨
    plt.subplot(2, 3, 1)
    total_memory = [m['total'] for m in memory_data]
    plt.plot(seq_lengths, total_memory, 'b-o', linewidth=2, markersize=8)
    plt.xlabel('åºåˆ—é•¿åº¦')
    plt.ylabel('æ€»å†…å­˜ä½¿ç”¨ (MB)')
    plt.title('æ€»å†…å­˜ä½¿ç”¨ vs åºåˆ—é•¿åº¦')
    plt.grid(True, alpha=0.3)
    plt.yscale('log')

    # å­å›¾2: å†…å­˜ç»„æˆ
    plt.subplot(2, 3, 2)
    kv_memory = [m['kv_cache'] for m in memory_data]
    attention_memory = [m['attention_matrix'] for m in memory_data]
    plt.stackplot(seq_lengths, [kv_memory, attention_memory],
                   labels=['KVç¼“å­˜', 'æ³¨æ„åŠ›çŸ©é˜µ'], alpha=0.7)
    plt.xlabel('åºåˆ—é•¿åº¦')
    plt.ylabel('å†…å­˜ä½¿ç”¨ (MB)')
    plt.title('å†…å­˜ç»„æˆåˆ†æ')
    plt.legend()
    plt.yscale('log')

    # å­å›¾3: å†…å­˜å æ¯”
    plt.subplot(2, 3, 3)
    kv_ratio = [m['kv_cache'] / m['total'] * 100 for m in memory_data]
    attention_ratio = [m['attention_matrix'] / m['total'] * 100 for m in memory_data]
    plt.plot(seq_lengths, kv_ratio, 'g-o', label='KVç¼“å­˜å æ¯”')
    plt.plot(seq_lengths, attention_ratio, 'r-s', label='æ³¨æ„åŠ›çŸ©é˜µå æ¯”')
    plt.xlabel('åºåˆ—é•¿åº¦')
    plt.ylabel('å æ¯” (%)')
    plt.title('å†…å­˜ä½¿ç”¨å æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å­å›¾4: å¢é•¿ç‡åˆ†æ
    plt.subplot(2, 3, 4)
    growth_rate = [memory_data[i]['total'] / memory_data[i-1]['total'] for i in range(1, len(memory_data))]
    plt.plot(seq_lengths[1:], growth_rate, 'purple', marker='^', linewidth=2)
    plt.xlabel('åºåˆ—é•¿åº¦')
    plt.ylabel('å†…å­˜å¢é•¿ç‡')
    plt.title('å†…å­˜ä½¿ç”¨å¢é•¿ç‡')
    plt.grid(True, alpha=0.3)

    # å­å›¾5: ç°å®ä¸–ç•Œå¯¹æ¯”
    plt.subplot(2, 3, 5)
    gpu_memory = [16, 24, 40, 80]  # ä¸åŒGPUçš„å†…å­˜
    seq_lengths_plot = [4096, 8192, 16384, 32768]

    for i, (mem, seq_len) in enumerate(zip(gpu_memory, seq_lengths_plot)):
        req_memory = memory_usage_mb(seq_len, d_model)['total']
        plt.scatter(seq_len, req_memory, s=200, alpha=0.6,
                   label=f'{mem}GB GPU' if i == 0 else '')
        plt.axhline(y=mem, color='red', linestyle='--', alpha=0.5)

    plt.xlabel('åºåˆ—é•¿åº¦')
    plt.ylabel('å†…å­˜ä½¿ç”¨ (MB)')
    plt.title('ç°å®GPUå†…å­˜é™åˆ¶')
    plt.legend()
    plt.yscale('log')
    plt.grid(True, alpha=0.3)

    # å­å›¾6: æˆæœ¬ä¼°ç®—
    plt.subplot(2, 3, 6)
    memory_gb = [m['total'] / 1024 for m in memory_data]
    cost_per_gb = 4.0  # å‡è®¾æ¯GBå†…å­˜æˆæœ¬$4
    total_cost = [m * cost_per_gb for m in memory_gb]

    plt.plot(seq_lengths, total_cost, 'orange', marker='D', linewidth=2, markersize=8)
    plt.xlabel('åºåˆ—é•¿åº¦')
    plt.ylabel('å†…å­˜æˆæœ¬ ($)')
    plt.title('å†…å­˜æˆæœ¬ä¼°ç®—')
    plt.grid(True, alpha=0.3)
    plt.yscale('log')

    plt.tight_layout()
    plt.show()

    # åˆ†æç»“æœ
    print("\nå…³é”®è§‚å¯Ÿ:")
    print("-" * 30)
    print("1. æ³¨æ„åŠ›çŸ©é˜µå†…å­˜å ç”¨éšåºåˆ—é•¿åº¦å¹³æ–¹å¢é•¿")
    print("2. KVç¼“å­˜å†…å­˜å ç”¨éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿")
    print("3. é•¿åºåˆ—ä¸‹ï¼Œæ³¨æ„åŠ›çŸ©é˜µæˆä¸ºä¸»è¦ç“¶é¢ˆ")
    print("4. ç°å®GPUå†…å­˜é™åˆ¶äº†æœ€å¤§å¯å¤„ç†åºåˆ—é•¿åº¦")

analyze_memory_scaling()

def practical_memory_bottleneck():
    """å®é™…çš„å†…å­˜ç“¶é¢ˆåœºæ™¯åˆ†æ"""

    print("\nå®é™…åœºæ™¯ä¸­çš„å†…å­˜ç“¶é¢ˆåˆ†æ")
    print("=" * 60)

    scenarios = [
        {
            "åœºæ™¯": "GPT-2æ¨ç† (7Bæ¨¡å‹)",
            "d_model": 4096,
            "seq_len": 2048,
            "gpu_memory": 16,  # GB
            "feasible": False
        },
        {
            "åœºæ™¯": "GPT-2æ¨ç† (7Bæ¨¡å‹)",
            "d_model": 4096,
            "seq_len": 1024,
            "gpu_memory": 16,  # GB
            "feasible": True
        },
        {
            "åœºæ™¯": "GPT-3æ¨ç† (175Bæ¨¡å‹)",
            "d_model": 12288,
            "seq_len": 4096,
            "gpu_memory": 80,  # GB
            "feasible": False
        },
        {
            "åœºæ™¯": "GPT-3æ¨ç† (175Bæ¨¡å‹)",
            "d_model": 12288,
            "seq_len": 512,
            "gpu_memory": 80,  # GB
            "feasible": True
        },
        {
            "åœºæ™¯": "é•¿æ–‡æ¡£ç†è§£",
            "d_model": 4096,
            "seq_len": 16384,
            "gpu_memory": 80,  # GB
            "feasible": False
        },
        {
            "åœºæ™¯": "è¶…é•¿åºåˆ—ç”Ÿæˆ",
            "d_model": 4096,
            "seq_len": 65536,
            "gpu_memory": 40,  # GB
            "feasible": False
        }
    ]

    print(f"{'åœºæ™¯':<25} {'æ¨¡å‹ç»´åº¦':<10} {'åºåˆ—é•¿åº¦':<10} {'GPUå†…å­˜':<10} {'å¯è¡Œ':<6}")
    print("-" * 70)

    for scenario in scenarios:
        required_memory = memory_usage_mb(scenario["seq_len"], scenario["d_model"])["total"]
        gpu_memory_gb = scenario["gpu_memory"]

        feasible = required_memory < gpu_memory_gb * 1024
        status = "âœ…" if feasible else "âŒ"

        print(f"{scenario['åœºæ™¯']:<25} {scenario['d_model']:<10} "
              f"{scenario['seq_len']:<10} {scenario['gpu_memory']:<10}GB {status}")

        if not feasible:
            shortage = required_memory - gpu_memory_gb * 1024
            print(f"  å†…å­˜ä¸è¶³: {shortage:.1f}MB")

practical_memory_bottleneck()
```

## ğŸ§© PagedAttentionæ ¸å¿ƒåŸç†

### åˆ†é¡µè™šæ‹Ÿå†…å­˜çš„æ¦‚å¿µ

PagedAttentionå€Ÿé‰´äº†æ“ä½œç³»ç»Ÿçš„åˆ†é¡µè™šæ‹Ÿå†…å­˜æœºåˆ¶ï¼š

```python
def explain_paging_concept():
    """è§£é‡Šåˆ†é¡µè™šæ‹Ÿå†…å­˜æ¦‚å¿µ"""

    print("åˆ†é¡µè™šæ‹Ÿå†…å­˜æ¦‚å¿µè§£æ")
    print("=" * 60)

    print("ğŸ“š æ“ä½œç³»ç»Ÿçš„åˆ†é¡µæœºåˆ¶:")
    print("  - é¡µé¢å¤§å°: 4KB")
    print("  - è™šæ‹Ÿåœ°å€ â†’ ç‰©ç†åœ°å€æ˜ å°„")
    print("  - æŒ‰éœ€åŠ è½½ï¼ŒèŠ‚çœç‰©ç†å†…å­˜")
    print()

    print("ğŸ¤– PagedAttentionçš„åˆ†é¡µæœºåˆ¶:")
    print("  - å—å¤§å°: 16ä¸ªtoken (å¯é…ç½®)")
    print("  - é€»è¾‘åœ°å€ â†’ ç‰©ç†åœ°å€æ˜ å°„")
    print("  - æŒ‰éœ€åŠ è½½ï¼ŒèŠ‚çœGPUå†…å­˜")
    print()

    print("ç±»æ¯”ç†è§£:")
    print("-" * 30)
    print("ä¼ ç»ŸAttention:")
    print("  ç±»æ¯”äºï¼šæŠŠæ•´æœ¬ä¹¦èƒŒä¸‹æ¥ â†’ è®°å¿†åŠ›æœ‰é™")
    print("  å®é™…ï¼šæŠŠæ‰€æœ‰KVéƒ½ä¿å­˜åœ¨æ˜¾å­˜ä¸­")
    print()

    print("PagedAttention:")
    print("  ç±»æ¯”äºï¼šåªè®°ä½å½“å‰é¡µçš„é¡µç  â†’ æŒ‰éœ€æŸ¥æ‰¾")
    print("  å®é™…ï¼šåªä¿å­˜å½“å‰å—çš„KVï¼Œå…¶ä»–æŒ‰éœ€ä»CPUå†…å­˜åŠ è½½")
    print()

def paged_attention_workflow():
    """å±•ç¤ºPagedAttentionçš„å·¥ä½œæµç¨‹"""

    print("PagedAttentionå·¥ä½œæµç¨‹")
    print("=" * 60)

    # æ¨¡æ‹Ÿå‚æ•°
    seq_len = 16
    block_size = 4
    d_model = 8

    print(f"å‚æ•°: åºåˆ—é•¿åº¦={seq_len}, å—å¤§å°={block_size}, æ¨¡å‹ç»´åº¦={d_model}")
    print(f"é¡µæ•°: {seq_len // block_size}")
    print()

    # æ¨¡æ‹Ÿé¡µè¡¨
    page_table = []
    for i in range(0, seq_len, block_size):
        page_table.append({
            'page_id': i // block_size,
            'block_indices': list(range(i, min(i + block_size, seq_len))),
            'physical_location': f"GPUå†…å­˜å—{i//block_size}",
            'loaded': False
        })

    print("é¡µè¡¨ç»“æ„:")
    print("-" * 30)
    for page in page_table:
        print(f"é¡µ {page['page_id']:2d}: ç´¢å¼• {page['block_indices']} "
              f"ä½ç½®: {page['physical_location']} åŠ è½½: {page['loaded']}")

    print(f"\nå·¥ä½œæµç¨‹:")
    print("-" * 30)

    # æ¨¡æ‹ŸæŸ¥è¯¢è¿‡ç¨‹
    query_positions = [3, 7, 12, 15]

    for q_pos in query_positions:
        page_id = q_pos // block_size
        block_pos = q_pos % block_size
        target_page = page_table[page_id]

        print(f"\næŸ¥è¯¢ä½ç½® {q_pos}:")
        print(f"  éœ€è¦é¡µ {page_id} ä¸­çš„å— {block_pos}")

        # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½
        if not target_page['loaded']:
            print(f"  é¡µ {page_id} æœªåŠ è½½ï¼Œä»CPUå†…å­˜åŠ è½½...")
            print(f"  åŠ è½½å— {target_page['block_indices']} åˆ° {target_page['physical_location']}")
            target_page['loaded'] = True
        else:
            print(f"  é¡µ {page_id} å·²åŠ è½½ï¼Œç›´æ¥ä½¿ç”¨")

        print(f"  ä» {target_page['physical_location']} ä¸­è·å–å— {block_pos}")

    print(f"\næ€»ç»“:")
    print("-" * 30)
    print("1. åªåŠ è½½æŸ¥è¯¢éœ€è¦çš„é¡µï¼ŒèŠ‚çœå†…å­˜")
    print("2. å¯ä»¥å¤„ç†ä»»æ„é•¿åº¦çš„åºåˆ—")
    print("3. æŒ‰éœ€åŠ è½½ï¼Œå‡å°‘IOå¼€é”€")

explain_paging_concept()
paged_attention_workflow()
```

### æ ¸å¿ƒç®—æ³•å®ç°

```python
import torch
import torch.nn.functional as F
from typing import Optional, List, Tuple

class PagedAttention:
    """PagedAttentionçš„å®ç°"""

    def __init__(self, block_size: int, num_heads: int, max_cache_size: int = 32):
        """
        åˆå§‹åŒ–PagedAttention

        Args:
            block_size: æ¯å—çš„å¤§å°ï¼ˆtokenæ•°é‡ï¼‰
            num_heads: æ³¨æ„åŠ›å¤´çš„æ•°é‡
            max_cache_size: æœ€å¤§ç¼“å­˜é¡µæ•°
        """
        self.block_size = block_size
        self.num_heads = num_heads
        self.max_cache_size = max_cache_size

        # é¡µè¡¨ç®¡ç†
        self.page_table = {}  # é¡µID â†’ é¡µé¢ä¿¡æ¯
        self.cache_pages = []    # å·²ç¼“å­˜çš„é¡µé¢
        self.free_pages = []     # å¯ç”¨é¡µé¢åˆ—è¡¨

        # ç»Ÿè®¡ä¿¡æ¯
        self.cache_hits = 0
        self.cache_misses = 0
        self.evictions = 0

    def allocate_page(self, page_id: int, seq_len: int, d_model: int, device: torch.device):
        """åˆ†é…æ–°çš„é¡µé¢"""
        if page_id in self.page_table:
            return self.page_table[page_id]

        # åˆ›å»ºæ–°é¡µé¢
        page = {
            'page_id': page_id,
            'seq_len': seq_len,
            'd_model': d_model,
            'device': device,
            'block_indices': None,  # å°†åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®
            'k_cache': None,      # [block_size, d_model]
            'v_cache': None,      # [block_size, d_model]
            'last_access': 0
        }

        # åˆå§‹åŒ–å—ç´¢å¼•
        start_idx = page_id * self.block_size
        end_idx = min(start_idx + self.block_size, seq_len)
        page['block_indices'] = list(range(start_idx, end_idx))
        page['k_cache'] = torch.zeros(self.block_size, d_model, device=device)
        page['v_cache'] = torch.zeros(self.block_size, d_model, device=device)

        # é¡µé¢æ›¿æ¢ç­–ç•¥ï¼ˆLRUï¼‰
        if len(self.cache_pages) >= self.max_cache_size:
            self._evict_page()

        # æ·»åŠ åˆ°ç¼“å­˜
        self.cache_pages.append(page)
        self.page_table[page_id] = page

        print(f"åˆ†é…é¡µ {page_id}ï¼Œå—èŒƒå›´: {page['block_indices']}")

        return page

    def _evict_page(self):
        """æ·˜æ±°æœ€ä¹…æœªä½¿ç”¨çš„é¡µé¢"""
        if not self.cache_pages:
            return

        # æ‰¾åˆ°æœ€ä¹…æœªä½¿ç”¨çš„é¡µé¢
        lru_page = min(self.cache_pages, key=lambda p: p['last_access'])

        print(f"æ·˜æ±°é¡µ {lru_page['page_id']}ï¼Œæœ€åè®¿é—®æ—¶é—´: {lru_page['last_access']}")

        # ä»ç¼“å­˜ä¸­ç§»é™¤
        self.cache_pages.remove(lru_page)
        del self.page_table[lru_page['page_id']]

        self.evictions += 1

    def get_block(self, page_id: int, block_idx: int):
        """è·å–ç‰¹å®šå—"""
        if page_id not in self.page_table:
            raise ValueError(f"é¡µ {page_id} ä¸å­˜åœ¨")

        page = self.page_table[page_id]

        # æ£€æŸ¥å—ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
        if block_idx not in page['block_indices']:
            raise ValueError(f"å— {block_idx} ä¸åœ¨é¡µ {page_id} ä¸­")

        # æ›´æ–°è®¿é—®æ—¶é—´
        page['last_access'] = len(self.cache_pages)
        self.cache_hits += 1

        return page['k_cache'][block_idx], page['v_cache'][block_idx]

    def forward(self, query, key, value, attention_mask=None):
        """
        PagedAttentionå‰å‘ä¼ æ’­

        Args:
            query: [batch_size, seq_len, d_model]
            key: [batch_size, seq_len, d_model]
            value: [batch_size, seq_len, d_model]
            attention_mask: [batch_size, seq_len, seq_len] (å¯é€‰)

        Returns:
            output: [batch_size, seq_len, d_model]
        """
        batch_size, seq_len, d_model = query.shape
        num_blocks = (seq_len + self.block_size - 1) // self.block_size

        print(f"PagedAttentionå‰å‘ä¼ æ’­")
        print(f"  è¾“å…¥å½¢çŠ¶: {query.shape}")
        print(f"  å—å¤§å°: {self.block_size}, å—æ•°é‡: {num_blocks}")
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {self.num_heads}")

        # åˆå§‹åŒ–è¾“å‡º
        output = torch.zeros_like(query)

        # ä¸ºæ¯ä¸ªæŸ¥è¯¢ä½ç½®è®¡ç®—Attention
        for q_idx in range(seq_len):
            print(f"\nå¤„ç†æŸ¥è¯¢ä½ç½® {q_idx}:")

            # æŸ¥æ‰¾å¯¹åº”çš„é¡µ
            page_id = q_idx // self.block_size
            if page_id >= num_blocks:
                print(f"   é¡µ {page_id} è¶…å‡ºèŒƒå›´")
                continue

            # è·å–é¡µï¼ˆæŒ‰éœ€åˆ†é…ï¼‰
            page = self.allocate_page(page_id, seq_len, d_model, query.device)

            # è®¡ç®—è¯¥æŸ¥è¯¢ä¸é¡µé¢ä¸­æ‰€æœ‰é”®çš„æ³¨æ„åŠ›
            k_block = page['k_cache']
            v_block = page['v_cache']

            # è·å–è¯¥æŸ¥è¯¢åœ¨é¡µé¢ä¸­çš„ä½ç½®
            local_idx = q_idx % self.block_size
            if local_idx >= len(k_block):
                print(f"  æœ¬åœ°ç´¢å¼• {local_idx} è¶…å‡ºé¡µé¢èŒƒå›´")
                continue

            # æå–æŸ¥è¯¢å‘é‡
            q_vec = query[:, q_idx:q_idx+1, :].expand(-1, local_idx + 1, -1)

            # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆåªä¸å½“å‰é¡µçš„å—ï¼‰
            scores = torch.matmul(q_vec, k_block.transpose(-2, -1))

            # ç¼©æ”¾
            scale = 1.0 / math.sqrt(d_model)
            scores = scores * scale

            # åº”ç”¨æ©ç ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if attention_mask is not None:
                # è·å–å¯¹åº”çš„æ©ç å—
                mask_block = attention_mask[:, q_idx:q_idx+1, page['block_indices'][0]:page['block_indices'][-1]]
                scores = scores.masked_fill(mask_block == 0, -1e9)

            # Softmax
            weights = F.softmax(scores, dim=-1)

            # åŠ æƒæ±‚å’Œ
            output[:, q_idx:q_idx+1, :] = torch.matmul(weights, v_block)

        print(f"\nç¼“å­˜ç»Ÿè®¡:")
        print(f"  ç¼“å­˜å‘½ä¸­: {self.cache_hits}")
        print(f"  ç¼“å­˜æœªå‘½ä¸­: {self.cache_misses}")
        print(f"  é¡µé¢æ·˜æ±°: {self.evictions}")
        print(f"  ç¼“å­˜å‘½ä¸­ç‡: {self.cache_hits/(self.cache_hits + self.cache_misses)*100:.1f}%")

        return output

# æµ‹è¯•PagedAttention
def test_paged_attention():
    """æµ‹è¯•PagedAttentionå®ç°"""

    print("PagedAttentionå®ç°æµ‹è¯•")
    print("=" * 60)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, d_model = 2, 16, 8
    torch.manual_seed(42)

    query = torch.randn(batch_size, seq_len, d_model)
    key = torch.randn(batch_size, seq_len, d_model)
    value = torch.randn(batch_size, seq_len, d_model)

    # åˆ›å»ºå› æœæ©ç 
    mask = torch.tril(torch.ones(seq_len, seq_len))

    # å®ä¾‹åŒ–PagedAttention
    paged_attn = PagedAttention(block_size=4, num_heads=1, max_cache_size=4)

    # å‰å‘ä¼ æ’­
    output = paged_attn.forward(query, key, value, mask)

    print(f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å‡ºç»Ÿè®¡: å‡å€¼={output.mean():.4f}, æ ‡å‡†å·®={output.std():.4f}")

    # éªŒè¯ä¸æ ‡å‡†Attentionçš„ä¸€è‡´æ€§
    print(f"\nä¸æ ‡å‡†Attentionå¯¹æ¯”:")
    from sklearn.metrics import mean_squared_error

    # æ ‡å‡†Attention
    standard_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_model)
    standard_weights = F.softmax(standard_scores, dim=-1)
    standard_output = torch.matmul(standard_weights, value)

    mse = mean_squared_error(output.flatten().numpy(), standard_output.flatten().numpy())
    print(f"MSE: {mse:.6f}")
    print(f"ç²¾åº¦åŒ¹é…: {'âœ“ ä¼˜ç§€' if mse < 1e-6 else 'âœ— éœ€è¦æ£€æŸ¥'}")

test_paged_attention()
```

## ğŸ”§ è¯¦ç»†å®ç°ï¼šè™šæ‹Ÿå†…å­˜ç®¡ç†

### è™šæ‹Ÿåœ°å€åˆ°ç‰©ç†åœ°å€çš„æ˜ å°„

```python
class VirtualMemoryManager:
    """è™šæ‹Ÿå†…å­˜ç®¡ç†å™¨"""

    def __init__(self, block_size: int, max_physical_blocks: int = 32):
        """
        è™šæ‹Ÿå†…å­˜ç®¡ç†å™¨

        Args:
            block_size: å—å¤§å°
            max_physical_blocks: æœ€å¤§ç‰©ç†å—æ•°
        """
        self.block_size = block_size
        self.max_physical_blocks = max_physical_blocks

        # è™šæ‹Ÿåœ°å€ç©ºé—´
        self.virtual_pages = {}  # è™šæ‹Ÿé¡µID â†’ è™šæ‹Ÿé¡µé¢ä¿¡æ¯
        self.next_virtual_page_id = 0

        # ç‰©ç†åœ°å€ç©ºé—´
        self.physical_blocks = list(range(max_physical_blocks))
        self.free_physical_blocks = list(range(max_physical_blocks))

        # æ˜ å°„è¡¨ï¼šè™šæ‹Ÿé¡µID â†’ ç‰©ç†å—ID
        self.page_mapping = {}

        # ç»Ÿè®¡ä¿¡æ¯
        self.page_faults = 0
        self.page_hits = 0
        self.total_requests = 0

    def allocate_virtual_page(self, seq_len: int, d_model: int, device: torch.device):
        """åˆ†é…è™šæ‹Ÿé¡µ"""
        virtual_page_id = self.next_virtual_page_id
        self.next_virtual_page_id += 1

        num_blocks = (seq_len + block_size - 1) // block_size

        virtual_page = {
            'virtual_id': virtual_page_id,
            'num_blocks': num_blocks,
            'seq_len': seq_len,
            'd_model': d_model,
            'device': device,
            'physical_blocks': [],
            'virtual_to_physical': {},
            'last_access': 0
        }

        # åˆå§‹åŒ–è™šæ‹Ÿåˆ°ç‰©ç†æ˜ å°„
        for i in range(num_blocks):
            virtual_page['virtual_to_physical'][i] = -1  # -1è¡¨ç¤ºæœªåˆ†é…

        self.virtual_pages[virtual_page_id] = virtual_page
        return virtual_page

    def allocate_physical_block(self, virtual_page: int, block_idx: int) -> int:
        """åˆ†é…ç‰©ç†å—"""
        if virtual_page not in self.virtual_pages:
            raise ValueError(f"è™šæ‹Ÿé¡µ {virtual_page} ä¸å­˜åœ¨")

        vp = self.virtual_pages[virtual_page]

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç‰©ç†å—
        if vp['virtual_to_physical'][block_idx] != -1:
            self.page_hits += 1
            return vp['virtual_to_physical'][block_idx]

        # é¡µé¢é”™è¯¯ï¼šéœ€è¦åˆ†é…ç‰©ç†å—
        self.page_faults += 1
        self.total_requests += 1

        if not self.free_physical_blocks:
            # ç‰©ç†å†…å­˜ä¸è¶³ï¼Œéœ€è¦æ·˜æ±°ä¸€ä¸ªé¡µ
            self._evict_oldest_page()

        # åˆ†é…ç‰©ç†å—
        physical_block_id = self.free_physical_blocks.pop(0)
        vp['virtual_to_physical'][block_idx] = physical_block_id
        vp['physical_blocks'].append(physical_block_id)

        print(f"åˆ†é…ç‰©ç†å— {physical_block_id} ç»™è™šæ‹Ÿé¡µ {virtual_page} çš„å— {block_idx}")

        return physical_block_id

    def _evict_oldest_page(self):
        """æ·˜æ±°æœ€è€çš„é¡µ"""
        # æ‰¾åˆ°æœ€ä¹…æœªè®¿é—®çš„è™šæ‹Ÿé¡µ
        oldest_page = min(self.virtual_pages.values(), key=lambda p: p['last_access'])

        print(f"æ·˜æ±°è™šæ‹Ÿé¡µ {oldest_page['virtual_id']}")

        # é‡Šæ”¾æ‰€æœ‰ç‰©ç†å—
        for physical_block_id in oldest_page['physical_blocks']:
            if physical_block_id in self.free_physical_blocks:
                print(f"  ç‰©ç†å— {physical_block_id} å·²æ˜¯ç©ºé—²çš„")
            else:
                self.free_physical_blocks.append(physical_block_id)
                print(f"  é‡Šæ”¾ç‰©ç†å— {physical_block_id}")

        # æ¸…ç†æ˜ å°„
        for virtual_idx, physical_idx in oldest_page['virtual_to_physical'].items():
            if physical_idx != -1:
                del oldest_page['virtual_to_physical'][virtual_idx]

        # ä»è™šæ‹Ÿåœ°å€ç©ºé—´ç§»é™¤
        del self.virtual_pages[oldest_page['virtual_id']]

    def translate_address(self, virtual_page_id: int, block_idx: int):
        """è™šæ‹Ÿåœ°å€åˆ°ç‰©ç†åœ°å€çš„è½¬æ¢"""
        if virtual_page_id not in self.virtual_pages:
            raise ValueError(f"è™šæ‹Ÿé¡µ {virtual_page_id} ä¸å­˜åœ¨")

        vp = self.virtual_pages[virtual_page_id]

        if block_idx >= vp['num_blocks']:
            raise ValueError(f"å—ç´¢å¼• {block_idx} è¶…å‡ºèŒƒå›´")

        # è·å–ç‰©ç†å—
        physical_block_id = self.allocate_physical_block(virtual_page['virtual_id'], block_idx)

        # æ›´æ–°è®¿é—®æ—¶é—´
        vp['last_access'] = self.total_requests

        return physical_block_id

    def get_cache_statistics(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.page_hits + self.page_faults
        if total_requests == 0:
            return {
                'hit_rate': 0.0,
                'total_requests': 0,
                'cache_hits': 0,
                'page_faults': 0,
                'evictions': len([p for p in self.virtual_pages.values() if len(p['physical_blocks']) == 0])
            }

        return {
            'hit_rate': self.page_hits / total_requests,
            'total_requests': total_requests,
            'cache_hits': self.page_hits,
            'page_faults': self.page_faults,
            'evictions': len([p for p in self.virtual_pages.values() if len(p['physical_blocks']) == 0])
        }

# æµ‹è¯•è™šæ‹Ÿå†…å­˜ç®¡ç†å™¨
def test_virtual_memory_manager():
    """æµ‹è¯•è™šæ‹Ÿå†…å­˜ç®¡ç†å™¨"""

    print("è™šæ‹Ÿå†…å­˜ç®¡ç†å™¨æµ‹è¯•")
    print("=" * 60)

    vm = VirtualMemoryManager(block_size=4, max_physical_blocks=4)

    # åˆ›å»ºè™šæ‹Ÿé¡µé¢
    pages = []
    for i in range(8):
        page = vm.allocate_virtual_page(seq_len=16, d_model=8, device='cpu')
        pages.append(page)
        print(f"åˆ†é…è™šæ‹Ÿé¡µ {i}")

    print(f"\nè™šæ‹Ÿå†…å­˜çŠ¶æ€:")
    print(f"  è™šæ‹Ÿé¡µæ•°: {len(vm.virtual_pages)}")
    print(f"  å¯ç”¨ç‰©ç†å—: {len(vm.free_physical_blocks)}")

    # æ¨¡æ‹Ÿéšæœºè®¿é—®
    import random
    random.seed(42)
    access_pattern = random.choices(range(16), k=20)

    print(f"\nè®¿é—®æ¨¡å¼: {access_pattern}")

    for i, pos in enumerate(access_pattern):
        page_id = pos // 4
        block_idx = pos % 4
        virtual_page = pages[page_id]

        print(f"\nè®¿é—® {i}: ä½ç½® {pos} (é¡µ {page_id}, å— {block_idx})")
        physical_block = vm.translate_address(page_id, block_idx)
        print(f"  ç‰©ç†å—: {physical_block}")

    print(f"\næœ€ç»ˆç»Ÿè®¡:")
    stats = vm.get_cache_statistics()
    print(f"  ç¼“å­˜å‘½ä¸­ç‡: {stats['hit_rate']:.2%}")
    print(f"  æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
    print(f"  ç¼“å­˜å‘½ä¸­: {stats['cache_hits']}")
    print(f"  é¡µé¢é”™è¯¯: {stats['page_faults']}")
    print(f"  æ·˜æ±°æ¬¡æ•°: {stats['evictions']}")

test_virtual_memory_manager()
```

## ğŸ“Š å†…å­˜æ•ˆç‡åˆ†æ

### å†…å­˜ä½¿ç”¨å¯¹æ¯”

```python
def memory_efficiency_comparison():
    """PagedAttention vs ä¼ ç»ŸAttentionçš„å†…å­˜æ•ˆç‡å¯¹æ¯”"""

    print("å†…å­˜æ•ˆç‡å¯¹æ¯”åˆ†æ")
    print("=" * 60)

    def traditional_attention_memory(seq_len, d_model, batch_size=1):
        """ä¼ ç»ŸAttentionå†…å­˜ä½¿ç”¨"""
        # KVç¼“å­˜: 2 * seq_len * d_model * batch_size * 4 bytes
        kv_cache = 2 * seq_len * d_model * 4
        # æ³¨æ„åŠ›çŸ©é˜µ: seq_len * seq_len * batch_size * 4 bytes
        attention_matrix = seq_len * seq_len * 4
        return kv_cache + attention_matrix

    def paged_attention_memory(seq_len, d_model, block_size, batch_size=1, cache_size=32):
        """PagedAttentionå†…å­˜ä½¿ç”¨"""
        # å½“å‰é¡µçš„KVç¼“å­˜
        current_kv = 2 * block_size * d_model * 4
        # æ€»KVç¼“å­˜ï¼ˆæ‰€æœ‰ç¼“å­˜é¡µï¼‰
        max_kv = cache_size * 2 * block_size * d_model * 4
        # è¾“å‡ºç¼“å­˜
        output_cache = seq_len * d_model * 4
        # é¡µè¡¨å’Œæ˜ å°„
        table_overhead = 1024 * 4  # å‡è®¾çš„é¡µè¡¨å¼€é”€
        return current_kv + max_kv + output_cache + table_overhead

    # æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦
    seq_lengths = [512, 1024, 2048, 4096, 8192, 16384, 32768]
    d_model = 4096
    block_size = 64
    cache_size = 32

    print(f"æ¨¡å‹é…ç½®: d_model={d_model}, å—å¤§å°={block_size}, ç¼“å­˜å¤§å°={cache_size}")
    print(f"æ‰¹æ¬¡å¤§å°: 1")
    print(f"{'åºåˆ—é•¿åº¦':<10} {'ä¼ ç»Ÿ(MB)':<12} {'åˆ†é¡µ(MB)':<12} {'èŠ‚çœæ¯”ä¾‹':<10} {'å®é™…èŠ‚çœ':<12}")
    print("-" * 60)

    total_traditional = 0
    total_paged = 0

    for seq_len in seq_lengths:
        traditional_mb = traditional_attention_memory(seq_len, d_model) / 1024 / 1024
        paged_mb = paged_attention_memory(seq_len, d_model, block_size, cache_size) / 1024 / 1024

        traditional_mb_mb = traditional_mb
        paged_mb_mb = paged_mb
        savings = (traditional_mb_mb - paged_mb_mb) / traditional_mb_mb * 100

        print(f"{seq_len:<10} {traditional_mb_mb:<12.1f} {paged_mb_mb:<12.1f} {savings:<10.1f}% "
              f"{traditional_mb_mb - paged_mb_mb:<12.1f}")

        total_traditional += traditional_mb_mb
        total_paged += paged_mb_mb

    print(f"\næ€»ä½“ç»Ÿè®¡:")
    print(f"  ä¼ ç»ŸAttentionæ€»å†…å­˜: {total_traditional:.1f} MB")
    print(f"  PagedAttentionæ€»å†…å­˜: {total_paged:.1f} MB")
    print(f"  æ€»èŠ‚çœæ¯”ä¾‹: {(total_traditional - total_paged)/total_traditional*100:.1f}%")

    print(f"\nå…³é”®å‘ç°:")
    print("1. PagedAttentionçš„å†…å­˜ä½¿ç”¨å¢é•¿é€Ÿåº¦è¿œä½äºä¼ ç»ŸAttention")
    print("2. åœ¨é•¿åºåˆ—ä¸‹ï¼Œå†…å­˜èŠ‚çœæ•ˆæœæ›´åŠ æ˜æ˜¾")
    print("3. ç¼“å­˜å¤§å°é™åˆ¶äº†æœ€å¤§èŠ‚çœå¹…åº¦")

    # å¯è§†åŒ–å†…å­˜å¢é•¿å¯¹æ¯”
    plt.figure(figsize=(12, 8))

    plt.subplot(2, 2, 1)
    traditional_memorys = [traditional_attention_memory(s, d_model) for s in seq_lengths]
    paged_memorys = [paged_attention_memory(s, d_model, block_size, cache_size) for s in seq_lengths]

    plt.plot(seq_lengths, traditional_memorys, 'r-o', label='ä¼ ç»ŸAttention')
    plt.plot(seq_lengths, paged_memorys, 'b-s', label='PagedAttention')
    plt.xlabel('åºåˆ—é•¿åº¦')
    plt.ylabel('å†…å­˜ä½¿ç”¨ (MB)')
    plt.title('å†…å­˜ä½¿ç”¨å¢é•¿å¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')

    plt.subplot(2, 2, 2)
    savings_ratio = [(t - p) / t * 100 for t, p in zip(traditional_memorys, paged_memorys)]
    plt.plot(seq_lengths, savings_ratio, 'g-^', label='èŠ‚çœæ¯”ä¾‹')
    plt.xlabel('åºåˆ—é•¿åº¦')
    plt.ylabel('å†…å­˜èŠ‚çœæ¯”ä¾‹ (%)')
    plt.title('å†…å­˜èŠ‚çœæ¯”ä¾‹')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def io_efficiency_analysis():
    """IOæ•ˆç‡åˆ†æ"""

    print("IOæ•ˆç‡åˆ†æ")
    print("=" * 50)

    print("ä¼ ç»ŸAttentionçš„IOæ¨¡å¼:")
    print("1. åŠ è½½æ‰€æœ‰KVåˆ°GPUå†…å­˜")
    print("2. è®¡ç®—å®Œæ•´æ³¨æ„åŠ›çŸ©é˜µ")
    print("3. æ‰§è¡ŒSoftmaxå’ŒåŠ æƒæ±‚å’Œ")
    print("4. å°†æ‰€æœ‰ç»“æœå†™å›GPUå†…å­˜")
    print("   â†’ å¤§é‡å†…å­˜IOæ“ä½œ")
    print()

    print("PagedAttentionçš„IOæ¨¡å¼:")
    print("1. æŒ‰éœ€åŠ è½½å½“å‰æŸ¥è¯¢ç›¸å…³çš„KVå—")
    print("2. åªè®¡ç®—å¿…è¦çš„ç›¸ä¼¼åº¦")
    print("3. ç«‹å³ä¸¢å¼ƒä¸éœ€è¦çš„æ•°æ®")
    print("   â†’ æœ€å°åŒ–IOæ“ä½œ")
    print()

    print("IOæ•ˆç‡å¯¹æ¯”:")
    print("-" * 30)
    print("ä¼ ç»ŸAttention:")
    print("  - å†…å­˜IO: 2Ã—NÂ²Ã—d (è¯»å–) + NÂ²Ã—d (å†™å…¥)")
    print("  - è®¡ç®—IO: O(NÂ²Ã—d)")
    print("  - æ€»IO: O(NÂ²Ã—d)")
    print()
    print("PagedAttention:")
    print("  - å†…å­˜IO: 2Ã—blockÃ—d Ã— page_count (æŒ‰éœ€)")
    print("  - è®¡ç®—IO: O(NÃ—blockÃ—d)")
    print("  - æ€»IO: O(NÃ—blockÃ—d) (é€šå¸¸æ¯”ä¼ ç»Ÿæ–¹æ³•å°)")
    print()

    print("å…³é”®ä¼˜åŠ¿:")
    print("âœ… å‡å°‘GPUå†…å­˜å¸¦å®½å ç”¨")
    print("âœ… æé«˜å†…å­˜åˆ©ç”¨ç‡")
    print("âœ… æ”¯æŒä»»æ„é•¿åºåˆ—")
    print("âœ… å®ç°çœŸæ­£çš„åŠ¨æ€æ‰¹å¤„ç†")

memory_efficiency_comparison()
```

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯

### æ¨ç†åœºæ™¯é€‚é…

```python
class PagedAttentionInference:
    """é€‚ç”¨äºæ¨ç†åœºæ™¯çš„PagedAttention"""

    def __init__(self, block_size: int, max_cache_size: int,
                 enable_prefix_caching=True):
        """
        æ¨ç†ä¼˜åŒ–ç‰ˆPagedAttention

        Args:
            block_size: å—å¤§å°
            max_cache_size: æœ€å¤§ç¼“å­˜é¡µæ•°
            enable_prefix_caching: æ˜¯å¦å¯ç”¨å‰ç¼€ç¼“å­˜
        """
        self.block_size = block_size
        self.max_cache_size = max_cache_size
        self.enable_prefix_caching = enable_prefix_caching

        # é¡µé¢ç®¡ç†
        self.pages = {}
        self.lru_order = []  # LRUç¼“å­˜åˆ—è¡¨
        self.free_pages = set()

        # å‰ç¼€ç¼“å­˜ï¼ˆç”¨äºé¢„è®¡ç®—ï¼‰
        self.prefix_cache = {}
        self.enable_prefix_caching = enable_prefix_caching

        # æ€§èƒ½ç»Ÿè®¡
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'total_requests': 0
        }

    def add_sequence(self, seq_id: int, k_cache: torch.Tensor, v_cache: torch.Tensor):
        """æ·»åŠ æ–°åºåˆ—åˆ°ç¼“å­˜"""
        seq_len = k_cache.shape[0]
        num_pages = (seq_len + self.block_size - 1) // self.block_size

        print(f"æ·»åŠ åºåˆ— {seq_id} (é•¿åº¦={seq_len}, é¡µæ•°={num_pages})")

        for page_idx in range(num_pages):
            page_id = f"{seq_id}_{page_idx}"

            # åˆ›å»ºé¡µé¢
            start_idx = page_idx * self.block_size
            end_idx = min(start_idx + self.block_size, seq_len)

            page = {
                'seq_id': seq_id,
                'page_id': page_id,
                'start_idx': start_idx,
                'end_idx': end_idx,
                'k_cache': k_cache[start_idx:end_idx],
                'v_cache': v_cache[start_idx:end_idx],
                'access_count': 0
            }

            # æ·»åŠ åˆ°ç¼“å­˜
            if len(self.pages) >= self.max_cache_size:
                self._evict_page()

            self.pages[page_id] = page
            self.lru_order.append(page_id)

        # æ¸…ç†ç©ºé—²é¡µé¢
        self.free_pages.clear()

    def _evict_page(self):
        """æ·˜æ±°æœ€ä¹…æœªä½¿ç”¨çš„é¡µé¢"""
        if not self.lru_order:
            return

        # æ‰¾åˆ°æœ€ä¹…æœªä½¿ç”¨çš„é¡µé¢
        lru_page_id = self.lru_order.pop(0)

        # æ£€æŸ¥æ˜¯å¦è¢«å¼•ç”¨
        page = self.pages[lru_page_id]

        print(f"æ·˜æ±°é¡µé¢ {lru_page_id} (åºåˆ— {page['seq_id']}, "
              f"è®¿é—®æ¬¡æ•°: {page['access_count']})")

        # ä»ç¼“å­˜ä¸­ç§»é™¤
        del self.pages[lru_page_id]
        self.free_pages.add(lru_page_id)

    def get_attention(self, query_seq_id: int, query_positions: List[int],
                     key_cache: torch.Tensor, value_cache: torch.Tensor):
        """è·å–æŒ‡å®šæŸ¥è¯¢ä½ç½®çš„æ³¨æ„åŠ›ç»“æœ"""

        print(f"è·å–åºåˆ— {query_seq_id} çš„æ³¨æ„åŠ› (ä½ç½®: {query_positions})")

        batch_size, seq_len, d_model = query_seq_id.shape
        output = torch.zeros(batch_size, len(query_positions), d_model)

        for i, q_pos in enumerate(query_positions):
            # æŸ¥æ‰¾å¯¹åº”çš„åºåˆ—
            seq_id = query_seq_id
            if seq_id not in self.pages:
                # åºåˆ—ä¸åœ¨ç¼“å­˜ä¸­ï¼Œéœ€è¦æ·»åŠ 
                print(f"åºåˆ— {seq_id} ä¸åœ¨ç¼“å­˜ä¸­ï¼Œè·³è¿‡")
                continue

            # æŸ¥æ‰¾å¯¹åº”çš„é¡µé¢
            page_id = f"{seq_id}_{q_pos // self.block_size}"
            if page_id not in self.pages:
                continue

            page = self.pages[page_id]
            local_pos = q_pos % self.block_size

            # æ£€æŸ¥ä½ç½®æ˜¯å¦åœ¨é¡µé¢èŒƒå›´å†…
            if local_pos < 0 or local_pos >= len(page['k_cache']):
                continue

            # è·å–KVå‘é‡
            k_vec = page['k_cache'][local_pos:local_pos+1].unsqueeze(0)
            v_vec = page['v_cache'][local_pos:local_pos+1].unsqueeze(0)

            # è®¡ç®—æ³¨æ„åŠ›
            scores = torch.matmul(query[:, i:i+1, :], k_vec)
            weights = F.softmax(scores, dim=-1)
            output[:, i:i+1, :] = torch.matmul(weights, v_vec)

            # æ›´æ–°è®¿é—®è®¡æ•°
            page['access_count'] += 1

            # ç§»åˆ°LRUåˆ—è¡¨æœ«å°¾
            if page_id in self.lru_order:
                self.lru_order.remove(page_id)
                self.lru_order.append(page_id)

        return output

class StreamingAttention:
    """æµå¼Attentionï¼šå¤„ç†è¶…é•¿åºåˆ—"""

    def __init__(self, block_size: int, window_size: int = None):
        """
        æµå¼Attention

        Args:
            block_size: å—å¤§å°
            window_size: æ»‘åŠ¨çª—å£å¤§å°
        """
        self.block_size = block_size
        self.window_size = window_size or block_size * 2

        # æ»‘åŠ¨çª—å£ç¼“å†²åŒº
        self.window_buffer = collections.deque(maxlen=window_size)

        # å½“å‰å¤„ç†çš„ä½ç½®
        self.current_position = 0

    def process_sequence(self, key_sequence, value_sequence):
        """æµå¼å¤„ç†åºåˆ—"""
        seq_len = len(key_sequence)
        print(f"æµå¼å¤„ç†åºåˆ— (é•¿åº¦: {seq_len})")

        results = []

        for pos in range(0, seq_len, self.block_size):
            if pos + self.block_size <= seq_len:
                # æ·»åŠ åˆ°çª—å£ç¼“å†²åŒº
                self.window_buffer.append({
                    'position': pos,
                    'key': key_sequence[pos:pos+self.block_size],
                    'value': value_sequence[pos:pos+self.block_size]
                })

                # ç§»åŠ¨çª—å£
                if len(self.window_buffer) > self.window_size:
                    self.window_buffer.popleft()

                print(f"  å¤„ç†ä½ç½® {pos}-{pos+self.block_size-1}")

            # å¤„ç†çª—å£ç¼“å†²åŒºä¸­çš„ä½ç½®
            window_outputs = []
            for window_item in self.window_buffer:
                window_pos = window_item['position']

                # è®¡ç®—è¯¥ä½ç½®çš„æ³¨æ„åŠ›
                k_block = window_item['key']  # [block_size, d_model]
                v_block = window_item['value']  # [block_size, d_model]

                # ä¸å½“å‰æŸ¥è¯¢ä½ç½®çš„æ³¨æ„åŠ›è®¡ç®—
                if self.current_position < seq_len:
                    q_vec = key_sequence[self.current_position:self.current_position+1]
                    scores = torch.matmul(q_vec, k_block.transpose(-2, -1))
                    weights = F.softmax(scores, dim=-1)
                    output = torch.matmul(weights, v_block)
                    window_outputs.append(output)

            # å¤„ç†çª—å£è¾“å‡º
            if window_outputs:
                # è¿™é‡Œå¯ä»¥æœ‰æ›´å¤æ‚çš„åå¤„ç†
                aggregated_output = torch.mean(torch.stack(window_outputs), dim=0)
                results.append(aggregated_output)

            self.current_position += self.block_size

        return torch.stack(results) if results else torch.empty(0)

# æµ‹è¯•å®é™…åº”ç”¨
def test_practical_applications():
    """æµ‹è¯•å®é™…åº”ç”¨åœºæ™¯"""

    print("å®é™…åº”ç”¨åœºæ™¯æµ‹è¯•")
    print("=" * 60)

    print("\nåœºæ™¯1: æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    print("-" * 30)
    doc_len = 10000
    d_model = 768
    chunk_size = 512

    doc_paged = PagedAttentionInference(
        block_size=32, max_cache_size=64
    )

    print(f"æ–‡æ¡£é•¿åº¦: {doc_len}")
    print(f"ä½¿ç”¨PagedAttentionå¤„ç†é•¿æ–‡æ¡£")

    # æ¨¡æ‹Ÿæ–‡æ¡£åˆ†å—
    num_chunks = (doc_len + chunk_size - 1) // chunk_size
    for i in range(num_chunks):
        chunk_start = i * chunk_size
        chunk_end = min(chunk_start + chunk_size, doc_len)
        print(f"  å¤„ç†æ–‡æ¡£å— {i} (ä½ç½® {chunk_start}-{chunk_end-1})")

    print(f"âœ… å¯ä»¥å¤„ç†ä»»æ„é•¿åº¦çš„æ–‡æ¡£")

    print("\nåœºæ™¯2: å®æ—¶æµå¼æ¨ç†")
    print("-" * 30)
    streaming_attn = StreamingAttention(block_size=128, window_size=512)

    # æ¨¡æ‹Ÿæ— é™é•¿çš„æ•°æ®æµ
    import itertools
    data_stream = itertools.count(1)

    print("å¤„ç†æ•°æ®æµ (å‰1000ä¸ªtoken):")
    for i, token_id in enumerate(data_stream):
        if i % 128 == 0:  # æ¯128ä¸ªtokenå¤„ç†ä¸€æ¬¡
            print(f"  å¤„ç†token {i}-{i+127}")

        if i >= 1000:  # æ¨¡æ‹Ÿæˆªæ–­
            break

    print(f"âœ… æ”¯æŒæ— é™é•¿åºåˆ—çš„æµå¼å¤„ç†")

    print("\nåœºæ™¯3: åŠ¨æ€æ‰¹å¤„ç†")
    print("-" - " * 30)

    # æ¨¡æ‹Ÿä¸åŒé•¿åº¦çš„è¯·æ±‚
    requests = [
        (128, 1),
        (256, 2),
        (512, 1),
        (1024, 1),
        (2048, 1)
    ]

    adaptive_paged = PagedAttentionInference(
        block_size=64, max_cache_size=32
    )

    for seq_len, batch_size in requests:
        print(f"  å¤„ç†æ‰¹æ¬¡å¤§å° {batch_size}, åºåˆ—é•¿åº¦ {seq_len}")
        # è¿™é‡Œå¯ä»¥æ ¹æ®æ‰¹æ¬¡å¤§å°åŠ¨æ€è°ƒæ•´ç¼“å­˜ç­–ç•¥

    print(f"âœ… æ”¯æŒä¸åŒå¤§å°çš„åŠ¨æ€æ‰¹å¤„ç†")

test_practical_applications()
```

## ğŸ¯ ä¼˜åŒ–ç­–ç•¥å’Œæœ€ä½³å®è·µ

### ç¼“å­˜ä¼˜åŒ–ç­–ç•¥

```python
class CacheOptimizedPagedAttention:
    """ç¼“å­˜ä¼˜åŒ–çš„PagedAttention"""

    def __init__(self, block_size: int, cache_size: int,
                 cache_strategy='lru',
                 prefetch_distance=2,
                 eviction_threshold=0.8):
        """
        ç¼“å­˜ä¼˜åŒ–PagedAttention

        Args:
            block_size: å—å¤§å°
            cache_size: ç¼“å­˜å¤§å°
            cache_strategy: ç¼“å­˜ç­–ç•¥ ('lru', 'fifo', 'lfu')
            prefetch_distance: é¢„å–è·ç¦»
            eviction_threshold: æ·˜æ±°é˜ˆå€¼
        """
        self.block_size = block_size
        self.cache_size = cache_size
        self.cache_strategy = cache_strategy
        self.prefetch_distance = prefetch_distance
        self.eviction_threshold = eviction_threshold

        # ç¼“å­˜ç®¡ç†
        self.cache = {}
        self.cache_order = []
        self.cache_scores = {}  # ç”¨äºLFUç­–ç•¥

        # é¢„å–ç¼“å†²åŒº
        self.prefetch_buffer = collections.deque(maxlen=prefetch_distance)
        self.prefetch_queue = []

        # æ€§èƒ½ç›‘æ§
        self.stats = {
            'cache_hits': 0,
            'cache_misses': 0,
            'prefetch_hits': 0,
            'evictions': 0,
            'adaptive_reconfigurations': 0
        }

    def get_cache_strategy_config(self):
        """è·å–ç¼“å­˜ç­–ç•¥é…ç½®"""
        strategies = {
            'lru': {
                'description': 'æœ€è¿‘æœ€å°‘ä½¿ç”¨',
                'advantages': ['ç®€å•å®ç°', 'å±€éƒ¨æ€§å¥½'],
                'disadvantages': ['ç¼“å­˜æ±¡æŸ“', 'å…¨å±€æ€§å·®']
            },
            'lfu': {
                'description': 'æœ€å°‘ç»å¸¸ä½¿ç”¨',
                'advantages': ['å‘½ä¸­ç‡æ›´é«˜', 'é€‚åº”è®¿é—®æ¨¡å¼'],
                'disadvantages': ['å®ç°å¤æ‚', 'éœ€è¦è®­ç»ƒ']
            },
            'fifo': {
                'description': 'å…ˆè¿›å…ˆå‡º',
                'advantages': ['å®ç°ç®€å•', 'æ— çŠ¶æ€'],
                'disadvantages': ['é€‚åº”æ€§å·®', 'å¯èƒ½ç¼“å­˜å¤±æ•ˆ']
            }
        }
        return strategies.get(self.cache_strategy, {})

    def _update_cache_score(self, page_id: str, access_pattern: List[str]):
        """æ›´æ–°ç¼“å­˜åˆ†æ•°ï¼ˆç”¨äºLFUç­–ç•¥ï¼‰"""
        if self.cache_strategy != 'lfu':
            return

        if page_id not in self.cache_scores:
            self.cache_scores[page_id] = 0.0

        # ç®€å•çš„é¢‘ç‡åŸºç¡€æ›´æ–°
        new_score = self.cache_scores[page_id] + 1.0
        self.cache_scores[page_id] = new_score

        # å¯ä»¥è€ƒè™‘æ›´å¤æ‚çš„æ›´æ–°ç­–ç•¥
        # ä¾‹å¦‚ï¼šåŸºäºè®¿é—®æ¨¡å¼ã€æ—¶é—´è¡°å‡ç­‰

    def _select_eviction_candidate(self):
        """é€‰æ‹©æ·˜æ±°å€™é€‰"""
        if self.cache_strategy == 'lru':
            # LRU: é€‰æ‹©æœ€ä¹…æœªä½¿ç”¨çš„é¡µé¢
            return self.cache_order[0] if self.cache_order else None

        elif self.cache_strategy == 'lfu':
            # LFU: é€‰æ‹©åˆ†æ•°æœ€ä½çš„é¡µé¢
            min_score_page = min(self.cache_scores.items(), key=lambda x: x[1])[0]
            return min_score_page[0]

        elif self.cache_strategy == 'fifo':
            # FIFO: é€‰æ‹©æœ€æ—©æ·»åŠ çš„é¡µé¢
            return self.cache_order[0] if self.cache_order else None

        return None

    def _evict_page_if_needed(self):
        """æ ¹æ®é˜ˆå€¼æ·˜æ±°é¡µé¢"""
        if len(self.cache) <= self.cache_size:
            return

        # è®¡ç®—ç¼“å­˜ä½¿ç”¨ç‡
        current_usage = len(self.cache) / self.cache_size

        if current_usage > self.eviction_threshold:
            candidate = self._select_eviction_candidate()
            if candidate:
                self._evict_page(candidate)

                self.stats['adaptive_reconfigurations'] += 1
                print(f"è‡ªé€‚åº”æ·˜æ±°é¡µé¢ {candidate} (ä½¿ç”¨ç‡: {current_usage:.2f})")

    def prefetch_pages(self, future_positions: List[int]):
        """é¢„å–æœªæ¥å¯èƒ½éœ€è¦çš„é¡µé¢"""
        for pos in future_positions:
            page_id = pos // self.block_size
            if page_id not in self.cache:
                self.prefetch_queue.append(page_id)

        # ä¿æŒé¢„å–é˜Ÿåˆ—å¤§å°
        while len(self.prefetch_queue) > self.prefetch_distance:
            self.prefetch_queue.popleft()

    def smart_page_allocation(self, page_id: str, access_pattern: List[str]):
        """æ™ºèƒ½é¡µé¢åˆ†é…ç­–ç•¥"""

        # æ ¹æ®è®¿é—®æ¨¡å¼é€‰æ‹©é¡µé¢å¤§å°
        if len(access_pattern) > 4:
            # å¦‚æœè®¿é—®æ¨¡å¼é•¿ï¼Œè€ƒè™‘å¢åŠ é¡µé¢å¤§å°
            # è¿™é‡Œå¯ä»¥å®ç°åŠ¨æ€é¡µé¢å¤§å°
            pass

        # æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆå¹¶ç›¸é‚»é¡µé¢
        adjacent_pages = []
        for i in range(page_id, page_id + 3):
            if str(i) in self.cache:
                adjacent_pages.append(str(i))

        if len(adjacent_pages) >= 2:
            print(f"å‘ç°ç›¸é‚»é¡µé¢ {adjacent_pages}ï¼Œè€ƒè™‘åˆå¹¶")
            # è¿™é‡Œå¯ä»¥å®ç°é¡µé¢åˆå¹¶é€»è¾‘
            pass

def test_cache_optimization():
    """æµ‹è¯•ç¼“å­˜ä¼˜åŒ–ç­–ç•¥"""

    print("ç¼“å­˜ä¼˜åŒ–ç­–ç•¥æµ‹è¯•")
    print("=" * 50)

    strategies = ['lru', 'lfu', 'fifo']

    for strategy in strategies:
        print(f"\næµ‹è¯•ç­–ç•¥: {strategy}")
        print("-" * 30)

        cache = CacheOptimizedPagedAttention(
            block_size=64,
            cache_size=16,
            cache_strategy=strategy
        )

        # æ¨¡æ‹Ÿè®¿é—®æ¨¡å¼
        access_patterns = [
            [1, 1, 1, 2, 2, 2, 3, 3, 4, 5, 1, 1],  # å±€éƒ¨æ€§è®¿é—®
            [1, 5, 9, 13, 17, 21, 1, 5, 9, 13],  # è·³è·ƒå¼è®¿é—®
            [5, 4, 3, 2, 1, 1, 2, 3, 4, 5, 5],  # åå‘è®¿é—®
            [1, 1, 2, 2, 3, 4, 4, 1, 1, 2, 3],  # å±€éƒ¨æ€§è®¿é—®
        ]

        for pattern_idx, pattern in enumerate(access_patterns):
            print(f"  è®¿é—®æ¨¡å¼ {pattern_idx}: {pattern}")

            # æ¨¡æ‹Ÿé¡µé¢è®¿é—®
            page_ids = list(set([p // 4 for p in pattern]))

            for page_id in page_ids:
                cache.allocate_virtual_page(page_id, 16, 64, torch.device('cpu'))
                # æ¨¡æ‹Ÿé¡µé¢è®¿é—®
                cache.get_cache_strategy_config()

        print(f"  æœ€ç»ˆç»Ÿè®¡: {cache.get_cache_statistics()}")

test_cache_optimization()
```

### å¹¶å‘å¤„ç†ä¼˜åŒ–

```python
class ConcurrentPagedAttention:
    """å¹¶å‘çš„PagedAttentionå®ç°"""

    def __init__(self, block_size: int, cache_size: int, num_threads: int = 4):
        """
        å¹¶å‘PagedAttention

        Args:
            block_size: å—å¤§å°
            cache_size: ç¼“å­˜å¤§å°
            num_threads: å¹¶å‘çº¿ç¨‹æ•°
        """
        self.block_size = block_size
        self.cache_size = cache_size
        self.num_threads = num_threads

        # çº¿ç¨‹å®‰å…¨çš„ç¼“å­˜ç®¡ç†
        self.cache = {}
        self.cache_lock = threading.Lock()
        self.access_queue = queue.Queue()

        # çº¿ç¨‹æ± 
        self.thread_pool = []
        for i in range(num_threads):
            thread = threading.Thread(target=self._worker_thread, daemon=True)
            thread.start()
            self.thread_pool.append(thread)

        # ç»Ÿè®¡ä¿¡æ¯
        self.completed_requests = 0
        self.failed_requests = 0

    def _worker_thread(self):
        """å·¥ä½œçº¿ç¨‹"""
        while True:
            try:
            request = self.access_queue.get(timeout=1.0)
            if request is None:
                continue

            request['result'] = self._process_request(request)
                self.access_queue.put(request)

                self.completed_requests += 1

            except Exception as e:
                request['error'] = str(e)
                self.failed_requests += 1
                self.access_queue.put(request)

    def _process_request(self, request):
        """å¤„ç†è¯·æ±‚"""
        with self.cache_lock:
            # å¤„ç†å®é™…çš„Attentionè®¡ç®—
            try:
                result = self._compute_attention(
                    request['query'], request['keys'],
                    request['values'], request['mask']
                )
                return result
            except Exception as e:
                    raise e

    def _compute_attention(self, query, keys, values, mask=None):
        """è®¡ç®—æ³¨æ„åŠ›ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        # å®é™…çš„Attentionè®¡ç®—é€»è¾‘
        scores = torch.matmul(query, keys.transpose(-2, -1))
        scores = scores / math.sqrt(query.size(-1))

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        weights = F.softmax(scores, dim=-1)
        output = torch.matmul(weights, values)

        return output

    def async_attention(self, query_positions, key_cache, value_cache, attention_mask=None):
        """å¼‚æ­¥Attention"""
        futures = []

        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        for i, pos in enumerate(query_positions):
            future = self._async_compute_attention(
                pos, key_cache, value_cache, attention_mask
            )
            futures.append(future)

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = []
        for future in futures:
            try:
                result = await future
                results.append(result)
            except Exception as e:
                print(f"æŸ¥è¯¢ä½ç½® {i} å¤„ç†å¤±è´¥: {e}")
                results.append(None)

        return results

    def _async_compute_attention(self, query_position, key_cache, value_cache, attention_mask):
        """å¼‚æ­¥è®¡ç®—å•ä¸ªä½ç½®"""
        # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„å¼‚æ­¥Attentionè®¡ç®—
        # ä¸ºæ¼”ç¤ºï¼Œä½¿ç”¨ç®€å•çš„åŒæ­¥è®¡ç®—
        return None

# æµ‹è¯•å¹¶å‘å¤„ç†
def test_concurrent_paged_attention():
    """æµ‹è¯•å¹¶å‘PagedAttention"""

    print("å¹¶å‘PagedAttentionæµ‹è¯•")
    print("=" * 50)

    concurrent_attn = ConcurrentPagedAttention(
        block_size=32, cache_size=16, num_threads=4
    )

    print(f"å¹¶å‘é…ç½®: å—å¤§å°={concurrent_attn.block_size}, "
          f"ç¼“å­˜å¤§å°={concurrent_attn.cache_size}, "
          "çº¿ç¨‹æ•°={concurrent_attn.num_threads}")

    # æ¨¡æ‹Ÿå¹¶å‘æŸ¥è¯¢
    num_queries = 20
    query_positions = [i * 4 for i in range(num_queries)]

    print(f"\nå¹¶å‘å¤„ç† {num_queries} ä¸ªæŸ¥è¯¢...")

    # æ¨¡æ‹Ÿæ·»åŠ KVç¼“å­˜
    for seq_id in range(4):
        k_cache = torch.randn(16, 64)
        v_cache = torch.randn(16, 64)
        concurrent_attn.add_sequence(seq_id, k_cache, v_cache)

    # æäº¤æŸ¥è¯¢è¯·æ±‚
    for pos in query_positions:
        # æ¨¡æ‹ŸæŸ¥è¯¢å‘é‡
        query = torch.randn(1, 64)

        # åˆ›å»ºè¯·æ±‚
        request = {
            'query': query,
            'keys': f"key_seq_0",  # ç®€åŒ–ç‰ˆ
            'values': f"value_seq_0",  # ç®€åŒ–ç‰ˆ
            'mask': None,
            'position': pos
        }

        concurrent_attn.access_queue.put(request)

    # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
        time.sleep(2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´

    print(f"ç»Ÿè®¡:")
    print(f"  å®Œæˆè¯·æ±‚: {concurrent_attn.completed_requests}")
    print(f" å¤±è´¥è¯·æ±‚: {concurrent_attn.failed_requests}")
    print(f"æˆåŠŸç‡: {concurrent_attn.completed_requests / (concurrent_attn.completed_requests + concurrent_attn.failed_requests) * 100:.1f}%")

test_concurrent_paged_attention()
```

## ğŸ¯ æ€»ç»“ä¸æœ€ä½³å®è·µ

### æ ¸å¿ƒä»·å€¼å›é¡¾

PagedAttentionçš„é©å‘½æ€§ä»·å€¼ä½“ç°åœ¨ï¼š

1. **çªç ´å†…å­˜é™åˆ¶**ï¼šä½¿é•¿åºåˆ—å¤„ç†æˆä¸ºå¯èƒ½
2. **IOæ•ˆç‡ä¼˜åŒ–**ï¼šæœ€å°åŒ–å†…å­˜IOæ“ä½œ
3. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒä»»æ„é•¿åº¦çš„åºåˆ—
4. **å®ç”¨æ€§**ï¼šåœ¨æ¨ç†åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚

### å®æ–½å»ºè®®

```python
def implementation_guidelines():
    """PagedAttentionå®æ–½æŒ‡å—"""

    print("PagedAttentionå®æ–½æŒ‡å—")
    print("=" * 50)

    print("ğŸ”§ æ ¸å¿ƒè®¾è®¡åŸåˆ™:")
    print("-" * 25)
    print("1. å—å¤§å°é€‰æ‹©: 16-128ä¸ªtoken (æ ¹æ®GPUå†…å­˜)")
    print("2. ç¼“å­˜å¤§å°: 16-64é¡µ (æ ¹æ®åº”ç”¨åœºæ™¯)")
    print("3. é¡µé¢æ›¿æ¢ç­–ç•¥: LRU (é»˜è®¤æ¨è) æˆ– LFU (éœ€è¦è®­ç»ƒ)")
    print("4. æŒ‰éœ€åŠ è½½: åªåŠ è½½å½“å‰éœ€è¦çš„KVå—")
    print()

    print("\nğŸ¯ æ€§èƒ½ä¼˜åŒ–:")
    print("-" * 25)
    print("1. é¢„å–: é¢„æµ‹æœªæ¥æŸ¥è¯¢ä½ç½®")
    print("2. æ‰¹é‡å¤„ç†: ä¸€æ¬¡å¤„ç†å¤šä¸ªæŸ¥è¯¢")
    print("3. å†…å­˜å¯¹é½: ç¡®ä¿å†…å­˜è®¿é—®æ•ˆç‡")
    print("4. å¼‚æ­¥å¤„ç†: å¹¶å‘å¤„ç†ç‹¬ç«‹æŸ¥è¯¢")

    print("\nğŸ”§ é”™è¯¯å¤„ç†:")
    print("-" * 25)
    print("1. é¡µé¢æœªåˆ†é…: ä¼˜é›…é™çº§åˆ°ä¼ ç»ŸAttention")
    print("2. å†…å­˜ä¸è¶³: åŠ¨æ€è°ƒæ•´ç¼“å­˜ç­–ç•¥")
    print("3. æ•°æ®æŸå: æä¾›æ¢å¤æœºåˆ¶")
    print("4. ç½‘ç»œé—®é¢˜: æä¾›é‡è¯•æœºåˆ¶")

    print("\nğŸ”§ ç›‘æ§æŒ‡æ ‡:")
    print("-" * 25)
    print("1. ç¼“å­˜å‘½ä¸­ç‡: ç›®æ ‡ > 80%")
    print("2. é¡µé¢é”™è¯¯ç‡: ç›®æ ‡ < 1%")
    print("3. å†…å­˜ä½¿ç”¨ç‡: ç›®æ ‡ > 90%")
    print("4. å»¶è¿Ÿ: ç›®æ ‡ < 50ms")

    print("\nğŸš€ æŠ€æœ¯é™·é˜±:")
    print("-" * 25)
    print("1. é¿å…é¢‘ç¹çš„é¡µé¢åˆ†é…å’Œé‡Šæ”¾")
    print("2. åˆç†è®¾ç½®ç¼“å­˜å¤§å°é¿å…å†…å­˜æµªè´¹")
    print("3. æ³¨æ„å—è¾¹ç•Œçš„å¤„ç†")
    print("4. è€ƒè™‘åºåˆ—é•¿åº¦ä¸æ˜¯å—å¤§å°çš„å€æ•°")

    print("\nâœ… æœ€ä½³å®è·µæ€»ç»“:")
    print("-" * 30)
    print("1. æ ¹æ®ç¡¬ä»¶é…ç½®é€‰æ‹©åˆé€‚çš„å‚æ•°")
    print("2. åœ¨æ¨ç†å‰è¿›è¡Œé¢„çƒ­å¡«å……ç¼“å­˜")
    print("3. ç›‘æ§æ€§èƒ½æŒ‡æ ‡å¹¶åŠ¨æ€è°ƒæ•´")
    print("4. å®ç°å¥åº·æ£€æŸ¥å’Œé”™è¯¯æ¢å¤")

implementation_guidelines()

def deployment_recommendations():
    """éƒ¨ç½²å»ºè®®"""

    print("\néƒ¨ç½²å»ºè®®:")
    print("=" * 50)

    print("ğŸ–¥ï¸ ä¸åŒåœºæ™¯çš„æ¨èé…ç½®:")
    print("-" * 30)

    configs = [
        {
            "æ–‡æ¡£é—®ç­”ç³»ç»Ÿ": {
                "block_size": 32,
                "cache_size": 64,
                "cache_strategy": "lru",
                "max_seq_len": 10000
            },
            {
                "block_size": 16,
                "cache_size": 32,
                "cache_strategy": "lfu",
                "max_seq_len": 5000
            },
            "ä»£ç è¡¥å…¨": {
                "block_size": 64,
                "cache_size": 128,
                "cache_strategy": "lru",
                "max_seq_len": 2000
            },
            {
                "å®æ—¶ç”Ÿæˆ": {
                    "block_size": 128,
                    "cache_size": 256,
                    "cache_strategy": "fifo",
                    "max_seq_len": 4096
                }
            }
        ]

    for scenario, config in configs.items():
        print(f"\n{scenario}:")
        print(f"  å—å¤§å°: {config['block_size']}")
        print(f"  ç¼“å­˜å¤§å°: {config['cache_size']}")
        print(f"  ç¼“å­˜ç­–ç•¥: {config['cache_strategy']}")
        print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {config['max_seq_len']}")

    print(f"\nğŸš€ ç¡¬ä»¶å…¼å®¹æ€§:")
    print("-" * 30)
    print("âœ… PyTorch: åŸç”Ÿæ”¯æŒ")
    print("âœ… JAX: éœ€è¦é€‚é…")
    print("âœ… TensorFlow: éœ€è¦å®ç°")

    print(f"\nğŸš€ ç¡¬ä»¶å»ºè®®:")
    print("-" * 30)
    print("1. ä½¿ç”¨ç°æœ‰çš„PagedAttentionå®ç°:")
    print("   - vLLM: https://github.com/vllm-project/transformer")
    print("   - FlashAttention: é›†æˆåœ¨FlashAttentionä¸­")
    print("   - xformers: å¯ç”¨ä½†éœ€è¦é€‚é…")
    print()
    print("2. è‡ªå®šä¹‰å®ç°æ—¶:")
    print("   - ä»åŸºç¡€ç‰ˆæœ¬å¼€å§‹ï¼Œé€æ­¥ä¼˜åŒ–")
    print("   - å……åˆ†æµ‹è¯•å†…å­˜å’Œæ•°å€¼ç²¾åº¦")
    print("   - æ·»åŠ é«˜çº§ä¼˜åŒ–ï¼ˆé¢„å–ã€æ™ºèƒ½ç¼“å­˜ç­‰ï¼‰")
    print()
    print("3. æ€§èƒ½éªŒè¯:")
    print("   - å¯¹æ¯”ä¸ä¼ ç»ŸAttentionçš„ç²¾åº¦")
    print("   - ç›‘æ§å†…å­˜ä½¿ç”¨å’Œè®¿é—®æ¨¡å¼")
    print("   - è¿›è¡Œå‹åŠ›æµ‹è¯•å’Œç¨³å®šæ€§éªŒè¯")

deployment_recommendations()
```

---

**è®°ä½**ï¼šPagedAttentionæ˜¯é•¿åºåˆ—å¤„ç†çš„å…³é”®æŠ€æœ¯ï¼Œå®ƒå·§å¦™åœ°è§£å†³äº†å†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚è™½ç„¶å®ç°ç›¸å¯¹å¤æ‚ï¼Œä½†å…¶å¸¦æ¥çš„æ”¶ç›Šæ˜¯å·¨å¤§çš„â€”â€”è®©é•¿åºåˆ—æ¨ç†å˜å¾—å¯è¡Œã€‚ç†è§£å…¶åŸç†å’Œå®ç°ï¼Œå¯¹äºæ„å»ºé«˜æ•ˆçš„AIç³»ç»Ÿè‡³å…³é‡è¦ã€‚

*ä¸‹ä¸€ç¯‡æ–‡ç« å°†æ·±å…¥è§£æKVç¼“å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œæ¢ç´¢å¦‚ä½•è¿›ä¸€æ­¥ä¼˜åŒ–å­˜å‚¨å’Œè®¿é—®æ•ˆç‡ã€‚* ğŸš€