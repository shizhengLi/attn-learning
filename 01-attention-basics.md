# Attentionæœºåˆ¶åŸºç¡€ï¼šä»ç›´è§‰åˆ°æ•°å­¦çš„å®Œæ•´è§£æ

## ğŸ¯ å¼•è¨€

æƒ³è±¡ä¸€ä¸‹ä½ æ­£åœ¨é˜…è¯»ä¸€æœ¬å¤æ‚çš„ä¹¦ç±ï¼Œå½“é‡åˆ°å…³é”®æ¦‚å¿µæ—¶ï¼Œä½ ä¼šä¸è‡ªè§‰åœ°å°†æ³¨æ„åŠ›é›†ä¸­åœ¨æŸäº›æ®µè½ï¼ŒåŒæ—¶å¿½ç•¥å…¶ä»–å†…å®¹ã€‚è¿™ç§äººç±»å¤©ç”Ÿçš„"æ³¨æ„åŠ›"æœºåˆ¶ï¼Œæ­£æ˜¯æ·±åº¦å­¦ä¹ ä¸­Attentionæœºåˆ¶çš„æ ¸å¿ƒçµæ„Ÿã€‚

Attentionæœºåˆ¶è‡ª2014å¹´æå‡ºä»¥æ¥ï¼Œå·²ç»æˆä¸ºäº†æ·±åº¦å­¦ä¹ é¢†åŸŸæœ€é‡è¦çš„æŠ€æœ¯åˆ›æ–°ä¹‹ä¸€ã€‚ä»æœ€åˆçš„æœºå™¨ç¿»è¯‘åˆ°ä»Šå¤©çš„å¤§è¯­è¨€æ¨¡å‹ï¼ŒAttentionæ— å¤„ä¸åœ¨ã€‚ç„¶è€Œï¼Œå¾ˆå¤šäººå¯¹Attentionçš„ç†è§£åœç•™åœ¨"é‡è¦æ€§çš„åŠ æƒ"è¿™ä¸ªæ¨¡ç³Šæ¦‚å¿µä¸Šã€‚

æœ¬æ–‡å°†ä»¥"æµ…è€…è§‰å…¶æµ…ï¼Œæ·±è€…è§‰å…¶æ·±"çš„æ–¹å¼ï¼Œä»æœ€ç›´è§‚çš„ç±»æ¯”å¼€å§‹ï¼Œé€æ­¥æ·±å…¥åˆ°æ•°å­¦å…¬å¼å’Œä»£ç å®ç°ï¼Œè®©ä½ çœŸæ­£ç†è§£Attentionæœºåˆ¶çš„æœ¬è´¨ã€‚

## ğŸ§  ä»äººç±»æ³¨æ„åŠ›åˆ°æœºå™¨æ³¨æ„åŠ›

### äººç±»æ³¨æ„åŠ›çš„å¯å‘

è®©æˆ‘ä»¬å…ˆä»äººç±»çš„æ³¨æ„åŠ›æœºåˆ¶è¯´èµ·ã€‚å½“ä½ è§‚å¯Ÿä¸€å¼ å›¾ç‰‡æ—¶ï¼š

```
æƒ³è±¡ä½ çœ‹åˆ°äº†ä¸€å¼ ç¹å¿™çš„è¡—é“ç…§ç‰‡ï¼š
- ä½ çš„çœ¼ç›å¯èƒ½ä¼šé¦–å…ˆæ³¨æ„åˆ°é‚£è¾†çº¢è‰²è·‘è½¦ï¼ˆå› ä¸ºå®ƒé¢œè‰²é²œè‰³ï¼‰
- ç„¶åæ³¨æ„åˆ°äººè¡Œé“ä¸Šçš„è¡Œäººï¼ˆå› ä¸ºä»–ä»¬åœ¨ç§»åŠ¨ï¼‰
- æœ€åå¯èƒ½æ³¨æ„åˆ°è¿œå¤„çš„å»ºç­‘ç‰©ï¼ˆå› ä¸ºå®ƒä»¬æ˜¯èƒŒæ™¯ï¼‰

åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œä½ çš„å¤§è„‘ï¼š
1. ä¸»åŠ¨é€‰æ‹©å…³æ³¨æŸäº›å¯¹è±¡
2. ç»™ä¸åŒå¯¹è±¡åˆ†é…ä¸åŒçš„"æ³¨æ„åŠ›æƒé‡"
3. æ ¹æ®æ³¨æ„åŠ›æƒé‡å¤„ç†ä¿¡æ¯
```

è¿™å°±æ˜¯äººç±»æ³¨æ„åŠ›çš„ä¸‰ä¸ªæ ¸å¿ƒç‰¹å¾ï¼š**é€‰æ‹©æ€§**ã€**æƒé‡åˆ†é…**ã€**ä¿¡æ¯å¤„ç†**ã€‚

### æœºå™¨æ³¨æ„åŠ›çš„æ ¸å¿ƒæ€æƒ³

æœºå™¨å­¦ä¹ ä¸­çš„Attentionæœºåˆ¶æ­£æ˜¯æ¨¡ä»¿äº†è¿™ç§æ€æƒ³ï¼š

```python
# äººç±»æ³¨æ„åŠ›ï¼šå…³æ³¨å›¾ç‰‡ä¸­çš„ä¸åŒå¯¹è±¡
human_attention = {
    "çº¢è‰²è·‘è½¦": 0.6,    # æœ€é‡è¦
    "è¡Œäºº": 0.3,        # æ¬¡é‡è¦
    "å»ºç­‘ç‰©": 0.1       # æœ€ä¸é‡è¦
}

# æœºå™¨æ³¨æ„åŠ›ï¼šå…³æ³¨åºåˆ—ä¸­çš„ä¸åŒä½ç½®
machine_attention = {
    "è¯1": 0.1,         # ä¸é‡è¦
    "è¯2": 0.7,         # é‡è¦
    "è¯3": 0.2          # ä¸€èˆ¬é‡è¦
}
```

å…³é”®åŒºåˆ«åœ¨äºï¼š
- **äººç±»æ³¨æ„åŠ›**ï¼šåŸºäºæ„Ÿå®˜å’Œç»éªŒï¼Œæ˜¯æ¨¡ç³Šå’Œç›´è§‰çš„
- **æœºå™¨æ³¨æ„åŠ›**ï¼šåŸºäºæ•°å­¦è®¡ç®—ï¼Œæ˜¯ç²¾ç¡®å’Œå¯å­¦ä¹ çš„

## ğŸ” Attentionæœºåˆ¶çš„æ•°å­¦æœ¬è´¨

### ä»ç®€å•ä¾‹å­å¼€å§‹

è®©æˆ‘ä»¬ä»ä¸€ä¸ªéå¸¸ç®€å•çš„ä¾‹å­å¼€å§‹ï¼Œç†è§£Attentionçš„æ•°å­¦æœ¬è´¨ã€‚

å‡è®¾æˆ‘ä»¬è¦ç¿»è¯‘ä¸€ä¸ªå¥å­ï¼š**"The cat sat on the mat"** â†’ **"çŒ«ååœ¨å«å­ä¸Š"**

å½“ç¿»è¯‘"çŒ«"è¿™ä¸ªè¯æ—¶ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“å®ƒå¯¹åº”è‹±æ–‡ä¸­çš„å“ªä¸ªè¯ã€‚ç›´è§‰å‘Šè¯‰æˆ‘ä»¬ï¼Œ"cat"æœ€é‡è¦ï¼Œ"the"æ¬¡ä¹‹ï¼Œå…¶ä»–è¯ä¸å¤ªç›¸å…³ã€‚

```
è‹±æ–‡å¥å­ï¼š["The", "cat", "sat", "on", "the", "mat"]
ç¿»è¯‘"çŒ«"æ—¶çš„æ³¨æ„åŠ›ï¼š
- "The": 0.2   (æœ‰äº›å…³ç³»ï¼Œä½†ä¸å¼º)
- "cat": 0.7   (å¼ºç›¸å…³ï¼)
- "sat": 0.05  (å…³ç³»å¾ˆå¼±)
- "on": 0.02   (å‡ ä¹æ— å…³)
- "the": 0.02   (å‡ ä¹æ— å…³)
- "mat": 0.01   (å‡ ä¹æ— å…³)
```

è¿™ä¸ª0.7çš„æƒé‡å°±æ˜¯"æ³¨æ„åŠ›"ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬ç¿»è¯‘"çŒ«"æ—¶åº”è¯¥é‡ç‚¹å…³æ³¨"cat"ã€‚

### Queryã€Keyã€Valueçš„è¯ç”Ÿ

ç°åœ¨çš„é—®é¢˜æ˜¯ï¼šæœºå™¨å¦‚ä½•è®¡ç®—å‡ºè¿™äº›æ³¨æ„åŠ›æƒé‡å‘¢ï¼Ÿè¿™å°±æ˜¯Queryã€Keyã€Valueå‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚

è®©æˆ‘ä»¬ç»§ç»­ç”¨ç¿»è¯‘çš„ä¾‹å­ï¼š

**Queryï¼ˆæŸ¥è¯¢ï¼‰**ï¼šæˆ‘è¦ç¿»è¯‘"çŒ«"ï¼Œæˆ‘éœ€è¦æ‰¾åˆ°è‹±æ–‡ä¸­å¯¹åº”çš„è¯
**Keyï¼ˆé”®ï¼‰**ï¼šæ¯ä¸ªè‹±æ–‡è¯éƒ½æœ‰ä¸€ä¸ª"èº«ä»½æ ‡è¯†"
**Valueï¼ˆå€¼ï¼‰**ï¼šæ¯ä¸ªè‹±æ–‡è¯çš„å®é™…å«ä¹‰

```python
# ç®€åŒ–çš„QKVæ¦‚å¿µ
query = "çŒ«çš„ç¿»è¯‘ç›®æ ‡"
keys = ["Theçš„èº«ä»½", "catçš„èº«ä»½", "satçš„èº«ä»½", ...]
values = ["Theçš„å«ä¹‰", "catçš„å«ä¹‰", "satçš„å«ä¹‰", ...]

# è®¡ç®—æ³¨æ„åŠ›ï¼šQueryä¸æ¯ä¸ªKeyçš„ç›¸ä¼¼åº¦
similarities = [
    similarity(query, "Theçš„èº«ä»½"),   # 0.2
    similarity(query, "catçš„èº«ä»½"),   # 0.7
    similarity(query, "satçš„èº«ä»½"),   # 0.05
    ...
]

# å½’ä¸€åŒ–ä¸ºæƒé‡
attention_weights = softmax(similarities)
```

### æ•°å­¦å…¬å¼çš„é€æ­¥æ¨å¯¼

ç°åœ¨è®©æˆ‘ä»¬æŠŠä¸Šé¢çš„ç›´è§‰è½¬åŒ–ä¸ºæ•°å­¦å…¬å¼ã€‚

#### ç¬¬1æ­¥ï¼šç›¸ä¼¼åº¦è®¡ç®—

æˆ‘ä»¬éœ€è¦è®¡ç®—Queryå’Œæ¯ä¸ªKeyçš„ç›¸ä¼¼åº¦ã€‚æœ€å¸¸ç”¨çš„æ–¹æ³•æ˜¯ç‚¹ç§¯ï¼š

```
ç›¸ä¼¼åº¦(Query, Key_i) = Query Â· Key_i
```

ä¸ºä»€ä¹ˆæ˜¯ç‚¹ç§¯ï¼Ÿå› ä¸ºï¼š
- å¦‚æœä¸¤ä¸ªå‘é‡æ–¹å‘ç›¸åŒï¼Œç‚¹ç§¯å¾ˆå¤§ï¼ˆç›¸ä¼¼åº¦é«˜ï¼‰
- å¦‚æœä¸¤ä¸ªå‘é‡å‚ç›´ï¼Œç‚¹ç§¯ä¸º0ï¼ˆç›¸ä¼¼åº¦ä½ï¼‰
- å¦‚æœä¸¤ä¸ªå‘é‡æ–¹å‘ç›¸åï¼Œç‚¹ç§¯ä¸ºè´Ÿï¼ˆç›¸ä¼¼åº¦ä½ï¼‰

#### ç¬¬2æ­¥ï¼šç¼©æ”¾ï¼ˆå¯é€‰ä½†é‡è¦ï¼‰

ç‚¹ç§¯å¯èƒ½ä¼šéšç€å‘é‡ç»´åº¦çš„å¢åŠ è€Œå˜å¾—å¾ˆå¤§ï¼Œå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚æ‰€ä»¥æˆ‘ä»¬è¦é™¤ä»¥ä¸€ä¸ªç¼©æ”¾å› å­ï¼š

```
ç¼©æ”¾åç›¸ä¼¼åº¦ = (Query Â· Key_i) / âˆšd
```

å…¶ä¸­dæ˜¯å‘é‡çš„ç»´åº¦ã€‚ä¸ºä»€ä¹ˆè¦é™¤ä»¥âˆšdï¼Ÿ

```python
# ç®€å•è§£é‡Šï¼š
# å‡è®¾å‘é‡ç»´åº¦d=100ï¼Œæ¯ä¸ªåˆ†é‡å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1
# é‚£ä¹ˆç‚¹ç§¯çš„æœŸæœ›æ˜¯0ï¼Œæ–¹å·®æ˜¯d=100
# é™¤ä»¥âˆšd=10åï¼Œæ–¹å·®å˜ä¸º1ï¼Œæ›´ç¨³å®š
```

#### ç¬¬3æ­¥ï¼šSoftmaxå½’ä¸€åŒ–

æˆ‘ä»¬éœ€è¦å°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºæƒé‡ï¼Œæƒé‡çš„å’Œåº”è¯¥ä¸º1ï¼š

```
æ³¨æ„åŠ›æƒé‡_i = exp(ç¼©æ”¾åç›¸ä¼¼åº¦_i) / Î£_j exp(ç¼©æ”¾åç›¸ä¼¼åº¦_j)
```

è¿™å°±æ˜¯Softmaxå‡½æ•°ï¼Œå®ƒç¡®ä¿ï¼š
- æ‰€æœ‰æƒé‡éƒ½åœ¨[0,1]ä¹‹é—´
- æƒé‡çš„å’Œä¸º1
- åŸå§‹ç›¸ä¼¼åº¦è¶Šå¤§ï¼Œæƒé‡ä¹Ÿè¶Šå¤§

#### ç¬¬4æ­¥ï¼šåŠ æƒæ±‚å’Œ

æœ€åï¼Œç”¨æ³¨æ„åŠ›æƒé‡å¯¹Valueè¿›è¡ŒåŠ æƒæ±‚å’Œï¼š

```
è¾“å‡º = Î£_i (æ³¨æ„åŠ›æƒé‡_i Ã— Value_i)
```

è¿™å°±æ˜¯Attentionæœºåˆ¶çš„å®Œæ•´æ•°å­¦è¡¨è¾¾å¼ï¼

### å®Œæ•´çš„æ•°å­¦å…¬å¼

å°†ä»¥ä¸Šæ­¥éª¤æ•´åˆï¼Œæˆ‘ä»¬å¾—åˆ°Scaled Dot-Product Attentionçš„å®Œæ•´å…¬å¼ï¼š

```
Attention(Q, K, V) = softmax(QK^T / âˆšd) V
```

å…¶ä¸­ï¼š
- Qï¼šQueryçŸ©é˜µ [sequence_length Ã— d]
- Kï¼šKeyçŸ©é˜µ [sequence_length Ã— d]
- Vï¼šValueçŸ©é˜µ [sequence_length Ã— d]
- QK^Tï¼šç›¸ä¼¼åº¦çŸ©é˜µ [sequence_length Ã— sequence_length]
- dï¼šå‘é‡çš„ç»´åº¦

## ğŸ’» ä»é›¶å¼€å§‹å®ç°Attention

ç†è®ºå·²ç»å¤Ÿå¤šäº†ï¼Œè®©æˆ‘ä»¬åŠ¨æ‰‹å®ç°ä¸€ä¸ªç®€å•çš„Attentionæœºåˆ¶ï¼

### åŸºç¡€å®ç°

```python
import numpy as np

def attention(query, keys, values):
    """
    åŸºç¡€Attentionæœºåˆ¶å®ç°

    Args:
        query: (d,) æŸ¥è¯¢å‘é‡
        keys: (n, d) é”®çŸ©é˜µ
        values: (n, d) å€¼çŸ©é˜µ

    Returns:
        output: (d,) åŠ æƒåçš„è¾“å‡º
        weights: (n,) æ³¨æ„åŠ›æƒé‡
    """
    d = query.shape[0]

    # ç¬¬1æ­¥ï¼šè®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç‚¹ç§¯ï¼‰
    similarities = np.dot(keys, query)  # (n,)

    # ç¬¬2æ­¥ï¼šç¼©æ”¾
    scaled_similarities = similarities / np.sqrt(d)

    # ç¬¬3æ­¥ï¼šSoftmaxå½’ä¸€åŒ–
    exp_similarities = np.exp(scaled_similarities)
    weights = exp_similarities / np.sum(exp_similarities)

    # ç¬¬4æ­¥ï¼šåŠ æƒæ±‚å’Œ
    output = np.dot(weights, values)

    return output, weights

# ç¤ºä¾‹ï¼šç¿»è¯‘åœºæ™¯
d = 64  # å‘é‡ç»´åº¦
n = 6   # åºåˆ—é•¿åº¦

# éšæœºåˆå§‹åŒ–QKVï¼ˆå®é™…ä¸­è¿™äº›æ˜¯å­¦ä¹ å¾—åˆ°çš„ï¼‰
query = np.random.randn(d)
keys = np.random.randn(n, d)
values = np.random.randn(n, d)

# è®¡ç®—Attention
output, weights = attention(query, keys, values)

print("æ³¨æ„åŠ›æƒé‡:", weights)
print("æƒé‡æ€»å’Œ:", np.sum(weights))  # åº”è¯¥æ¥è¿‘1
```

### æ‰¹é‡å¤„ç†å®ç°

å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦å¤„ç†å¤šä¸ªæŸ¥è¯¢ï¼š

```python
def batch_attention(queries, keys, values):
    """
    æ‰¹é‡Attentionå®ç°

    Args:
        queries: (batch_size, d) æŸ¥è¯¢çŸ©é˜µ
        keys: (seq_len, d) é”®çŸ©é˜µ
        values: (seq_len, d) å€¼çŸ©é˜µ

    Returns:
        outputs: (batch_size, d) è¾“å‡ºçŸ©é˜µ
        weights: (batch_size, seq_len) æ³¨æ„åŠ›æƒé‡çŸ©é˜µ
    """
    batch_size, d = queries.shape
    seq_len = keys.shape[0]

    outputs = []
    all_weights = []

    for i in range(batch_size):
        output, weights = attention(queries[i], keys, values)
        outputs.append(output)
        all_weights.append(weights)

    return np.array(outputs), np.array(all_weights)
```

### PyTorchå®ç°

```python
import torch
import torch.nn.functional as F

def pytorch_attention(query, keys, values):
    """
    PyTorchç‰ˆæœ¬çš„Attentionå®ç°
    """
    d = query.size(-1)

    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = torch.matmul(keys, query.unsqueeze(-1)).squeeze(-1)

    # ç¼©æ”¾
    scaled_similarities = similarities / torch.sqrt(torch.tensor(d, dtype=torch.float32))

    # Softmax
    weights = F.softmax(scaled_similarities, dim=-1)

    # åŠ æƒæ±‚å’Œ
    output = torch.matmul(weights.unsqueeze(0), values).squeeze(0)

    return output, weights

# ä½¿ç”¨ç¤ºä¾‹
query = torch.randn(64)
keys = torch.randn(6, 64)
values = torch.randn(6, 64)

output, weights = pytorch_attention(query, keys, values)
print("PyTorch Attentionå®Œæˆ!")
```

## ğŸ¨ å¯è§†åŒ–ç†è§£Attention

ä¸ºäº†æ›´ç›´è§‚åœ°ç†è§£Attentionï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¯è§†åŒ–ï¼š

```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(query, keys, values, token_names=None):
    """å¯è§†åŒ–Attentionæƒé‡"""
    # è®¡ç®—Attention
    d = query.shape[0]
    similarities = np.dot(keys, query)
    scaled_similarities = similarities / np.sqrt(d)
    weights = np.exp(scaled_similarities) / np.sum(np.exp(scaled_similarities))

    # å¯è§†åŒ–
    plt.figure(figsize=(10, 6))

    # æŸ±çŠ¶å›¾æ˜¾ç¤ºæƒé‡
    plt.subplot(1, 2, 1)
    if token_names is None:
        token_names = [f"Token_{i}" for i in range(len(weights))]

    bars = plt.bar(token_names, weights)
    plt.title('Attentionæƒé‡åˆ†å¸ƒ')
    plt.ylabel('æƒé‡')
    plt.xticks(rotation=45)

    # ä¸ºæ¯ä¸ªæŸ±å­æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, weight in zip(bars, weights):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'{weight:.3f}', ha='center', va='bottom')

    # çƒ­å›¾æ˜¾ç¤ºç›¸ä¼¼åº¦çŸ©é˜µ
    plt.subplot(1, 2, 2)
    similarity_matrix = np.outer(similarities, similarities)
    sns.heatmap(similarity_matrix, annot=True, cmap='YlOrRd',
                xticklabels=token_names, yticklabels=token_names)
    plt.title('ç›¸ä¼¼åº¦çŸ©é˜µ')

    plt.tight_layout()
    plt.show()

# ç¤ºä¾‹ï¼šç¿»è¯‘åœºæ™¯å¯è§†åŒ–
tokens = ["The", "cat", "sat", "on", "the", "mat"]
query = torch.randn(64)
keys = torch.randn(6, 64)
values = torch.randn(6, 64)

query_np = query.numpy()
keys_np = keys.numpy()
values_np = values.numpy()

visualize_attention(query_np, keys_np, values_np, tokens)
```

## ğŸ”„ å¤šå¤´Attentionï¼ˆMulti-Head Attentionï¼‰

å•ä¸ªAttentionæœºåˆ¶åªèƒ½å…³æ³¨ä¸€ç§"å…³ç³»"ï¼Œä½†ç°å®ä¸­ä¸€ä¸ªè¯å¯èƒ½ä¸å…¶ä»–è¯æœ‰å¤šç§ä¸åŒçš„å…³ç³»ã€‚è¿™å°±æ˜¯Multi-Head Attentionçš„åŠ¨æœºã€‚

### ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´ï¼Ÿ

```python
# ç¤ºä¾‹ï¼šå¥å­"ç‹—è¿½çŒ«"
# å¯¹äº"è¿½"è¿™ä¸ªè¯ï¼Œå®ƒå¯èƒ½æœ‰å¤šç§å…³ç³»ï¼š
# 1. "è¿½"çš„ä¸»ä½“å…³ç³» â†’ "ç‹—"
# 2. "è¿½"çš„å®¢ä½“å…³ç³» â†’ "çŒ«"
# 3. "è¿½"çš„è¯­æ³•å…³ç³» â†’ åŠ¨è¯

# å•å¤´Attentionåªèƒ½æ•æ‰ä¸€ç§å…³ç³»
# å¤šå¤´Attentionå¯ä»¥åŒæ—¶æ•æ‰å¤šç§å…³ç³»
```

### Multi-Head Attentionçš„åŸç†

Multi-Head Attentionå°†è¾“å…¥åˆ†æˆå¤šä¸ª"å¤´"ï¼Œæ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„å…³ç³»ï¼š

```
è¾“å…¥ç»´åº¦ï¼šd_model = 512
å¤´æ•°ï¼šnum_heads = 8
æ¯ä¸ªå¤´çš„ç»´åº¦ï¼šd_k = d_model / num_heads = 64

æµç¨‹ï¼š
1. å°†Qã€Kã€Våˆ†åˆ«çº¿æ€§å˜æ¢ä¸º8ä¸ªå¤´
2. å¯¹æ¯ä¸ªå¤´åˆ†åˆ«è®¡ç®—Attention
3. å°†8ä¸ªå¤´çš„è¾“å‡ºæ‹¼æ¥
4. çº¿æ€§å˜æ¢å¾—åˆ°æœ€ç»ˆè¾“å‡º
```

### ä»£ç å®ç°

```python
class MultiHeadAttention:
    def __init__(self, d_model, num_heads):
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads

        # åˆå§‹åŒ–æƒé‡çŸ©é˜µ
        self.W_q = np.random.randn(d_model, d_model)
        self.W_k = np.random.randn(d_model, d_model)
        self.W_v = np.random.randn(d_model, d_model)
        self.W_o = np.random.randn(d_model, d_model)

    def split_heads(self, x):
        """å°†è¾“å…¥åˆ†å‰²ä¸ºå¤šä¸ªå¤´"""
        # x: (batch_size, seq_len, d_model)
        batch_size, seq_len, _ = x.shape
        x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)
        return x.transpose(0, 2, 1, 3)  # (batch_size, num_heads, seq_len, d_k)

    def combine_heads(self, x):
        """å°†å¤šä¸ªå¤´åˆå¹¶"""
        # x: (batch_size, num_heads, seq_len, d_k)
        batch_size, _, seq_len, _ = x.shape
        x = x.transpose(0, 2, 1, 3)  # (batch_size, seq_len, num_heads, d_k)
        return x.reshape(batch_size, seq_len, self.d_model)

    def forward(self, Q, K, V):
        """å‰å‘ä¼ æ’­"""
        # çº¿æ€§å˜æ¢
        Q = np.dot(Q, self.W_q)
        K = np.dot(K, self.W_k)
        V = np.dot(V, self.W_v)

        # åˆ†å‰²ä¸ºå¤šä¸ªå¤´
        Q_heads = self.split_heads(Q)
        K_heads = self.split_heads(K)
        V_heads = self.split_heads(V)

        # å¯¹æ¯ä¸ªå¤´è®¡ç®—Attention
        batch_size, num_heads, seq_len, d_k = Q_heads.shape
        attention_outputs = []

        for i in range(num_heads):
            head_output = self._scaled_dot_product_attention(
                Q_heads[:, i, :, :],
                K_heads[:, i, :, :],
                V_heads[:, i, :, :]
            )
            attention_outputs.append(head_output)

        # åˆå¹¶å¤šå¤´è¾“å‡º
        combined = np.stack(attention_outputs, axis=1)
        combined = self.combine_heads(combined)

        # æœ€ç»ˆçº¿æ€§å˜æ¢
        output = np.dot(combined, self.W_o)

        return output

    def _scaled_dot_product_attention(self, Q, K, V):
        """ç¼©æ”¾ç‚¹ç§¯Attention"""
        d_k = Q.shape[-1]
        scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)
        weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)
        output = np.matmul(weights, V)
        return output

# ä½¿ç”¨ç¤ºä¾‹
d_model = 512
num_heads = 8
seq_len = 10
batch_size = 4

Q = np.random.randn(batch_size, seq_len, d_model)
K = np.random.randn(batch_size, seq_len, d_model)
V = np.random.randn(batch_size, seq_len, d_model)

mha = MultiHeadAttention(d_model, num_heads)
output = mha.forward(Q, K, V)
print("Multi-Head Attentionè¾“å‡ºå½¢çŠ¶:", output.shape)
```

## ğŸ­ Attentionæœºåˆ¶çš„ç›´è§‚ç†è§£

### å‡ ä½•è§£é‡Š

æˆ‘ä»¬å¯ä»¥ä»å‡ ä½•è§’åº¦ç†è§£Attentionï¼š

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

def geometric_attention_demo():
    """å‡ ä½•è§£é‡ŠAttention"""
    # åˆ›å»º3Då‘é‡
    fig = plt.figure(figsize=(12, 4))

    # Queryå‘é‡
    query = np.array([1, 0.5, 0.3])

    # Keyå‘é‡
    keys = np.array([
        [0.8, 0.2, 0.1],   # ç›¸ä¼¼
        [0.1, 0.9, 0.2],   # ä¸ç›¸ä¼¼
        [0.9, 0.4, 0.2]    # å¾ˆç›¸ä¼¼
    ])

    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = np.dot(keys, query)
    weights = np.exp(similarities) / np.sum(np.exp(similarities))

    # 3Då¯è§†åŒ–
    ax = fig.add_subplot(121, projection='3d')

    # ç»˜åˆ¶Query
    ax.quiver(0, 0, 0, query[0], query[1], query[2],
              color='red', arrow_length_ratio=0.1, linewidth=3, label='Query')

    # ç»˜åˆ¶Keys
    colors = ['blue', 'green', 'orange']
    for i, (key, weight) in enumerate(zip(keys, weights)):
        ax.quiver(0, 0, 0, key[0], key[1], key[2],
                  color=colors[i], arrow_length_ratio=0.1,
                  linewidth=2*weight+1,
                  label=f'Key {i+1} (weight={weight:.2f})')

    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    ax.set_title('Attentionçš„å‡ ä½•è§£é‡Š')
    ax.legend()

    # æƒé‡å¯è§†åŒ–
    ax2 = fig.add_subplot(122)
    ax2.bar(['Key 1', 'Key 2', 'Key 3'], weights, color=colors)
    ax2.set_ylabel('Attentionæƒé‡')
    ax2.set_title('Attentionæƒé‡åˆ†å¸ƒ')

    plt.tight_layout()
    plt.show()

geometric_attention_demo()
```

### ä¿¡æ¯è¿‡æ»¤çš„è§†è§’

Attentionæœºåˆ¶æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª**ä¿¡æ¯è¿‡æ»¤å™¨**ï¼š

```python
def attention_as_filter():
    """å°†Attentionç†è§£ä¸ºä¿¡æ¯è¿‡æ»¤å™¨"""

    # å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«å¤šç§ä¿¡æ¯çš„åºåˆ—
    information = {
        "é‡è¦ä¿¡æ¯1": "è¿™æ˜¯å…³é”®å†…å®¹",
        "é‡è¦ä¿¡æ¯2": "è¿™ä¹Ÿæ˜¯å…³é”®",
        "æ¬¡è¦ä¿¡æ¯1": "è¿™ä¸ªä¸å¤ªé‡è¦",
        "æ¬¡è¦ä¿¡æ¯2": "è¿™ä¸ªå¯ä»¥å¿½ç•¥",
        "å™ªå£°ä¿¡æ¯": "è¿™çº¯ç²¹æ˜¯å™ªå£°"
    }

    # Attentionå°±åƒä¸€ä¸ªæ™ºèƒ½è¿‡æ»¤å™¨
    def intelligent_filter(query, information):
        weights = {
            "é‡è¦ä¿¡æ¯1": 0.4,    # é‡ç‚¹å…³æ³¨
            "é‡è¦ä¿¡æ¯2": 0.3,    # é‡ç‚¹å…³æ³¨
            "æ¬¡è¦ä¿¡æ¯1": 0.2,    # é€‚å½“å…³æ³¨
            "æ¬¡è¦ä¿¡æ¯2": 0.08,   # å°‘é‡å…³æ³¨
            "å™ªå£°ä¿¡æ¯": 0.02     # å‡ ä¹å¿½ç•¥
        }

        # è¿‡æ»¤ç»“æœï¼šé‡ç‚¹å…³æ³¨é‡è¦ä¿¡æ¯
        filtered_content = ""
        for item, weight in weights.items():
            if weight > 0.1:  # åªä¿ç•™æƒé‡è¶…è¿‡é˜ˆå€¼çš„
                filtered_content += information[item] + " "

        return filtered_content, weights

    query = "æˆ‘éœ€è¦æœ€é‡è¦çš„ä¿¡æ¯"
    result, weights = intelligent_filter(query, information)

    print("è¿‡æ»¤ç»“æœ:", result)
    print("æ³¨æ„åŠ›åˆ†å¸ƒ:", weights)

attention_as_filter()
```

## ğŸš€ å¸¸è§è¯¯åŒºä¸æ¾„æ¸…

### è¯¯åŒº1ï¼šAttentionå°±æ˜¯åŠ æƒå¹³å‡

**é”™è¯¯ç†è§£**ï¼šAttentionåªæ˜¯ç®€å•çš„åŠ æƒå¹³å‡

**æ­£ç¡®ç†è§£**ï¼šAttentionæ˜¯**å¯å­¦ä¹ çš„**åŠ æƒå¹³å‡ã€‚å…³é”®åœ¨äºï¼š
- æƒé‡æ˜¯é€šè¿‡å­¦ä¹ å¾—åˆ°çš„ï¼Œä¸æ˜¯é¢„å…ˆå›ºå®šçš„
- Qã€Kã€Véƒ½æ˜¯å¯å­¦ä¹ çš„å‚æ•°
- æƒé‡è®¡ç®—è¿‡ç¨‹æœ¬èº«åŒ…å«å¤æ‚çš„éçº¿æ€§å˜æ¢

### è¯¯åŒº2ï¼šAttentionåªç”¨äºNLP

**é”™è¯¯ç†è§£**ï¼šAttentionåªç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†

**æ­£ç¡®ç†è§£**ï¼šAttentionæ˜¯é€šç”¨çš„æœºåˆ¶ï¼Œå¹¿æ³›åº”ç”¨äºï¼š
- **è®¡ç®—æœºè§†è§‰**ï¼šå›¾åƒå­—å¹•ç”Ÿæˆã€ç›®æ ‡æ£€æµ‹
- **è¯­éŸ³è¯†åˆ«**ï¼šå£°å­¦æ¨¡å‹å»ºæ¨¡
- **æ¨èç³»ç»Ÿ**ï¼šç”¨æˆ·-ç‰©å“äº¤äº’å»ºæ¨¡
- **å¼ºåŒ–å­¦ä¹ **ï¼šç­–ç•¥å’Œå€¼å‡½æ•°å­¦ä¹ 

### è¯¯åŒº3ï¼šMulti-Headå°±æ˜¯å¹¶è¡Œè®¡ç®—

**é”™è¯¯ç†è§£**ï¼šMulti-Headåªæ˜¯ä¸ºäº†åŠ é€Ÿè®¡ç®—

**æ­£ç¡®ç†è§£**ï¼šMulti-Headçš„æ ¸å¿ƒä»·å€¼åœ¨äºï¼š
- æ¯ä¸ªå¤´å­¦ä¹ ä¸åŒçš„è¡¨ç¤ºç©ºé—´
- æ•æ‰ä¸åŒç±»å‹çš„å…³ç³»å’Œæ¨¡å¼
- æé«˜æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›

## ğŸ“Š æ€§èƒ½åˆ†æä¸ä¼˜åŒ–

### è®¡ç®—å¤æ‚åº¦

è®©æˆ‘ä»¬åˆ†æAttentionçš„è®¡ç®—å¤æ‚åº¦ï¼š

```python
def attention_complexity(seq_len, d_model):
    """åˆ†æAttentionçš„è®¡ç®—å¤æ‚åº¦"""

    # QK^Tè®¡ç®—ï¼šO(seq_len^2 Ã— d_model)
    qkt_complexity = seq_len ** 2 * d_model

    # Softmaxè®¡ç®—ï¼šO(seq_len^2)
    softmax_complexity = seq_len ** 2

    # åŠ æƒæ±‚å’Œï¼šO(seq_len^2 Ã— d_model)
    weighted_sum_complexity = seq_len ** 2 * d_model

    total_complexity = qkt_complexity + softmax_complexity + weighted_sum_complexity

    print(f"åºåˆ—é•¿åº¦: {seq_len}, æ¨¡å‹ç»´åº¦: {d_model}")
    print(f"QK^Tè®¡ç®—å¤æ‚åº¦: O({qkt_complexity})")
    print(f"Softmaxè®¡ç®—å¤æ‚åº¦: O({softmax_complexity})")
    print(f"åŠ æƒæ±‚å’Œå¤æ‚åº¦: O({weighted_sum_complexity})")
    print(f"æ€»å¤æ‚åº¦: O({total_complexity})")
    print(f"è¿‘ä¼¼ä¸º: O({seq_len}^2 Ã— {d_model})")

    return total_complexity

# ä¸åŒè§„æ¨¡çš„å¤æ‚åº¦åˆ†æ
print("=== ä¸åŒè§„æ¨¡çš„Attentionå¤æ‚åº¦ ===")
attention_complexity(512, 512)    # ä¸­ç­‰è§„æ¨¡
attention_complexity(2048, 512)   # è¾ƒå¤§è§„æ¨¡
attention_complexity(8192, 512)   # è¶…å¤§è§„æ¨¡
```

### å†…å­˜ä½¿ç”¨åˆ†æ

```python
def attention_memory_usage(seq_len, d_model, dtype_size=4):
    """åˆ†æAttentionçš„å†…å­˜ä½¿ç”¨"""

    # Q, K, VçŸ©é˜µï¼š3 Ã— seq_len Ã— d_model
    qkv_memory = 3 * seq_len * d_model * dtype_size

    # æ³¨æ„åŠ›çŸ©é˜µï¼šseq_len Ã— seq_len
    attention_matrix_memory = seq_len * seq_len * dtype_size

    # ä¸­é—´ç»“æœï¼šseq_len Ã— d_model
    intermediate_memory = seq_len * d_model * dtype_size

    total_memory = qkv_memory + attention_matrix_memory + intermediate_memory

    print(f"åºåˆ—é•¿åº¦: {seq_len}, æ¨¡å‹ç»´åº¦: {d_model}")
    print(f"QKVçŸ©é˜µå†…å­˜: {qkv_memory / 1024 / 1024:.2f} MB")
    print(f"æ³¨æ„åŠ›çŸ©é˜µå†…å­˜: {attention_matrix_memory / 1024 / 1024:.2f} MB")
    print(f"ä¸­é—´ç»“æœå†…å­˜: {intermediate_memory / 1024 / 1024:.2f} MB")
    print(f"æ€»å†…å­˜ä½¿ç”¨: {total_memory / 1024 / 1024:.2f} MB")

    return total_memory

print("\n=== Attentionå†…å­˜ä½¿ç”¨åˆ†æ ===")
attention_memory_usage(512, 512)    # ä¸­ç­‰è§„æ¨¡
attention_memory_usage(2048, 512)   # è¾ƒå¤§è§„æ¨¡
attention_memory_usage(8192, 512)   # è¶…å¤§è§„æ¨¡
```

## ğŸ¯ æ€»ç»“ä¸å±•æœ›

### æ ¸å¿ƒè¦ç‚¹å›é¡¾

é€šè¿‡æœ¬æ–‡çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æŒæ¡äº†ï¼š

1. **Attentionçš„æœ¬è´¨**ï¼šå¯å­¦ä¹ çš„åŠ æƒæ±‚å’Œæœºåˆ¶
2. **æ•°å­¦åŸç†**ï¼šQKVç›¸ä¼¼åº¦è®¡ç®—ã€Softmaxå½’ä¸€åŒ–ã€åŠ æƒæ±‚å’Œ
3. **å®ç°æ–¹æ³•**ï¼šä»åŸºç¡€çš„NumPyå®ç°åˆ°å®Œæ•´çš„Multi-Head Attention
4. **å‡ ä½•è§£é‡Š**ï¼šå‘é‡ç©ºé—´ä¸­çš„ç›¸ä¼¼åº¦åº¦é‡
5. **æ€§èƒ½åˆ†æ**ï¼šè®¡ç®—å¤æ‚åº¦å’Œå†…å­˜ä½¿ç”¨çš„æƒè¡¡

### ä»æµ…åˆ°æ·±çš„çŸ¥è¯†ä½“ç³»

**æµ…å±‚æ¬¡ç†è§£**ï¼š
- Attentionå°±æ˜¯"å…³æ³¨é‡è¦çš„éƒ¨åˆ†"
- é€šè¿‡æƒé‡åˆ†é…å®ç°ä¿¡æ¯è¿‡æ»¤
- ç±»ä¼¼äººç±»çš„æ³¨æ„åŠ›æœºåˆ¶

**æ·±å±‚æ¬¡ç†è§£**ï¼š
- Attentionæ˜¯å¯å­¦ä¹ çš„ç›¸ä¼¼åº¦è®¡ç®—
- QKVæä¾›äº†ä¸°å¯Œçš„è¡¨è¾¾èƒ½åŠ›
- Multi-Headå®ç°äº†å¤šå…³ç³»å»ºæ¨¡
- æ•°å­¦ä¸Šæ˜¯æœ€ä¼˜çš„çº¿æ€§ç»„åˆï¼ˆåœ¨ç‰¹å®šå‡è®¾ä¸‹ï¼‰

### ä¸‹ä¸€æ­¥å­¦ä¹ æ–¹å‘

1. **æ·±å…¥ç†è§£Scaled Dot-Product Attention**ï¼šç¼©æ”¾å› å­çš„æ·±å±‚å«ä¹‰
2. **å­¦ä¹ FlashAttentionç­‰é«˜æ•ˆå®ç°**ï¼šIOå¤æ‚åº¦çš„ä¼˜åŒ–
3. **æŒæ¡ä¸åŒAttentionå˜ä½“**ï¼šSparse Attentionã€Local Attentionç­‰
4. **ç†è§£Attentionåœ¨å®é™…åº”ç”¨ä¸­çš„ä¼˜åŒ–**ï¼šæ¨ç†åŠ é€Ÿã€å†…å­˜ä¼˜åŒ–ç­‰

---

**è®°ä½**ï¼šAttentionæœºåˆ¶çœ‹ä¼¼ç®€å•ï¼Œä½†å…¶èƒŒåçš„æ•°å­¦åŸç†å’Œå·¥ç¨‹å®ç°éƒ½è•´å«ç€æ·±åˆ»çš„æ™ºæ…§ã€‚æŒæ¡äº†Attentionï¼Œä½ å°±æŒæ¡äº†ç°ä»£æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒå¼•æ“ä¹‹ä¸€ã€‚

*ä¸‹ä¸€ç¯‡æ–‡ç« å°†æ·±å…¥è§£æScaled Dot-Product Attentionï¼Œç†è§£ç¼©æ”¾å› å­çš„é‡è¦æ€§ä»¥åŠæ•°å€¼ç¨³å®šæ€§çš„å…³é”®æŠ€å·§ã€‚* ğŸš€