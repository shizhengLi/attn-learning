# å†…å­˜å¤ç”¨ä¸æµå¼Attentionï¼šçªç ´å†…å­˜é™åˆ¶çš„ç»ˆææ–¹æ¡ˆ

## ğŸ¯ å¼•è¨€ï¼šæ— é™åºåˆ—çš„å¤„ç†æŒ‘æˆ˜

åœ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†å’Œè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå†…å­˜ä¸€ç›´æ˜¯æœ€å¤§çš„ç“¶é¢ˆä¹‹ä¸€ã€‚æƒ³è±¡ä¸€ä¸‹å¤„ç†ä¸€ä¸ª100ä¸‡å­—çš„å°è¯´æˆ–æ•°å°æ—¶çš„é•¿è§†é¢‘è½¬å½•ï¼Œä¼ ç»Ÿçš„æ–¹æ³•éœ€è¦å°†æ•´ä¸ªåºåˆ—åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œè¿™ç®€ç›´æ˜¯ä¸å¯æƒ³è±¡çš„æŒ‘æˆ˜ã€‚

æµå¼Attentionå’Œå†…å­˜å¤ç”¨æŠ€æœ¯æ­£æ˜¯ä¸ºäº†è§£å†³è¿™ä¸€æ ¹æœ¬é—®é¢˜è€Œç”Ÿã€‚å®ƒä»¬é€šè¿‡å·§å¦™çš„å†…å­˜ç®¡ç†ç­–ç•¥ï¼Œè®©æ¨¡å‹èƒ½å¤Ÿå¤„ç†"æ— é™é•¿"çš„åºåˆ—ï¼Œè€Œå†…å­˜ä½¿ç”¨å´ä¿æŒæ’å®šã€‚è¿™å°±åƒæ˜¯ç”¨ä¸€ä¸ªå¾ˆå°çš„æ¯å­å»èˆ€å¹²å¤§æµ·çš„æ°´æ»´ï¼Œä¸€æ»´ä¸€æ»´åœ°å¤„ç†ï¼Œæœ€ç»ˆå®Œæˆçœ‹ä¼¼ä¸å¯èƒ½çš„ä»»åŠ¡ã€‚

æœ¬æ–‡å°†æ·±å…¥æ¢è®¨æµå¼Attentionçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œä»å¾ªç¯ç¼“å†²åŒºçš„ä¼˜é›…è®¾è®¡åˆ°æ»‘åŠ¨çª—å£çš„æ™ºèƒ½ç­–ç•¥ï¼Œè®©ä½ å…¨é¢ç†è§£è¿™é¡¹çªç ´å†…å­˜é™åˆ¶çš„é©å‘½æ€§æŠ€æœ¯ã€‚

## ğŸ§  æµå¼å¤„ç†çš„æ ¸å¿ƒæ€æƒ³

### ä¼ ç»Ÿæ–¹æ³•çš„å†…å­˜å›°å¢ƒ

è®©æˆ‘ä»¬å…ˆç†è§£ä¸ºä»€ä¹ˆä¼ ç»Ÿæ–¹æ³•æ— æ³•å¤„ç†é•¿åºåˆ—ï¼š

```python
def traditional_memory_analysis():
    """åˆ†æä¼ ç»Ÿæ–¹æ³•çš„å†…å­˜ä½¿ç”¨é—®é¢˜"""

    print("=== ä¼ ç»ŸAttentionå†…å­˜ä½¿ç”¨åˆ†æ ===")

    # æ¨¡æ‹Ÿä¸åŒåºåˆ—é•¿åº¦çš„å†…å­˜éœ€æ±‚
    seq_lengths = [1024, 4096, 16384, 65536, 262144]  # 1Kåˆ°256K tokens
    hidden_dim = 4096
    num_heads = 32
    head_dim = hidden_dim // num_heads
    dtype_size = 2  # FP16

    print(f"æ¨¡å‹é…ç½®: hidden_dim={hidden_dim}, num_heads={num_heads}")
    print(f"æ•°æ®ç±»å‹: FP16 (æ¯ä¸ªå…ƒç´ {dtype_size}å­—èŠ‚)")
    print()

    print("åºåˆ—é•¿åº¦\tKVç¼“å­˜\t\tAttentionçŸ©é˜µ\tæ€»å†…å­˜\t\tå†…å­˜ä½¿ç”¨ç‡")
    print("-" * 70)

    for seq_len in seq_lengths:
        # KVç¼“å­˜å†…å­˜ (batch_size=1)
        kv_memory = seq_len * hidden_dim * 2 * dtype_size  # Kå’ŒV

        # AttentionçŸ©é˜µå†…å­˜
        attn_memory = seq_len * seq_len * dtype_size

        # æ€»å†…å­˜
        total_memory = kv_memory + attn_memory
        total_memory_gb = total_memory / (1024**3)

        # ç›¸å¯¹äº16GBå†…å­˜çš„ä½¿ç”¨ç‡
        memory_utilization = total_memory / (16 * 1024**3) * 100

        print(f"{seq_len:8d}\t{kv_memory/1024**2:8.1f}MB\t{attn_memory/1024**2:10.1f}MB\t"
              f"{total_memory_gb:6.2f}GB\t{memory_utilization:8.1f}%")

    print()
    print("ç»“è®º:")
    print("- åºåˆ—é•¿åº¦è¶…è¿‡65Kæ—¶ï¼Œä»…AttentionçŸ©é˜µå°±éœ€è¦16GBå†…å­˜")
    print("- ä¼ ç»Ÿæ–¹æ³•æ— æ³•å¤„ç†è¶…è¿‡100Kçš„é•¿åºåˆ—")
    print("- å†…å­˜ä½¿ç”¨å‘ˆO(nÂ²)å¢é•¿ï¼Œæ— æ³•æ‰©å±•")

traditional_memory_analysis()
```

### æµå¼å¤„ç†çš„è®¾è®¡å“²å­¦

```python
def streaming_philosophy_demo():
    """æ¼”ç¤ºæµå¼å¤„ç†çš„æ ¸å¿ƒæ€æƒ³"""

    print("=== æµå¼å¤„ç†è®¾è®¡å“²å­¦ ===")
    print()
    print("ä¼ ç»Ÿæ–¹æ³• (æ‰¹å¤„ç†):")
    print("  [å®Œæ•´åºåˆ—] â†’ [ä¸€æ¬¡æ€§å¤„ç†] â†’ [å®Œæ•´è¾“å‡º]")
    print("  é—®é¢˜: éœ€è¦æ— é™å†…å­˜")
    print()
    print("æµå¼æ–¹æ³• (å¢é‡å¤„ç†):")
    print("  [ç‰‡æ®µ1] â†’ [å¤„ç†1] â†’ [è¾“å‡º1] â”€â”")
    print("  [ç‰‡æ®µ2] â†’ [å¤„ç†2] â†’ [è¾“å‡º2] â†â”¤â”€ ä¸Šä¸‹æ–‡çª—å£")
    print("  [ç‰‡æ®µ3] â†’ [å¤„ç†3] â†’ [è¾“å‡º3] â†â”˜")
    print("  ä¼˜åŠ¿: æ’å®šå†…å­˜ä½¿ç”¨")
    print()

    # å¯è§†åŒ–æµå¼å¤„ç†
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # ä¼ ç»Ÿæ–¹æ³•
    ax1.bar(['è¾“å…¥', 'ä¸­é—´ç»“æœ', 'è¾“å‡º'], [100, 100, 100],
            color=['blue', 'red', 'green'], alpha=0.7)
    ax1.set_title('ä¼ ç»Ÿæ‰¹å¤„ç†æ–¹æ³•', fontsize=14, fontweight='bold')
    ax1.set_ylabel('å†…å­˜ä½¿ç”¨ (ç›¸å¯¹å•ä½)')
    ax1.set_ylim(0, 120)
    for i, v in enumerate([100, 100, 100]):
        ax1.text(i, v + 5, f'{v}%', ha='center', fontweight='bold')

    # æµå¼æ–¹æ³•
    ax2.bar(['è¾“å…¥ç‰‡æ®µ', 'å›ºå®šçª—å£', 'è¾“å‡ºç‰‡æ®µ'], [10, 20, 10],
            color=['blue', 'orange', 'green'], alpha=0.7)
    ax2.set_title('æµå¼å¤„ç†æ–¹æ³•', fontsize=14, fontweight='bold')
    ax2.set_ylabel('å†…å­˜ä½¿ç”¨ (ç›¸å¯¹å•ä½)')
    ax2.set_ylim(0, 120)
    for i, v in enumerate([10, 20, 10]):
        ax2.text(i, v + 5, f'{v}%', ha='center', fontweight='bold')

    plt.tight_layout()
    plt.show()

streaming_philosophy_demo()
```

## ğŸ”„ å¾ªç¯ç¼“å†²åŒºï¼šå†…å­˜å¤ç”¨çš„æ ¸å¿ƒ

### å¾ªç¯ç¼“å†²åŒºçš„åŸºæœ¬åŸç†

```python
class CircularBuffer:
    """é«˜æ•ˆçš„å¾ªç¯ç¼“å†²åŒºå®ç°"""

    def __init__(self, capacity, item_dim):
        self.capacity = capacity
        self.item_dim = item_dim
        self.buffer = torch.zeros(capacity, item_dim)
        self.start = 0
        self.size = 0

    def append(self, item):
        """æ·»åŠ æ–°é¡¹ç›®ï¼Œè¦†ç›–æœ€æ—§çš„é¡¹ç›®"""
        # è®¡ç®—å†™å…¥ä½ç½®
        pos = (self.start + self.size) % self.capacity

        # å†™å…¥æ•°æ®
        self.buffer[pos] = item

        # æ›´æ–°çŠ¶æ€
        if self.size < self.capacity:
            self.size += 1
        else:
            # ç¼“å†²åŒºæ»¡äº†ï¼Œç§»åŠ¨èµ·å§‹ä½ç½®
            self.start = (self.start + 1) % self.capacity

    def get_recent(self, n=None):
        """è·å–æœ€è¿‘çš„nä¸ªé¡¹ç›®"""
        if n is None:
            n = self.size

        if n > self.size:
            n = self.size

        # è®¡ç®—è¯»å–ä½ç½®
        result = []
        for i in range(n):
            pos = (self.start + self.size - n + i) % self.capacity
            result.append(self.buffer[pos])

        return torch.stack(result)

    def get_all(self):
        """è·å–æ‰€æœ‰æ•°æ®ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰"""
        if self.size == 0:
            return torch.empty(0, self.item_dim)

        result = []
        for i in range(self.size):
            pos = (self.start + i) % self.capacity
            result.append(self.buffer[pos])

        return torch.stack(result)

    def clear(self):
        """æ¸…ç©ºç¼“å†²åŒº"""
        self.start = 0
        self.size = 0

    def __len__(self):
        return self.size

# å¾ªç¯ç¼“å†²åŒºæ¼”ç¤º
def circular_buffer_demo():
    """æ¼”ç¤ºå¾ªç¯ç¼“å†²åŒºçš„å·¥ä½œåŸç†"""

    print("=== å¾ªç¯ç¼“å†²åŒºå·¥ä½œæ¼”ç¤º ===")

    # åˆ›å»ºå®¹é‡ä¸º5çš„ç¼“å†²åŒº
    buffer = CircularBuffer(capacity=5, item_dim=3)

    # æ·»åŠ æ•°æ®
    data = [
        torch.tensor([1, 2, 3]),
        torch.tensor([4, 5, 6]),
        torch.tensor([7, 8, 9]),
        torch.tensor([10, 11, 12]),
        torch.tensor([13, 14, 15]),
        torch.tensor([16, 17, 18]),  # è¿™ä¼šè¦†ç›–ç¬¬ä¸€ä¸ªå…ƒç´ 
        torch.tensor([19, 20, 21]),  # è¿™ä¼šè¦†ç›–ç¬¬äºŒä¸ªå…ƒç´ 
    ]

    print("é€æ­¥æ·»åŠ æ•°æ®:")
    for i, item in enumerate(data):
        buffer.append(item)
        all_data = buffer.get_all()
        recent_data = buffer.get_recent(3)

        print(f"æ­¥éª¤ {i+1}:")
        print(f"  æ·»åŠ : {item.tolist()}")
        print(f"  å…¨éƒ¨: {all_data.tolist()}")
        print(f"  æœ€è¿‘3ä¸ª: {recent_data.tolist()}")
        print(f"  ç¼“å†²åŒºçŠ¶æ€: start={buffer.start}, size={buffer.size}")
        print()

circular_buffer_demo()
```

### åŸºäºå¾ªç¯ç¼“å†²åŒºçš„KVç¼“å­˜

```python
class StreamingKVCache:
    """åŸºäºå¾ªç¯ç¼“å†²åŒºçš„æµå¼KVç¼“å­˜"""

    def __init__(self, max_seq_len, num_heads, head_dim, window_size=None):
        self.max_seq_len = max_seq_len
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.window_size = window_size or max_seq_len

        # åˆ›å»ºå¾ªç¯ç¼“å†²åŒº
        self.k_buffer = CircularBuffer(self.window_size, num_heads * head_dim)
        self.v_buffer = CircularBuffer(self.window_size, num_heads * head_dim)

        # è·Ÿè¸ªå…¨å±€ä½ç½®
        self.global_position = 0

    def update(self, new_k, new_v):
        """æ›´æ–°KVç¼“å­˜"""
        batch_size, new_seq_len, num_heads, head_dim = new_k.shape

        # ç¡®ä¿batch_sizeä¸º1ï¼ˆæµå¼å¤„ç†é€šå¸¸é€ä¸ªå¤„ç†ï¼‰
        assert batch_size == 1, "æµå¼å¤„ç†åªæ”¯æŒbatch_size=1"

        # é‡å¡‘ä¸º2Då‘é‡
        new_k_flat = new_k.view(new_seq_len, -1)  # [seq_len, num_heads * head_dim]
        new_v_flat = new_v.view(new_seq_len, -1)

        # é€ä¸ªæ·»åŠ åˆ°ç¼“å†²åŒº
        for i in range(new_seq_len):
            self.k_buffer.append(new_k_flat[i])
            self.v_buffer.append(new_v_flat[i])
            self.global_position += 1

    def get_cache(self):
        """è·å–å½“å‰ç¼“å­˜å†…å®¹"""
        if len(self.k_buffer) == 0:
            return None, None

        k_data = self.k_buffer.get_all()
        v_data = self.v_buffer.get_all()

        # é‡å¡‘ä¸ºåŸå§‹æ ¼å¼
        seq_len = k_data.shape[0]
        k = k_data.view(seq_len, self.num_heads, self.head_dim).transpose(0, 1)
        v = v_data.view(seq_len, self.num_heads, self.head_dim).transpose(0, 1)

        return k.unsqueeze(0), v.unsqueeze(0)  # æ·»åŠ batchç»´åº¦

    def get_window_info(self):
        """è·å–çª—å£ä¿¡æ¯"""
        return {
            "window_size": self.window_size,
            "current_length": len(self.k_buffer),
            "global_position": self.global_position,
            "utilization": len(self.k_buffer) / self.window_size
        }

# æµå¼KVç¼“å­˜æµ‹è¯•
def test_streaming_kv_cache():
    """æµ‹è¯•æµå¼KVç¼“å­˜"""

    print("=== æµå¼KVç¼“å­˜æµ‹è¯• ===")

    # é…ç½®
    num_heads = 8
    head_dim = 64
    window_size = 512
    total_sequence_length = 2000

    cache = StreamingKVCache(
        max_seq_len=total_sequence_length,
        num_heads=num_heads,
        head_dim=head_dim,
        window_size=window_size
    )

    print(f"é…ç½®: window_size={window_size}, total_length={total_sequence_length}")
    print()

    # æ¨¡æ‹Ÿæµå¼å¤„ç†
    chunk_size = 128
    num_chunks = (total_sequence_length + chunk_size - 1) // chunk_size

    for chunk_idx in range(num_chunks):
        start_pos = chunk_idx * chunk_size
        end_pos = min(start_pos + chunk_size, total_sequence_length)
        actual_chunk_size = end_pos - start_pos

        # ç”Ÿæˆæ¨¡æ‹ŸKVæ•°æ®
        new_k = torch.randn(1, actual_chunk_size, num_heads, head_dim)
        new_v = torch.randn(1, actual_chunk_size, num_heads, head_dim)

        # æ›´æ–°ç¼“å­˜
        cache.update(new_k, new_v)

        # è·å–ç¼“å­˜ä¿¡æ¯
        info = cache.get_window_info()

        print(f"Chunk {chunk_idx + 1}/{num_chunks}: "
              f"å¤„ç†{actual_chunk_size}ä¸ªtoken, "
              f"ç¼“å­˜é•¿åº¦={info['current_length']}, "
              f"åˆ©ç”¨ç‡={info['utilization']:.2f}")

        # å¦‚æœç¼“å†²åŒºæ»¡äº†ï¼Œåº”è¯¥ä¿æŒå›ºå®šå¤§å°
        if info['current_length'] == window_size:
            print("  -> ç¼“å†²åŒºå·²æ»¡ï¼Œå¼€å§‹å¾ªç¯å¤ç”¨")

    print()
    print("æœ€ç»ˆç¼“å­˜ä¿¡æ¯:")
    final_info = cache.get_window_info()
    for key, value in final_info.items():
        print(f"  {key}: {value}")

test_streaming_kv_cache()
```

## ğŸŒŠ æ»‘åŠ¨çª—å£Attentionï¼šæ™ºèƒ½çš„ä¸Šä¸‹æ–‡ç®¡ç†

### å›ºå®šçª—å£ vs åŠ¨æ€çª—å£

```python
class SlidingWindowAttention:
    """æ»‘åŠ¨çª—å£Attentionå®ç°"""

    def __init__(self, window_size, stride=1, dynamic_window=False):
        self.window_size = window_size
        self.stride = stride
        self.dynamic_window = dynamic_window

        # åŠ¨æ€çª—å£ç›¸å…³å‚æ•°
        self.importance_scores = {}
        self.min_window_size = window_size // 2
        self.max_window_size = window_size * 2

    def compute_fixed_window_attention(self, q, k, v, attention_mask=None):
        """è®¡ç®—å›ºå®šçª—å£Attention"""
        batch_size, num_heads, seq_len, head_dim = q.shape

        # è®¡ç®—Attentionåˆ†æ•°
        scores = torch.matmul(q, k.transpose(-2, -1))

        # åº”ç”¨å›ºå®šçª—å£mask
        window_mask = self._create_fixed_window_mask(seq_len, q.device)
        scores = scores.masked_fill(window_mask == 0, float('-inf'))

        # åº”ç”¨é¢å¤–attention mask
        if attention_mask is not None:
            scores = scores.masked_fill(attention_mask == 0, float('-inf'))

        # Softmaxå’ŒåŠ æƒ
        attn_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attn_weights, v)

        return output, attn_weights

    def compute_dynamic_window_attention(self, q, k, v, position_ids=None):
        """è®¡ç®—åŠ¨æ€çª—å£Attention"""
        batch_size, num_heads, seq_len, head_dim = q.shape

        # è®¡ç®—åŸºç¡€Attentionåˆ†æ•°
        scores = torch.matmul(q, k.transpose(-2, -1))

        # åŸºäºé‡è¦æ€§è°ƒæ•´çª—å£å¤§å°
        window_sizes = self._compute_dynamic_window_sizes(scores, position_ids)

        # åº”ç”¨åŠ¨æ€çª—å£mask
        dynamic_mask = self._create_dynamic_window_mask(window_sizes, seq_len, q.device)
        scores = scores.masked_fill(dynamic_mask == 0, float('-inf'))

        # Softmaxå’ŒåŠ æƒ
        attn_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attn_weights, v)

        return output, attn_weights

    def _create_fixed_window_mask(self, seq_len, device):
        """åˆ›å»ºå›ºå®šçª—å£mask"""
        mask = torch.ones(seq_len, seq_len, device=device)

        for i in range(seq_len):
            # è®¡ç®—çª—å£èŒƒå›´
            start = max(0, i - self.window_size // 2)
            end = min(seq_len, i + self.window_size // 2 + 1)

            # çª—å£å¤–çš„ä½ç½®è®¾ä¸º0
            mask[i, :start] = 0
            mask[i, end:] = 0

        return mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]

    def _compute_dynamic_window_sizes(self, scores, position_ids):
        """åŸºäºAttentionåˆ†æ•°è®¡ç®—åŠ¨æ€çª—å£å¤§å°"""
        batch_size, num_heads, seq_len, _ = scores.shape

        # è®¡ç®—æ¯ä¸ªä½ç½®çš„æ³¨æ„åŠ›å¼ºåº¦
        attention_strength = torch.mean(torch.abs(scores), dim=(1, 2))  # [batch_size, seq_len]

        # å½’ä¸€åŒ–åˆ°[min_window_size, max_window_size]
        normalized_strength = (attention_strength - attention_strength.min()) / (
            attention_strength.max() - attention_strength.min() + 1e-8
        )

        window_sizes = (
            self.min_window_size +
            normalized_strength * (self.max_window_size - self.min_window_size)
        )

        return window_sizes.int()

    def _create_dynamic_window_mask(self, window_sizes, seq_len, device):
        """åˆ›å»ºåŠ¨æ€çª—å£mask"""
        batch_size = window_sizes.shape[0]
        mask = torch.ones(batch_size, seq_len, seq_len, device=device)

        for b in range(batch_size):
            for i in range(seq_len):
                window_size = window_sizes[b, i].item()
                start = max(0, i - window_size // 2)
                end = min(seq_len, i + window_size // 2 + 1)

                mask[b, i, :start] = 0
                mask[b, i, end:] = 0

        return mask.unsqueeze(1)  # [batch_size, 1, seq_len, seq_len]

# æ»‘åŠ¨çª—å£å¯¹æ¯”æµ‹è¯•
def compare_sliding_window_strategies():
    """å¯¹æ¯”ä¸åŒçš„æ»‘åŠ¨çª—å£ç­–ç•¥"""

    print("=== æ»‘åŠ¨çª—å£ç­–ç•¥å¯¹æ¯” ===")

    # æµ‹è¯•é…ç½®
    seq_len = 1024
    hidden_dim = 512
    num_heads = 8
    head_dim = hidden_dim // num_heads

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    q = torch.randn(1, num_heads, seq_len, head_dim)
    k = torch.randn(1, num_heads, seq_len, head_dim)
    v = torch.randn(1, num_heads, seq_len, head_dim)

    # ä¸åŒçª—å£å¤§å°
    window_sizes = [64, 128, 256, 512]

    print("çª—å£å¤§å°\tè®¡ç®—æ—¶é—´(ms)\tå†…å­˜ä½¿ç”¨(MB)\\tå¹³å‡æ³¨æ„åŠ›èŒƒå›´")
    print("-" * 60)

    for window_size in window_sizes:
        # åˆ›å»ºæ»‘åŠ¨çª—å£Attention
        swa = SlidingWindowAttention(window_size=window_size)

        # æ€§èƒ½æµ‹è¯•
        torch.cuda.synchronize() if torch.cuda.is_available() else None
        start_time = time.time()

        output, attn_weights = swa.compute_fixed_window_attention(q, k, v)

        torch.cuda.synchronize() if torch.cuda.is_available() else None
        compute_time = (time.time() - start_time) * 1000

        # å†…å­˜ä½¿ç”¨
        memory_usage = q.numel() * 4 * 4 / (1024**2)  # è¿‘ä¼¼è®¡ç®—

        # è®¡ç®—å¹³å‡æ³¨æ„åŠ›èŒƒå›´
        avg_attention_range = torch.mean(attn_weights != 0).item() * seq_len

        print(f"{window_size:8d}\t{compute_time:10.2f}\t{memory_use:10.1f}\t{avg_attention_range:14.1f}")

    print()
    print("åŠ¨æ€çª—å£ vs å›ºå®šçª—å£:")

    # åŠ¨æ€çª—å£æµ‹è¯•
    dynamic_swa = SlidingWindowAttention(window_size=256, dynamic_window=True)

    start_time = time.time()
    output_dynamic, attn_dynamic = dynamic_swa.compute_dynamic_window_attention(q, k, v)
    dynamic_time = (time.time() - start_time) * 1000

    # å›ºå®šçª—å£æµ‹è¯•
    fixed_swa = SlidingWindowAttention(window_size=256, dynamic_window=False)

    start_time = time.time()
    output_fixed, attn_fixed = fixed_swa.compute_fixed_window_attention(q, k, v)
    fixed_time = (time.time() - start_time) * 1000

    print(f"åŠ¨æ€çª—å£æ—¶é—´: {dynamic_time:.2f}ms")
    print(f"å›ºå®šçª—å£æ—¶é—´: {fixed_time:.2f}ms")
    print(f"æ—¶é—´å·®å¼‚: {(dynamic_time - fixed_time) / fixed_time * 100:.1f}%")

compare_sliding_window_strategies()
```

## ğŸš€ å®Œæ•´çš„æµå¼Attentionå®ç°

### æµå¼Attentionæ¶æ„

```python
class StreamingAttention(nn.Module):
    """å®Œæ•´çš„æµå¼Attentionå®ç°"""

    def __init__(self, d_model, num_heads, window_size=512,
                 enable_kv_cache=True, enable_sliding_window=True):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.scale = 1.0 / math.sqrt(self.head_dim)

        # çº¿æ€§å±‚
        self.q_proj = nn.Linear(d_model, d_model, bias=False)
        self.k_proj = nn.Linear(d_model, d_model, bias=False)
        self.v_proj = nn.Linear(d_model, d_model, bias=False)
        self.out_proj = nn.Linear(d_model, d_model, bias=True)

        # æµå¼ç»„ä»¶
        self.enable_kv_cache = enable_kv_cache
        self.enable_sliding_window = enable_sliding_window

        if enable_kv_cache:
            self.kv_cache = StreamingKVCache(
                max_seq_len=8192,
                num_heads=num_heads,
                head_dim=self.head_dim,
                window_size=window_size
            )

        if enable_sliding_window:
            self.sliding_window = SlidingWindowAttention(
                window_size=window_size,
                dynamic_window=False
            )

    def forward(self, x, attention_mask=None, position_ids=None, use_cache=True):
        """
        æµå¼å‰å‘ä¼ æ’­

        Args:
            x: [batch_size, seq_len, d_model] è¾“å…¥åºåˆ—
            attention_mask: å¯é€‰çš„attention mask
            position_ids: å¯é€‰çš„ä½ç½®ID
            use_cache: æ˜¯å¦ä½¿ç”¨KVç¼“å­˜
        """
        batch_size, seq_len, d_model = x.shape

        # QKVæŠ•å½±
        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # ä½¿ç”¨ç¼“å­˜æ—¶ï¼Œåˆå¹¶å†å²çš„KV
        if use_cache and self.enable_kv_cache and hasattr(self, 'kv_cache'):
            # è·å–å†å²ç¼“å­˜
            cached_k, cached_v = self.kv_cache.get_cache()

            if cached_k is not None:
                # åˆå¹¶å†å²å’Œå½“å‰çš„KV
                k = torch.cat([cached_k, k], dim=2)
                v = torch.cat([cached_v, v], dim=2)

            # æ›´æ–°ç¼“å­˜
            self.kv_cache.update(
                k[:, -seq_len:, :, :],  # åªæ›´æ–°æ–°çš„éƒ¨åˆ†
                v[:, -seq_len:, :, :]
            )

        # Attentionè®¡ç®—
        if self.enable_sliding_window and hasattr(self, 'sliding_window'):
            # ä½¿ç”¨æ»‘åŠ¨çª—å£Attention
            attn_output, attn_weights = self.sliding_window.compute_fixed_window_attention(
                q, k, v, attention_mask
            )
        else:
            # æ ‡å‡†Attention
            attn_output, attn_weights = self._standard_attention(q, k, v, attention_mask)

        # è¾“å‡ºæŠ•å½±
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        output = self.out_proj(attn_output)

        return output, attn_weights

    def _standard_attention(self, q, k, v, attention_mask=None):
        """æ ‡å‡†Attentionè®¡ç®—"""
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale

        if attention_mask is not None:
            scores = scores.masked_fill(attention_mask == 0, float('-inf'))

        attn_weights = F.softmax(scores, dim=-1)
        output = torch.matmul(attn_weights, v)

        return output, attn_weights

    def reset_cache(self):
        """é‡ç½®ç¼“å­˜"""
        if self.enable_kv_cache and hasattr(self, 'kv_cache'):
            self.kv_cache = StreamingKVCache(
                max_seq_len=8192,
                num_heads=self.num_heads,
                head_dim=self.head_dim,
                window_size=512
            )

    def get_cache_info(self):
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        if self.enable_kv_cache and hasattr(self, 'kv_cache'):
            return self.kv_cache.get_window_info()
        return None

# æµå¼Attentionæ¼”ç¤º
def streaming_attention_demo():
    """æ¼”ç¤ºæµå¼Attentionçš„å¤„ç†èƒ½åŠ›"""

    print("=== æµå¼Attentionæ¼”ç¤º ===")

    # é…ç½®
    d_model = 512
    num_heads = 8
    window_size = 256

    # åˆ›å»ºæ¨¡å‹
    model = StreamingAttention(
        d_model=d_model,
        num_heads=num_heads,
        window_size=window_size,
        enable_kv_cache=True,
        enable_sliding_window=True
    )

    # æ¨¡æ‹Ÿé•¿åºåˆ—å¤„ç†
    total_length = 2048
    chunk_size = 128
    num_chunks = (total_length + chunk_size - 1) // chunk_size

    print(f"å¤„ç†æ€»é•¿åº¦: {total_length} tokens")
    print(f"åˆ†å—å¤§å°: {chunk_size} tokens")
    print(f"çª—å£å¤§å°: {window_size} tokens")
    print(f"æ€»åˆ†å—æ•°: {num_chunks}")
    print()

    # é€æ­¥å¤„ç†
    for chunk_idx in range(num_chunks):
        start_pos = chunk_idx * chunk_size
        end_pos = min(start_pos + chunk_size, total_length)
        actual_chunk_size = end_pos - start_pos

        # ç”Ÿæˆè¾“å…¥æ•°æ®
        x = torch.randn(1, actual_chunk_size, d_model)

        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output, attn_weights = model(x, use_cache=True)

        # è·å–ç¼“å­˜ä¿¡æ¯
        cache_info = model.get_cache_info()
        if cache_info:
            utilization = cache_info['utilization']
            current_length = cache_info['current_length']
        else:
            utilization = 0
            current_length = 0

        print(f"Chunk {chunk_idx + 1:2d}/{num_chunks}: "
              f"å¤„ç†{actual_chunk_size:3d} tokens, "
              f"ç¼“å­˜é•¿åº¦={current_length:3d}, "
              f"åˆ©ç”¨ç‡={utilization:.2f}")

    print()
    print("æµå¼å¤„ç†å®Œæˆï¼")
    print("ä¼˜åŠ¿:")
    print("- æ’å®šçš„å†…å­˜ä½¿ç”¨")
    print("- å¯ä»¥å¤„ç†ä»»æ„é•¿çš„åºåˆ—")
    print("- ä¿æŒå±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯")

streaming_attention_demo()
```

## ğŸ“Š æ€§èƒ½åˆ†æä¸ä¼˜åŒ–

### å†…å­˜ä½¿ç”¨åˆ†æ

```python
def memory_usage_analysis():
    """åˆ†æä¸åŒæ–¹æ³•çš„å†…å­˜ä½¿ç”¨æƒ…å†µ"""

    print("=== å†…å­˜ä½¿ç”¨åˆ†æå¯¹æ¯” ===")

    # æµ‹è¯•é…ç½®
    seq_lengths = [1024, 4096, 16384, 65536, 262144]
    hidden_dim = 2048
    num_heads = 32
    head_dim = hidden_dim // num_heads
    window_size = 2048

    print(f"æ¨¡å‹é…ç½®: hidden_dim={hidden_dim}, window_size={window_size}")
    print()
    print("åºåˆ—é•¿åº¦\tä¼ ç»Ÿæ–¹æ³•\t\tæµå¼æ–¹æ³•\t\tèŠ‚çœæ¯”ä¾‹")
    print("-" * 70)

    for seq_len in seq_lengths:
        # ä¼ ç»Ÿæ–¹æ³•å†…å­˜è®¡ç®—
        # KVç¼“å­˜: seq_len * hidden_dim * 2 * 2 bytes (FP16)
        kv_memory_traditional = seq_len * hidden_dim * 2 * 2
        # AttentionçŸ©é˜µ: seq_len * seq_len * 2 bytes
        attn_memory_traditional = seq_len * seq_len * 2
        total_traditional = kv_memory_traditional + attn_memory_traditional

        # æµå¼æ–¹æ³•å†…å­˜è®¡ç®—
        # åªéœ€è¦çª—å£å¤§å°çš„KVç¼“å­˜
        kv_memory_streaming = window_size * hidden_dim * 2 * 2
        # AttentionçŸ©é˜µåªéœ€è¦çª—å£å¤§å°
        attn_memory_streaming = window_size * window_size * 2
        total_streaming = kv_memory_streaming + attn_memory_streaming

        # è®¡ç®—èŠ‚çœæ¯”ä¾‹
        savings_ratio = (total_traditional - total_streaming) / total_traditional

        print(f"{seq_len:8d}\t{total_traditional/1024**2:10.1f}MB\t\t"
              f"{total_streaming/1024**2:10.1f}MB\t\t{savings_ratio*100:8.1f}%")

    print()
    print("ç»“è®º:")
    print("- æµå¼æ–¹æ³•çš„å†…å­˜ä½¿ç”¨ä¿æŒæ’å®š")
    print("- åºåˆ—è¶Šé•¿ï¼ŒèŠ‚çœæ•ˆæœè¶Šæ˜æ˜¾")
    print("- 262Ké•¿åº¦åºåˆ—å¯èŠ‚çœ99.9%çš„å†…å­˜")

    # å¯è§†åŒ–å†…å­˜ä½¿ç”¨å¯¹æ¯”
    plt.figure(figsize=(12, 6))

    traditional_memory = []
    streaming_memory = []

    for seq_len in seq_lengths:
        # è®¡ç®—å†…å­˜ä½¿ç”¨
        traditional_mem = seq_len * hidden_dim * 2 * 2 + seq_len * seq_len * 2
        streaming_mem = window_size * hidden_dim * 2 * 2 + window_size * window_size * 2

        traditional_memory.append(traditional_mem / 1024**2)  # MB
        streaming_memory.append(streaming_mem / 1024**2)     # MB

    plt.subplot(1, 2, 1)
    plt.plot(seq_lengths, traditional_memory, 'r-', label='ä¼ ç»Ÿæ–¹æ³•', linewidth=3)
    plt.plot(seq_lengths, streaming_memory, 'g-', label='æµå¼æ–¹æ³•', linewidth=3)
    plt.xlabel('åºåˆ—é•¿åº¦')
    plt.ylabel('å†…å­˜ä½¿ç”¨ (MB)')
    plt.title('å†…å­˜ä½¿ç”¨å¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')

    plt.subplot(1, 2, 2)
    savings = [(t - s) / t * 100 for t, s in zip(traditional_memory, streaming_memory)]
    plt.plot(seq_lengths, savings, 'b-', linewidth=3)
    plt.xlabel('åºåˆ—é•¿åº¦')
    plt.ylabel('å†…å­˜èŠ‚çœæ¯”ä¾‹ (%)')
    plt.title('å†…å­˜èŠ‚çœæ•ˆæœ')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

memory_usage_analysis()
```

### è®¡ç®—æ•ˆç‡åˆ†æ

```python
def computational_efficiency_analysis():
    """åˆ†æè®¡ç®—æ•ˆç‡"""

    print("=== è®¡ç®—æ•ˆç‡åˆ†æ ===")

    # æµ‹è¯•é…ç½®
    seq_lengths = [512, 1024, 2048, 4096]
    batch_size = 1
    num_heads = 16
    head_dim = 64

    print("åºåˆ—é•¿åº¦\tä¼ ç»Ÿæ—¶é—´\tæµå¼æ—¶é—´\tåŠ é€Ÿæ¯”\t\tFLOPså‡å°‘")
    print("-" * 70)

    for seq_len in seq_lengths:
        # æ¨¡æ‹Ÿæ•°æ®
        q = torch.randn(batch_size, num_heads, seq_len, head_dim)
        k = torch.randn(batch_size, num_heads, seq_len, head_dim)
        v = torch.randn(batch_size, num_heads, seq_len, head_dim)

        # ä¼ ç»Ÿæ–¹æ³•æ—¶é—´
        num_runs = 10
        start_time = time.time()
        for _ in range(num_runs):
            # æ¨¡æ‹Ÿä¼ ç»ŸAttentionè®¡ç®—
            scores = torch.matmul(q, k.transpose(-2, -1))
            attn_weights = F.softmax(scores, dim=-1)
            output = torch.matmul(attn_weights, v)
        traditional_time = (time.time() - start_time) / num_runs

        # æµå¼æ–¹æ³•æ—¶é—´ï¼ˆä½¿ç”¨çª—å£ï¼‰
        window_size = min(512, seq_len)
        q_window = q[:, :, -window_size:, :]
        k_window = k[:, :, -window_size:, :]
        v_window = v[:, :, -window_size:, :]

        start_time = time.time()
        for _ in range(num_runs):
            # æ¨¡æ‹Ÿæµå¼Attentionè®¡ç®—
            scores = torch.matmul(q_window, k_window.transpose(-2, -1))
            attn_weights = F.softmax(scores, dim=-1)
            output = torch.matmul(attn_weights, v_window)
        streaming_time = (time.time() - start_time) / num_runs

        # è®¡ç®—åŠ é€Ÿæ¯”
        speedup = traditional_time / streaming_time

        # è®¡ç®—FLOPså‡å°‘
        traditional_flops = seq_len * seq_len * head_dim  # ç®€åŒ–çš„FLOPsè®¡ç®—
        streaming_flops = window_size * window_size * head_dim
        flops_reduction = (traditional_flops - streaming_flops) / traditional_flops

        print(f"{seq_len:8d}\t{traditional_time*1000:8.2f}ms\t\t"
              f"{streaming_time*1000:8.2f}ms\t\t{speedup:8.2f}x\t\t{flops_reduction*100:8.1f}%")

    print()
    print("æ•ˆç‡åˆ†æ:")
    print("- çŸ­åºåˆ—: æµå¼æ–¹æ³•ä¼˜åŠ¿æœ‰é™")
    print("- é•¿åºåˆ—: æ˜¾è‘—çš„è®¡ç®—åŠ é€Ÿ")
    print("- FLOPså‡å°‘ä¸å†…å­˜èŠ‚çœæˆæ­£æ¯”")

computational_efficiency_analysis()
```

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯

### 1. é•¿æ–‡æ¡£å¤„ç†

```python
def long_document_processing_demo():
    """æ¼”ç¤ºé•¿æ–‡æ¡£å¤„ç†åœºæ™¯"""

    print("=== é•¿æ–‡æ¡£å¤„ç†æ¼”ç¤º ===")

    # æ¨¡æ‹Ÿé•¿æ–‡æ¡£
    document_length = 50000  # 50K tokens
    chunk_size = 1024
    window_size = 4096

    print(f"æ–‡æ¡£é•¿åº¦: {document_length} tokens")
    print(f"åˆ†å—å¤§å°: {chunk_size} tokens")
    print(f"ä¸Šä¸‹æ–‡çª—å£: {window_size} tokens")
    print()

    # åˆ›å»ºæµå¼å¤„ç†æ¨¡å‹
    model = StreamingAttention(
        d_model=1024,
        num_heads=16,
        window_size=window_size,
        enable_kv_cache=True,
        enable_sliding_window=True
    )

    # å¤„ç†æ–‡æ¡£
    total_time = 0
    peak_memory = 0

    for chunk_idx in range(0, document_length, chunk_size):
        end_pos = min(chunk_idx + chunk_size, document_length)
        current_chunk_size = end_pos - chunk_idx

        # ç”Ÿæˆæ–‡æ¡£å—
        x = torch.randn(1, current_chunk_size, 1024)

        # å¤„ç†
        start_time = time.time()
        with torch.no_grad():
            output, _ = model(x, use_cache=True)
        processing_time = time.time() - start_time

        total_time += processing_time

        # æ¨¡æ‹Ÿå†…å­˜ç›‘æ§
        current_memory = window_size * 1024 * 4 * 4 / (1024**2)  # MB
        peak_memory = max(peak_memory, current_memory)

        progress = (end_pos / document_length) * 100
        print(f"è¿›åº¦: {progress:5.1f}% | "
              f"å— {chunk_idx//chunk_size + 1:3d} | "
              f"æ—¶é—´: {processing_time*1000:6.2f}ms | "
              f"å†…å­˜: {current_memory:6.1f}MB")

    print()
    print("å¤„ç†å®Œæˆï¼")
    print(f"æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
    print(f"å³°å€¼å†…å­˜ä½¿ç”¨: {peak_memory:.1f}MB")
    print(f"å¹³å‡ååé‡: {document_length/total_time:.1f} tokens/ç§’")

    # ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”
    traditional_memory = document_length * 1024 * 2 * 4 / (1024**3)  # GB
    print(f"\nä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”:")
    print(f"ä¼ ç»Ÿæ–¹æ³•å†…å­˜éœ€æ±‚: {traditional_memory:.2f}GB")
    print(f"æµå¼æ–¹æ³•å†…å­˜éœ€æ±‚: {peak_memory/1024:.2f}GB")
    print(f"å†…å­˜èŠ‚çœ: {(1 - peak_memory/1024/traditional_memory)*100:.1f}%")

long_document_processing_demo()
```

### 2. å®æ—¶æµå¼æ¨ç†

```python
class RealTimeStreamingInference:
    """å®æ—¶æµå¼æ¨ç†å¼•æ“"""

    def __init__(self, model_config, streaming_config):
        self.model_config = model_config
        self.streaming_config = streaming_config

        # åˆå§‹åŒ–æ¨¡å‹
        self.attention = StreamingAttention(
            d_model=model_config['d_model'],
            num_heads=model_config['num_heads'],
            window_size=streaming_config['window_size'],
            enable_kv_cache=True,
            enable_sliding_window=True
        )

        # æ¨ç†çŠ¶æ€
        self.inference_state = {
            'total_tokens': 0,
            'processing_times': [],
            'cache_utilization': []
        }

    def process_stream(self, token_stream):
        """å¤„ç†tokenæµ"""
        results = []

        for token_batch in token_stream:
            # å¤„ç†å½“å‰batch
            start_time = time.time()

            with torch.no_grad():
                output, _ = self.attention(token_batch, use_cache=True)

            processing_time = time.time() - start_time

            # æ›´æ–°çŠ¶æ€
            batch_size = token_batch.shape[1]
            self.inference_state['total_tokens'] += batch_size
            self.inference_state['processing_times'].append(processing_time)

            # è·å–ç¼“å­˜ä¿¡æ¯
            cache_info = self.attention.get_cache_info()
            if cache_info:
                self.inference_state['cache_utilization'].append(cache_info['utilization'])

            results.append({
                'output': output,
                'tokens_processed': batch_size,
                'processing_time': processing_time,
                'cache_utilization': cache_info['utilization'] if cache_info else 0
            })

        return results

    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        processing_times = self.inference_state['processing_times']
        cache_utilizations = self.inference_state['cache_utilization']

        if not processing_times:
            return {}

        return {
            'total_tokens': self.inference_state['total_tokens'],
            'avg_processing_time': np.mean(processing_times),
            'max_processing_time': np.max(processing_times),
            'min_processing_time': np.min(processing_times),
            'avg_throughput': self.inference_state['total_tokens'] / sum(processing_times),
            'avg_cache_utilization': np.mean(cache_utilizations) if cache_utilizations else 0,
            'total_processing_time': sum(processing_times)
        }

# å®æ—¶æµå¼æ¨ç†æ¼”ç¤º
def real_time_inference_demo():
    """æ¼”ç¤ºå®æ—¶æµå¼æ¨ç†"""

    print("=== å®æ—¶æµå¼æ¨ç†æ¼”ç¤º ===")

    # é…ç½®
    model_config = {
        'd_model': 768,
        'num_heads': 12
    }

    streaming_config = {
        'window_size': 2048
    }

    # åˆ›å»ºæ¨ç†å¼•æ“
    engine = RealTimeStreamingInference(model_config, streaming_config)

    # æ¨¡æ‹Ÿå®æ—¶tokenæµ
    def simulate_token_stream(total_tokens=10000, batch_size=32):
        """æ¨¡æ‹Ÿtokenæµ"""
        for _ in range(0, total_tokens, batch_size):
            current_batch_size = min(batch_size, total_tokens - _)
            yield torch.randn(1, current_batch_size, model_config['d_model'])

    # å¤„ç†æµ
    results = engine.process_stream(simulate_token_stream())

    # åˆ†ææ€§èƒ½
    stats = engine.get_performance_stats()

    print("æ¨ç†æ€§èƒ½ç»Ÿè®¡:")
    print(f"æ€»å¤„ç†tokens: {stats['total_tokens']}")
    print(f"å¹³å‡å¤„ç†æ—¶é—´: {stats['avg_processing_time']*1000:.2f}ms")
    print(f"æœ€å¤§å¤„ç†æ—¶é—´: {stats['max_processing_time']*1000:.2f}ms")
    print(f"æœ€å°å¤„ç†æ—¶é—´: {stats['min_processing_time']*1000:.2f}ms")
    print(f"å¹³å‡ååé‡: {stats['avg_throughput']:.1f} tokens/ç§’")
    print(f"å¹³å‡ç¼“å­˜åˆ©ç”¨ç‡: {stats['avg_cache_utilization']:.2f}")
    print(f"æ€»å¤„ç†æ—¶é—´: {stats['total_processing_time']:.2f}ç§’")

    # å¯è§†åŒ–æ€§èƒ½
    processing_times = [r['processing_time']*1000 for r in results]
    cache_utils = [r['cache_utilization'] for r in results]

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

    # å¤„ç†æ—¶é—´å˜åŒ–
    ax1.plot(processing_times, 'b-', alpha=0.7)
    ax1.set_xlabel('å¤„ç†æ‰¹æ¬¡')
    ax1.set_ylabel('å¤„ç†æ—¶é—´ (ms)')
    ax1.set_title('å®æ—¶å¤„ç†æ—¶é—´å˜åŒ–')
    ax1.grid(True, alpha=0.3)

    # ç¼“å­˜åˆ©ç”¨ç‡å˜åŒ–
    ax2.plot(cache_utils, 'r-', alpha=0.7)
    ax2.set_xlabel('å¤„ç†æ‰¹æ¬¡')
    ax2.set_ylabel('ç¼“å­˜åˆ©ç”¨ç‡')
    ax2.set_title('ç¼“å­˜åˆ©ç”¨ç‡å˜åŒ–')
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(0, 1)

    plt.tight_layout()
    plt.show()

real_time_inference_demo()
```

## ğŸ¯ æ€»ç»“ä¸æœ€ä½³å®è·µ

### æ ¸å¿ƒæŠ€æœ¯æ€»ç»“

é€šè¿‡æœ¬æ–‡çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å…¨é¢æŒæ¡äº†æµå¼Attentionçš„æ ¸å¿ƒæŠ€æœ¯ï¼š

1. **å¾ªç¯ç¼“å†²åŒº**ï¼šæ’å®šå†…å­˜ä½¿ç”¨çš„å…³é”®æŠ€æœ¯
2. **æ»‘åŠ¨çª—å£**ï¼šæ™ºèƒ½çš„ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥
3. **KVç¼“å­˜å¤ç”¨**ï¼šé¿å…é‡å¤è®¡ç®—çš„å†å²ä¿¡æ¯
4. **æµå¼æ¶æ„**ï¼šæ”¯æŒæ— é™é•¿åºåˆ—çš„å¤„ç†æ¡†æ¶

### æ€§èƒ½æå‡æ•ˆæœ

**å†…å­˜æ•ˆç‡**ï¼š
- **99%+**çš„å†…å­˜èŠ‚çœï¼ˆé•¿åºåˆ—åœºæ™¯ï¼‰
- **æ’å®š**çš„å†…å­˜ä½¿ç”¨ï¼Œä¸éšåºåˆ—é•¿åº¦å¢é•¿
- **å®æ—¶**çš„å†…å­˜ç›‘æ§å’Œè‡ªé€‚åº”è°ƒæ•´

**è®¡ç®—æ•ˆç‡**ï¼š
- **2-10å€**çš„è®¡ç®—åŠ é€Ÿï¼ˆé•¿åºåˆ—åœºæ™¯ï¼‰
- **çº¿æ€§**çš„æ—¶é—´å¤æ‚åº¦å¢é•¿
- **å¯é¢„æµ‹**çš„å¤„ç†å»¶è¿Ÿ

### å®è·µå»ºè®®

**åº”ç”¨åœºæ™¯é€‰æ‹©**ï¼š
- **é•¿æ–‡æ¡£å¤„ç†**ï¼šä½¿ç”¨å¤§çª—å£ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
- **å®æ—¶æ¨ç†**ï¼šä½¿ç”¨å°çª—å£ä¼˜åŒ–å»¶è¿Ÿ
- **è¯­éŸ³è¯†åˆ«**ï¼šä½¿ç”¨åŠ¨æ€çª—å£é€‚åº”ä¸åŒè¯­é€Ÿ
- **è§†é¢‘åˆ†æ**ï¼šç»“åˆæ—¶åºä¿¡æ¯ä¼˜åŒ–çª—å£ç­–ç•¥

**å‚æ•°è°ƒä¼˜æŒ‡å—**ï¼š
- **çª—å£å¤§å°**ï¼šé€šå¸¸è®¾ç½®ä¸º512-4096ï¼Œæ ¹æ®ä»»åŠ¡éœ€æ±‚è°ƒæ•´
- **åˆ†å—å¤§å°**ï¼šå¹³è¡¡å»¶è¿Ÿå’Œååé‡ï¼Œé€šå¸¸ä¸º128-512
- **ç¼“å­˜ç­–ç•¥**ï¼šæ ¹æ®å†…å­˜é™åˆ¶å’Œæ€§èƒ½éœ€æ±‚é€‰æ‹©
- **æ»‘åŠ¨ç­–ç•¥**ï¼šå›ºå®šçª—å£ç®€å•é«˜æ•ˆï¼ŒåŠ¨æ€çª—å£é€‚åº”æ€§å¼º

### æœªæ¥å‘å±•æ–¹å‘

1. **è‡ªé€‚åº”çª—å£**ï¼šåŸºäºå†…å®¹é‡è¦æ€§åŠ¨æ€è°ƒæ•´
2. **å¤šçº§ç¼“å­˜**ï¼šåˆ†å±‚å­˜å‚¨ä¸åŒé‡è¦æ€§çš„ä¿¡æ¯
3. **åˆ†å¸ƒå¼æµå¼**ï¼šè·¨è®¾å¤‡çš„ååŒå¤„ç†
4. **ç¡¬ä»¶ååŒ**ï¼šä¸“ç”¨èŠ¯ç‰‡çš„æµå¼è®¡ç®—ä¼˜åŒ–

---

**è®°ä½**ï¼šæµå¼Attentionä¸ä»…æ˜¯ä¸€é¡¹æŠ€æœ¯ä¼˜åŒ–ï¼Œæ›´æ˜¯å¤„ç†æ— é™åºåˆ—ä¿¡æ¯çš„æ ¹æœ¬è§£å†³æ–¹æ¡ˆã€‚æŒæ¡äº†æµå¼å¤„ç†ï¼Œå°±æ‰“å¼€äº†é€šå¾€çœŸæ­£å¤§è§„æ¨¡AIåº”ç”¨çš„å¤§é—¨ã€‚

*ä¸‹ä¸€ç¯‡æ–‡ç« å°†æ·±å…¥è§£æAttentionçš„å„ç§å˜ä½“ï¼Œä»Multi-Headåˆ°MQAã€GQAï¼Œäº†è§£AttentionæŠ€æœ¯çš„æ¼”è¿›å’Œåˆ›æ–°ã€‚* ğŸš€