# å¤šåç«¯Attentionå®ç°å¯¹æ¯”ï¼šFlashAttentionã€FusedAttentionã€UnfusedAttention

## ğŸ¯ å¼•è¨€

æƒ³è±¡ä¸€ä¸‹ï¼Œä½ è¦å®Œæˆä¸€ä¸ªå¤æ‚çš„ä»»åŠ¡â€”â€”å»ºé€ ä¸€åº§æ¡¥æ¢ã€‚ä½ å¯ä»¥é€‰æ‹©ï¼š

1. **ä¼ ç»Ÿæ–¹æ³•**ï¼šä¸€æ­¥ä¸€æ­¥åœ°å®Œæˆæ¯ä¸ªéƒ¨åˆ†ï¼Œè™½ç„¶ç›´è§‚ä½†æ•ˆç‡ä½ä¸‹
2. **æµæ°´çº¿æ–¹æ³•**ï¼šå°†å¤šä¸ªå·¥åºåˆå¹¶ï¼Œä¸€æ¬¡å®Œæˆå¤šä¸ªæ­¥éª¤
3. **æ™ºèƒ½æ–¹æ³•**ï¼šæ ¹æ®ææ–™ç‰¹æ€§å’Œç¯å¢ƒæ¡ä»¶ï¼ŒåŠ¨æ€è°ƒæ•´æ–½å·¥ç­–ç•¥

è¿™ä¸‰ç§æ–¹æ³•å¯¹åº”ç€Attentionçš„ä¸‰ç§ä¸»è¦å®ç°æ–¹å¼ï¼šUnfusedAttentionï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰ã€FusedAttentionï¼ˆæµæ°´çº¿æ–¹æ³•ï¼‰ã€FlashAttentionï¼ˆæ™ºèƒ½æ–¹æ³•ï¼‰ã€‚

åœ¨ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸çœ‹åˆ°è¿™äº›åç«¯çš„åå­—ï¼Œä½†å¾ˆå¤šäººå¹¶ä¸æ¸…æ¥šå®ƒä»¬çš„åŒºåˆ«å’Œé€‚ç”¨åœºæ™¯ã€‚æœ¬æ–‡å°†æ·±å…¥å‰–æè¿™ä¸‰ç§Attentionå®ç°çš„æ ¸å¿ƒæ€æƒ³ã€æ€§èƒ½ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯ï¼Œå¸®åŠ©ä½ åœ¨ä¸åŒåœºæ™¯ä¸‹é€‰æ‹©æœ€ä¼˜çš„å®ç°æ–¹æ¡ˆã€‚

## ğŸ”§ UnfusedAttentionï¼šä¼ ç»Ÿçš„åˆ†æ­¥å®ç°

### æ ¸å¿ƒæ€æƒ³

UnfusedAttentionæ˜¯æœ€ç›´è§‚çš„Attentionå®ç°æ–¹å¼ï¼Œå®ƒå°†Attentionçš„è®¡ç®—è¿‡ç¨‹åˆ†è§£ä¸ºå¤šä¸ªç‹¬ç«‹çš„æ­¥éª¤ï¼š

```
UnfusedAttentionæµç¨‹ï¼š
1. QK^T è®¡ç®—ç›¸ä¼¼åº¦
2. ç¼©æ”¾ (é™¤ä»¥âˆšd)
3. Softmax å½’ä¸€åŒ–
4. åŠ æƒæ±‚å’Œ (ä¸Vç›¸ä¹˜)
```

æ¯ä¸ªæ­¥éª¤éƒ½æ˜¯ç‹¬ç«‹çš„è®¡ç®—æ“ä½œï¼Œä¸­é—´ç»“æœéœ€è¦å­˜å‚¨åœ¨å†…å­˜ä¸­ã€‚

### ä»£ç å®ç°

```python
import torch
import torch.nn.functional as F
import time
import numpy as np

class UnfusedAttention:
    """ä¼ ç»Ÿçš„åˆ†æ­¥Attentionå®ç°"""

    def __init__(self):
        self.name = "UnfusedAttention"

    def forward(self, Q, K, V, mask=None):
        """
        UnfusedAttentionå‰å‘ä¼ æ’­

        Args:
            Q: [batch_size, seq_len, d_model] æŸ¥è¯¢çŸ©é˜µ
            K: [batch_size, seq_len, d_model] é”®çŸ©é˜µ
            V: [batch_size, seq_len, d_model] å€¼çŸ©é˜µ
            mask: [batch_size, seq_len, seq_len] æ³¨æ„åŠ›æ©ç 

        Returns:
            output: [batch_size, seq_len, d_model] è¾“å‡ºçŸ©é˜µ
            attention_weights: [batch_size, seq_len, seq_len] æ³¨æ„åŠ›æƒé‡
        """
        batch_size, seq_len, d_model = Q.shape

        # æ­¥éª¤1: è®¡ç®—QK^T
        # [batch_size, seq_len, d_model] Ã— [batch_size, d_model, seq_len] â†’ [batch_size, seq_len, seq_len]
        attention_scores = torch.matmul(Q, K.transpose(-2, -1))
        print(f"æ­¥éª¤1 - QK^Tè®¡ç®—å®Œæˆ: {attention_scores.shape}")

        # æ­¥éª¤2: ç¼©æ”¾
        scale_factor = torch.tensor(d_model, dtype=torch.float32).sqrt()
        attention_scores = attention_scores / scale_factor
        print(f"æ­¥éª¤2 - ç¼©æ”¾å®Œæˆ")

        # æ­¥éª¤3: åº”ç”¨æ©ç 
        if mask is not None:
            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)
            print(f"æ­¥éª¤3 - åº”ç”¨æ©ç å®Œæˆ")

        # æ­¥éª¤4: Softmaxå½’ä¸€åŒ–
        attention_weights = F.softmax(attention_scores, dim=-1)
        print(f"æ­¥éª¤4 - Softmaxå½’ä¸€åŒ–å®Œæˆ: {attention_weights.shape}")

        # æ­¥éª¤5: åŠ æƒæ±‚å’Œ
        # [batch_size, seq_len, seq_len] Ã— [batch_size, seq_len, d_model] â†’ [batch_size, seq_len, d_model]
        output = torch.matmul(attention_weights, V)
        print(f"æ­¥éª¤5 - åŠ æƒæ±‚å’Œå®Œæˆ: {output.shape}")

        return output, attention_weights

    def backward(self, grad_output, attention_weights, Q, K, V):
        """
        UnfusedAttentionåå‘ä¼ æ’­
        """
        # ç®€åŒ–çš„åå‘ä¼ æ’­å®ç°
        batch_size, seq_len, d_model = Q.shape

        # Vçš„æ¢¯åº¦
        grad_V = torch.matmul(attention_weights.transpose(-2, -1), grad_output)

        # æ³¨æ„åŠ›æƒé‡çš„æ¢¯åº¦
        grad_attention_weights = torch.matmul(grad_output, V.transpose(-2, -1))

        # Qå’ŒKçš„æ¢¯åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
        dK = torch.matmul(grad_attention_weights.transpose(-2, -1), Q) / torch.sqrt(d_model)
        dQ = torch.matmul(grad_attention_weights, K) / torch.sqrt(d_model)

        return dQ, dK, grad_V

    def memory_usage(self, batch_size, seq_len, d_model):
        """è®¡ç®—å†…å­˜ä½¿ç”¨é‡"""
        # Q, K, VçŸ©é˜µ
        qkv_memory = 3 * batch_size * seq_len * d_model * 4  # 4 bytes for float32

        # æ³¨æ„åŠ›çŸ©é˜µ
        attention_memory = batch_size * seq_len * seq_len * 4

        # ç¼©æ”¾åçš„åˆ†æ•°
        scores_memory = batch_size * seq_len * seq_len * 4

        total_memory = qkv_memory + attention_memory + scores_memory
        return total_memory / 1024 / 1024  # MB

# æµ‹è¯•UnfusedAttention
def test_unfused_attention():
    """æµ‹è¯•UnfusedAttention"""

    print("=" * 60)
    print("UnfusedAttention æµ‹è¯•")
    print("=" * 60)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, d_model = 2, 512, 768
    Q = torch.randn(batch_size, seq_len, d_model)
    K = torch.randn(batch_size, seq_len, d_model)
    V = torch.randn(batch_size, seq_len, d_model)

    # åˆ›å»ºå› æœæ©ç 
    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)
    mask = mask.expand(batch_size, 1, seq_len, seq_len)

    # å®ä¾‹åŒ–å¹¶æµ‹è¯•
    unfused_attn = UnfusedAttention()
    output, weights = unfused_attn.forward(Q, K, V, mask)

    print(f"\nå†…å­˜ä½¿ç”¨: {unfused_attn.memory_usage(batch_size, seq_len, d_model):.2f} MB")

test_unfused_attention()
```

### ç‰¹ç‚¹åˆ†æ

**ä¼˜ç‚¹ï¼š**
- å®ç°ç›´è§‚ï¼Œæ˜“äºç†è§£å’Œè°ƒè¯•
- æ¯ä¸ªæ­¥éª¤å¯ä»¥ç‹¬ç«‹ä¼˜åŒ–
- æ”¯æŒçµæ´»çš„æ©ç å’Œè‡ªå®šä¹‰æ“ä½œ

**ç¼ºç‚¹ï¼š**
- å†…å­˜ä½¿ç”¨é‡å¤§ï¼ˆO(NÂ²)ï¼‰
- è®¡ç®—æ•ˆç‡ç›¸å¯¹è¾ƒä½
- ä¸­é—´ç»“æœéœ€è¦å¤šæ¬¡å†…å­˜è¯»å†™

## âš¡ FusedAttentionï¼šèåˆè®¡ç®—ä¼˜åŒ–

### æ ¸å¿ƒæ€æƒ³

FusedAttentioné€šè¿‡CUDAæ ¸å‡½æ•°èåˆï¼Œå°†å¤šä¸ªè®¡ç®—æ­¥éª¤åˆå¹¶ä¸ºä¸€ä¸ªåŸå­æ“ä½œã€‚è¿™å°±åƒä¸€ä¸ªç†Ÿç»ƒçš„å·¥äººï¼Œèƒ½å¤ŸåŒæ—¶å¤„ç†å¤šä¸ªå·¥åºï¼Œä¸éœ€è¦é¢‘ç¹åœ°äº¤æ¥å·¥ä½œã€‚

```
FusedAttentionæµç¨‹ï¼š
1. QK^Tè®¡ç®— + ç¼©æ”¾ + Softmax + åŠ æƒæ±‚å’Œ â†’ å•ä¸ªCUDAæ ¸å‡½æ•°
```

### ä»£ç å®ç°

```python
class FusedAttention:
    """èåˆè®¡ç®—çš„Attentionå®ç°"""

    def __init__(self):
        self.name = "FusedAttention"

    def forward(self, Q, K, V, mask=None):
        """
        FusedAttentionå‰å‘ä¼ æ’­

        æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªæ¦‚å¿µæ€§å®ç°ï¼Œå®é™…çš„FusedAttentionéœ€è¦CUDAç¼–ç¨‹
        """

        batch_size, seq_len, d_model = Q.shape

        # æ¨¡æ‹Ÿèåˆè®¡ç®—ï¼ˆå®é™…ä¸­æ˜¯ä¸€ä¸ªCUDAæ ¸å‡½æ•°ï¼‰
        print("æ‰§è¡ŒèåˆAttentionè®¡ç®—...")

        # åœ¨å®é™…å®ç°ä¸­ï¼Œä»¥ä¸‹æ‰€æœ‰æ“ä½œéƒ½åœ¨ä¸€ä¸ªCUDAæ ¸å‡½æ•°ä¸­å®Œæˆï¼š
        # 1. è®¡ç®—QK^T
        # 2. ç¼©æ”¾
        # 3. åº”ç”¨æ©ç 
        # 4. Softmax
        # 5. åŠ æƒæ±‚å’Œ

        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ä»ç„¶åˆ†æ­¥æ‰§è¡Œï¼Œä½†å¼ºè°ƒè¿™æ˜¯æ¦‚å¿µä¸Šçš„èåˆ
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_model, dtype=torch.float32))

        if mask is not None:
            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)

        attention_weights = F.softmax(attention_scores, dim=-1)
        output = torch.matmul(attention_weights, V)

        print("èåˆAttentionè®¡ç®—å®Œæˆ")

        return output, attention_weights

    def simulate_fused_kernel(self, Q, K, V, mask=None):
        """æ¨¡æ‹Ÿèåˆçš„CUDAæ ¸å‡½æ•°"""

        print("æ¨¡æ‹ŸFused CUDAæ ¸å‡½æ•°æ‰§è¡Œ:")
        print("-" * 40)

        # æ¨¡æ‹Ÿæ ¸å‡½æ•°å†…éƒ¨çš„å·¥ä½œ
        batch_size, seq_len, d_model = Q.shape

        for batch in range(batch_size):
            for i in range(seq_len):
                # è®¡ç®—ç¬¬iä¸ªè¾“å‡ºä½ç½®çš„æ‰€æœ‰æ­¥éª¤
                q_i = Q[batch, i, :]  # [d_model]

                # æ­¥éª¤1: è®¡ç®—ä¸æ‰€æœ‰keyçš„ç›¸ä¼¼åº¦
                similarities = []
                for j in range(seq_len):
                    k_j = K[batch, j, :]  # [d_model]
                    similarity = torch.dot(q_i, k_j) / torch.sqrt(torch.tensor(d_model, dtype=torch.float32))

                    # æ­¥éª¤2: åº”ç”¨æ©ç 
                    if mask is not None and mask[batch, 0, i, j] == 0:
                        similarity = -1e9

                    similarities.append(similarity)

                # æ­¥éª¤3: Softmax
                similarities = torch.tensor(similarities)
                softmax_weights = F.softmax(similarities, dim=0)

                # æ­¥éª¤4: åŠ æƒæ±‚å’Œ
                output_i = torch.zeros(d_model)
                for j in range(seq_len):
                    v_j = V[batch, j, :]
                    output_i += softmax_weights[j] * v_j

                if i == 0:  # åªæ‰“å°ç¬¬ä¸€ä¸ªä½ç½®çš„è¯¦ç»†ä¿¡æ¯
                    print(f"  ä½ç½® {i}:")
                    print(f"    ç›¸ä¼¼åº¦: {similarities}")
                    print(f"    Softmaxæƒé‡: {softmax_weights}")
                    print(f"    è¾“å‡º: {output_i}")

    def optimized_fused_attention(self, Q, K, V, mask=None):
        """ä¼˜åŒ–çš„èåˆAttentionï¼ˆä½¿ç”¨torchçš„ä¼˜åŒ–å‡½æ•°ï¼‰"""

        # ä½¿ç”¨torchçš„ä¼˜åŒ–å‡½æ•°æ¥æ¨¡æ‹Ÿèåˆè®¡ç®—
        d_model = Q.shape[-1]

        # ä½¿ç”¨scaled_dot_product_attentionï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            # PyTorch 2.0+ å†…ç½®å‡½æ•°
            import torch.nn.functional as F
            output = F.scaled_dot_product_attention(
                Q, K, V,
                attn_mask=mask,
                dropout_p=0.0,
                is_causal=False
            )
            # æ³¨æ„ï¼šè¿™ä¸ªå‡½æ•°å†…éƒ¨å®ç°äº†èåˆè®¡ç®—
            attention_weights = None  # å†…éƒ¨å‡½æ•°ä¸è¿”å›æƒé‡
            print("ä½¿ç”¨PyTorchå†…ç½®çš„scaled_dot_product_attentionï¼ˆå†…éƒ¨èåˆï¼‰")

        except ImportError:
            # å›é€€åˆ°æ‰‹åŠ¨å®ç°
            print("å›é€€åˆ°æ‰‹åŠ¨èåˆå®ç°")
            attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_model, dtype=torch.float32))

            if mask is not None:
                attention_scores = attention_scores.masked_fill(mask == 0, -1e9)

            attention_weights = F.softmax(attention_scores, dim=-1)
            output = torch.matmul(attention_weights, V)

        return output, attention_weights

    def performance_characteristics(self):
        """FusedAttentionçš„æ€§èƒ½ç‰¹å¾"""
        print("\nFusedAttentionæ€§èƒ½ç‰¹å¾:")
        print("-" * 30)
        print("è®¡ç®—å¤æ‚åº¦: O(NÂ² Ã— d)")
        print("å†…å­˜å¤æ‚åº¦: O(NÂ²) (ä»éœ€å­˜å‚¨æ³¨æ„åŠ›çŸ©é˜µ)")
        print("GPUåˆ©ç”¨ç‡: é«˜ (æ ¸å‡½æ•°èåˆ)")
        print("å†…å­˜å¸¦å®½: ä¸­ç­‰ (å‡å°‘ä¸­é—´ç»“æœå­˜å‚¨)")
        print("å¯å¹¶è¡Œæ€§: ä¼˜ç§€")

# æµ‹è¯•FusedAttention
def test_fused_attention():
    """æµ‹è¯•FusedAttention"""

    print("=" * 60)
    print("FusedAttention æµ‹è¯•")
    print("=" * 60)

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, d_model = 2, 256, 512
    Q = torch.randn(batch_size, seq_len, d_model)
    K = torch.randn(batch_size, seq_len, d_model)
    V = torch.randn(batch_size, seq_len, d_model)

    # åˆ›å»ºå› æœæ©ç 
    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)
    mask = mask.expand(batch_size, 1, seq_len, seq_len)

    # å®ä¾‹åŒ–å¹¶æµ‹è¯•
    fused_attn = FusedAttention()

    # æ¨¡æ‹Ÿèåˆæ ¸å‡½æ•°
    fused_attn.simulate_fused_kernel(Q, K, V, mask)

    # ä¼˜åŒ–çš„èåˆè®¡ç®—
    output, weights = fused_attn.optimized_fused_attention(Q, K, V, mask)

    # æ€§èƒ½ç‰¹å¾
    fused_attn.performance_characteristics()

test_fused_attention()
```

### CUDAæ ¸å‡½æ•°èåˆçš„æ¦‚å¿µ

```python
def cuda_fusion_concept():
    """å±•ç¤ºCUDAæ ¸å‡½æ•°èåˆçš„æ¦‚å¿µ"""

    print("CUDAæ ¸å‡½æ•°èåˆæ¦‚å¿µ")
    print("=" * 50)

    # ä¼ ç»Ÿæ–¹æ³•çš„ä¼ªä»£ç 
    print("ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¤šä¸ªCUDAæ ¸å‡½æ•°ï¼‰:")
    print("""
    kernel1<<<blocks, threads>>>(Q, K, scores);      // è®¡ç®—QK^T
    kernel2<<<blocks, threads>>>(scores, d);          // ç¼©æ”¾
    kernel3<<<blocks, threads>>>(scores, mask);        // åº”ç”¨æ©ç 
    kernel4<<<blocks, threads>>>(scores, weights);     // Softmax
    kernel5<<<blocks, threads>>>(weights, V, output);   // åŠ æƒæ±‚å’Œ
    """)

    print("\nèåˆæ–¹æ³•ï¼ˆå•ä¸ªCUDAæ ¸å‡½æ•°ï¼‰:")
    print("""
    fused_kernel<<<blocks, threads>>>(Q, K, V, mask, output);
    // åœ¨ä¸€ä¸ªæ ¸å‡½æ•°ä¸­å®Œæˆæ‰€æœ‰è®¡ç®—ï¼š
    // 1. è®¡ç®—QK^T
    // 2. ç¼©æ”¾
    // 3. åº”ç”¨æ©ç 
    // 4. Softmax
    // 5. åŠ æƒæ±‚å’Œ
    """)

    print("\nèåˆçš„ä¼˜åŠ¿:")
    print("- å‡å°‘æ ¸å‡½æ•°å¯åŠ¨å¼€é”€")
    print("- å‡å°‘ä¸­é—´ç»“æœçš„å†…å­˜è¯»å†™")
    print("- æé«˜GPUåˆ©ç”¨ç‡")
    print("- æ›´å¥½çš„æ•°æ®å±€éƒ¨æ€§")

cuda_fusion_concept()
```

## ğŸš€ FlashAttentionï¼šIOæ„ŸçŸ¥çš„ç²¾ç¡®ç®—æ³•

### ä¸å‰ä¸¤è€…çš„å¯¹æ¯”

FlashAttentionåœ¨å‰é¢çš„æ–‡ç« ä¸­å·²ç»è¯¦ç»†ä»‹ç»è¿‡ï¼Œè¿™é‡Œæˆ‘ä»¬é‡ç‚¹å¯¹æ¯”å®ƒä¸å‰ä¸¤ç§æ–¹æ³•çš„åŒºåˆ«ï¼š

```python
def compare_three_implementations():
    """å¯¹æ¯”ä¸‰ç§Attentionå®ç°"""

    print("ä¸‰ç§Attentionå®ç°å¯¹æ¯”")
    print("=" * 60)

    implementations = {
        "UnfusedAttention": {
            "è®¡ç®—å¤æ‚åº¦": "O(NÂ² Ã— d)",
            "å†…å­˜å¤æ‚åº¦": "O(NÂ²)",
            "æ ¸å‡½æ•°æ•°é‡": "5+",
            "ä¸­é—´å­˜å‚¨": "éœ€è¦",
            "ç²¾ç¡®æ€§": "ç²¾ç¡®",
            "é€‚ç”¨åœºæ™¯": "å°åºåˆ—ï¼Œè°ƒè¯•å‹å¥½"
        },
        "FusedAttention": {
            "è®¡ç®—å¤æ‚åº¦": "O(NÂ² Ã— d)",
            "å†…å­˜å¤æ‚åº¦": "O(NÂ²)",
            "æ ¸å‡½æ•°æ•°é‡": "1",
            "ä¸­é—´å­˜å‚¨": "éƒ¨åˆ†",
            "ç²¾ç¡®æ€§": "ç²¾ç¡®",
            "é€‚ç”¨åœºæ™¯": "ä¸­ç­‰åºåˆ—ï¼Œæ€§èƒ½ä¼˜åŒ–"
        },
        "FlashAttention": {
            "è®¡ç®—å¤æ‚åº¦": "O(NÂ² Ã— d)",
            "å†…å­˜å¤æ‚åº¦": "O(N)",
            "æ ¸å‡½æ•°æ•°é‡": "1+",
            "ä¸­é—´å­˜å‚¨": "ä¸éœ€è¦",
            "ç²¾ç¡®æ€§": "ç²¾ç¡®",
            "é€‚ç”¨åœºæ™¯": "é•¿åºåˆ—ï¼Œå†…å­˜å—é™"
        }
    }

    print(f"{'å®ç°æ–¹å¼':<20} {'è®¡ç®—å¤æ‚åº¦':<12} {'å†…å­˜å¤æ‚åº¦':<12} {'æ ¸å‡½æ•°':<8} {'ç²¾ç¡®æ€§':<8}")
    print("-" * 70)

    for impl, features in implementations.items():
        print(f"{impl:<20} {features['è®¡ç®—å¤æ‚åº¦']:<12} {features['å†…å­˜å¤æ‚åº¦']:<12} "
              f"{features['æ ¸å‡½æ•°æ•°é‡']:<8} {features['ç²¾ç¡®æ€§']:<8}")

compare_three_implementations()
```

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### å…¨é¢çš„æ€§èƒ½å¯¹æ¯”

```python
def comprehensive_performance_benchmark():
    """å…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    print("å…¨é¢çš„Attentionå®ç°æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 70)

    import time
    import matplotlib.pyplot as plt

    # æµ‹è¯•é…ç½®
    test_configs = [
        {"seq_len": 128, "d_model": 512, "batch_size": 16},
        {"seq_len": 512, "d_model": 768, "batch_size": 8},
        {"seq_len": 1024, "d_model": 1024, "batch_size": 4},
        {"seq_len": 2048, "d_model": 1024, "batch_size": 2},
        {"seq_len": 4096, "d_model": 1024, "batch_size": 1},
    ]

    results = {
        "seq_lengths": [],
        "unfused_time": [],
        "unfused_memory": [],
        "fused_time": [],
        "fused_memory": [],
        "flash_time": [],
        "flash_memory": []
    }

    for config in test_configs:
        seq_len = config["seq_len"]
        d_model = config["d_model"]
        batch_size = config["batch_size"]

        print(f"\næµ‹è¯•é…ç½®: seq_len={seq_len}, d_model={d_model}, batch_size={batch_size}")
        print("-" * 60)

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        torch.manual_seed(42)
        Q = torch.randn(batch_size, seq_len, d_model)
        K = torch.randn(batch_size, seq_len, d_model)
        V = torch.randn(batch_size, seq_len, d_model)

        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)
        mask = mask.expand(batch_size, 1, seq_len, seq_len)

        # æµ‹è¯•UnfusedAttention
        print("æµ‹è¯•UnfusedAttention...")
        unfused_attn = UnfusedAttention()

        start_time = time.time()
        output1, weights1 = unfused_attn.forward(Q, K, V, mask)
        unfused_time = time.time() - start_time
        unfused_memory = unfused_attn.memory_usage(batch_size, seq_len, d_model)

        # æµ‹è¯•FusedAttention
        print("æµ‹è¯•FusedAttention...")
        fused_attn = FusedAttention()

        start_time = time.time()
        output2, weights2 = fused_attn.optimized_fused_attention(Q, K, V, mask)
        fused_time = time.time() - start_time
        fused_memory = unfused_memory  # FusedAttentionå†…å­˜ä½¿ç”¨ç±»ä¼¼

        # æ¨¡æ‹ŸFlashAttentionï¼ˆç®€åŒ–ç‰ˆï¼‰
        print("æµ‹è¯•FlashAttention...")
        flash_attn = FlashAttention()  # å‡è®¾æˆ‘ä»¬æœ‰è¿™ä¸ªå®ç°

        start_time = time.time()
        output3, weights3 = flash_attn.forward(Q, K, V, mask)
        flash_time = time.time() - start_time
        flash_memory = 3 * batch_size * seq_len * d_model * 4 / 1024 / 1024  # ä»…QKV + è¾“å‡º

        # éªŒè¯ç»“æœä¸€è‡´æ€§
        max_diff = torch.max(torch.abs(output1 - output2))
        print(f"ç»“æœä¸€è‡´æ€§æ£€æŸ¥: æœ€å¤§å·®å¼‚ = {max_diff:.6f}")

        # è®°å½•ç»“æœ
        results["seq_lengths"].append(seq_len)
        results["unfused_time"].append(unfused_time)
        results["unfused_memory"].append(unfused_memory)
        results["fused_time"].append(fused_time)
        results["fused_memory"].append(fused_memory)
        results["flash_time"].append(flash_time)
        results["flash_memory"].append(flash_memory)

        # æ‰“å°å½“å‰é…ç½®çš„ç»“æœ
        print(f"Unfused: {unfused_time:.4f}s, {unfused_memory:.1f}MB")
        print(f"Fused:    {fused_time:.4f}s, {fused_memory:.1f}MB")
        print(f"Flash:   {flash_time:.4f}s, {flash_memory:.1f}MB")

    # å¯è§†åŒ–ç»“æœ
    visualize_performance_results(results)

def visualize_performance_results(results):
    """å¯è§†åŒ–æ€§èƒ½æµ‹è¯•ç»“æœ"""

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # 1. æ‰§è¡Œæ—¶é—´å¯¹æ¯”
    axes[0, 0].plot(results["seq_lengths"], results["unfused_time"], 'r-o', label='Unfused')
    axes[0, 0].plot(results["seq_lengths"], results["fused_time"], 'g-s', label='Fused')
    axes[0, 0].plot(results["seq_lengths"], results["flash_time"], 'b-^', label='Flash')
    axes[0, 0].set_xlabel('åºåˆ—é•¿åº¦')
    axes[0, 0].set_ylabel('æ‰§è¡Œæ—¶é—´ (ç§’)')
    axes[0, 0].set_title('æ‰§è¡Œæ—¶é—´å¯¹æ¯”')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # 2. å†…å­˜ä½¿ç”¨å¯¹æ¯”
    axes[0, 1].plot(results["seq_lengths"], results["unfused_memory"], 'r-o', label='Unfused')
    axes[0, 1].plot(results["seq_lengths"], results["fused_memory"], 'g-s', label='Fused')
    axes[0, 1].plot(results["seq_lengths"], results["flash_memory"], 'b-^', label='Flash')
    axes[0, 1].set_xlabel('åºåˆ—é•¿åº¦')
    axes[0, 1].set_ylabel('å†…å­˜ä½¿ç”¨ (MB)')
    axes[0, 1].set_title('å†…å­˜ä½¿ç”¨å¯¹æ¯”')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # 3. åŠ é€Ÿæ¯”ï¼ˆç›¸å¯¹äºUnfusedï¼‰
    fused_speedup = [u/f for u, f in zip(results["unfused_time"], results["fused_time"])]
    flash_speedup = [u/f for u, f in zip(results["unfused_time"], results["flash_time"])]

    axes[1, 0].plot(results["seq_lengths"], fused_speedup, 'g-s', label='Fused vs Unfused')
    axes[1, 0].plot(results["seq_lengths"], flash_speedup, 'b-^', label='Flash vs Unfused')
    axes[1, 0].set_xlabel('åºåˆ—é•¿åº¦')
    axes[1, 0].set_ylabel('åŠ é€Ÿæ¯”')
    axes[1, 0].set_title('åŠ é€Ÿæ¯”å¯¹æ¯”')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # 4. å†…å­˜èŠ‚çœæ¯”ä¾‹ï¼ˆç›¸å¯¹äºUnfusedï¼‰
    fused_memory_saving = [(u-f)/u*100 for u, f in zip(results["unfused_memory"], results["fused_memory"])]
    flash_memory_saving = [(u-f)/u*100 for u, f in zip(results["unfused_memory"], results["flash_memory"])]

    axes[1, 1].plot(results["seq_lengths"], fused_memory_saving, 'g-s', label='Fused vs Unfused')
    axes[1, 1].plot(results["seq_lengths"], flash_memory_saving, 'b-^', label='Flash vs Unfused')
    axes[1, 1].set_xlabel('åºåˆ—é•¿åº¦')
    axes[1, 1].set_ylabel('å†…å­˜èŠ‚çœæ¯”ä¾‹ (%)')
    axes[1, 1].set_title('å†…å­˜èŠ‚çœå¯¹æ¯”')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

comprehensive_performance_benchmark()
```

## ğŸ¯ å®é™…åº”ç”¨æŒ‡å¯¼

### é€‰æ‹©æŒ‡å—

```python
def attention_backend_selection_guide():
    """Attentionåç«¯é€‰æ‹©æŒ‡å—"""

    print("Attentionåç«¯é€‰æ‹©æŒ‡å—")
    print("=" * 50)

    scenarios = [
        {
            "åœºæ™¯": "å¼€å‘å’Œè°ƒè¯•",
            "åºåˆ—é•¿åº¦": "< 512",
            "å†…å­˜çº¦æŸ": "å®½æ¾",
            "ç²¾åº¦è¦æ±‚": "é«˜",
            "æ¨èåç«¯": "UnfusedAttention",
            "åŸå› ": "æ˜“äºç†è§£å’Œè°ƒè¯•ï¼Œä¸­é—´ç»“æœå¯è®¿é—®"
        },
        {
            "åœºæ™¯": "ä¸­ç­‰è§„æ¨¡æ¨¡å‹è®­ç»ƒ",
            "åºåˆ—é•¿åº¦": "512-2048",
            "å†…å­˜çº¦æŸ": "ä¸­ç­‰",
            "ç²¾åº¦è¦æ±‚": "é«˜",
            "æ¨èåç«¯": "FusedAttention",
            "åŸå› ": "æ€§èƒ½å’Œå†…å­˜çš„å¹³è¡¡ï¼Œç²¾åº¦ä¿è¯"
        },
        {
            "åœºæ™¯": "é•¿åºåˆ—å¤„ç†",
            "åºåˆ—é•¿åº¦": "2048-8192",
            "å†…å­˜çº¦æŸ": "ä¸¥æ ¼",
            "ç²¾åº¦è¦æ±‚": "é«˜",
            "æ¨èåç«¯": "FlashAttention",
            "åŸå› ": "å†…å­˜ä½¿ç”¨æœ€ä¼˜ï¼Œä¿æŒç²¾åº¦"
        },
        {
            "åœºæ™¯": "è¶…é•¿åºåˆ—å¤„ç†",
            "åºåˆ—é•¿åº¦": "> 8192",
            "å†…å­˜çº¦æŸ": "æåº¦ä¸¥æ ¼",
            "ç²¾åº¦è¦æ±‚": "ä¸­ç­‰",
            "æ¨èåç«¯": "FlashAttention + å…¶ä»–ä¼˜åŒ–",
            "åŸå› ": "éœ€è¦ç»“åˆå…¶ä»–ä¼˜åŒ–æŠ€æœ¯"
        },
        {
            "åœºæ™¯": "æ¨ç†éƒ¨ç½²",
            "åºåˆ—é•¿åº¦": "ä»»æ„",
            "å†…å­˜çº¦æŸ": "ä¸­ç­‰",
            "ç²¾åº¦è¦æ±‚": "é«˜",
            "æ¨èåç«¯": "æ ¹æ®åºåˆ—é•¿åº¦åŠ¨æ€é€‰æ‹©",
            "åŸå› ": "ä¸åŒåœºæ™¯éœ€è¦ä¸åŒç­–ç•¥"
        }
    ]

    print(f"{'åœºæ™¯':<20} {'åºåˆ—é•¿åº¦':<12} {'æ¨èåç«¯':<20} {'ä¸»è¦åŸå› '}")
    print("-" * 80)

    for scenario in scenarios:
        print(f"{scenario['åœºæ™¯']:<20} {scenario['åºåˆ—é•¿åº¦']:<12} {scenario['æ¨èåç«¯']:<20} {scenario['åŸå› ']}")

    print("\nå†³ç­–æ ‘:")
    print("-" * 30)
    print("if åºåˆ—é•¿åº¦ < 512:")
    print("    if éœ€è¦è°ƒè¯• â†’ UnfusedAttention")
    print("    else â†’ FusedAttention")
    print("elif åºåˆ—é•¿åº¦ < 2048:")
    print("    if å†…å­˜å……è¶³ â†’ FusedAttention")
    print("    else â†’ FlashAttention")
    print("else:")
    print("    FlashAttention + ä¼˜åŒ–ç­–ç•¥")

attention_backend_selection_guide()
```

### å®é™…éƒ¨ç½²ç­–ç•¥

```python
class AttentionBackendManager:
    """Attentionåç«¯ç®¡ç†å™¨"""

    def __init__(self):
        self.backends = {
            'unfused': UnfusedAttention(),
            'fused': FusedAttention(),
            'flash': FlashAttention()  # å‡è®¾å¯ç”¨
        }
        self.performance_cache = {}

    def select_backend(self, seq_len, batch_size, d_model, memory_budget_mb, enable_debug=False):
        """
        æ™ºèƒ½é€‰æ‹©åç«¯

        Args:
            seq_len: åºåˆ—é•¿åº¦
            batch_size: æ‰¹æ¬¡å¤§å°
            d_model: æ¨¡å‹ç»´åº¦
            memory_budget_mb: å†…å­˜é¢„ç®—
            enable_debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
        """

        # å¦‚æœå¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œå¼ºåˆ¶ä½¿ç”¨UnfusedAttention
        if enable_debug:
            return self.backends['unfused']

        # æ£€æŸ¥ç¼“å­˜
        cache_key = (seq_len, batch_size, d_model)
        if cache_key in self.performance_cache:
            return self.backends[self.performance_cache[cache_key]]

        # ä¼°ç®—å†…å­˜éœ€æ±‚
        unfused_memory = 3 * batch_size * seq_len * d_model * 4 / 1024 / 1024  # QKV + æ³¨æ„åŠ›çŸ©é˜µ
        flash_memory = 3 * batch_size * seq_len * d_model * 4 / 1024 / 1024  # ä»…QKV

        # å†³ç­–é€»è¾‘
        if seq_len < 512:
            backend = 'unfused'
        elif seq_len < 2048:
            if memory_budget_mb > unfused_memory * 1.5:
                backend = 'fused'
            else:
                backend = 'flash'
        else:
            if memory_budget_mb > flash_memory:
                backend = 'flash'
            else:
                # å†…å­˜ä¸è¶³ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–
                backend = 'flash'
                print(f"è­¦å‘Š: å†…å­˜é¢„ç®—({memory_budget_mb}MB)ä¸è¶³ï¼Œè€ƒè™‘ä½¿ç”¨å…¶ä»–ä¼˜åŒ–ç­–ç•¥")

        # ç¼“å­˜å†³ç­–
        self.performance_cache[cache_key] = backend
        return self.backends[backend]

    def get_performance_estimate(self, backend_name, seq_len, batch_size, d_model):
        """è·å–æ€§èƒ½ä¼°ç®—"""

        estimates = {
            'unfused': {
                'memory_mb': 3 * batch_size * seq_len * d_model * 4 / 1024 / 1024 + seq_len * seq_len * batch_size * 4 / 1024 / 1024,
                'compute_flops': seq_len * seq_len * d_model * 6,
                'kernel_count': 5,
                'accuracy': 'exact'
            },
            'fused': {
                'memory_mb': 3 * batch_size * seq_len * d_model * 4 / 1024 / 1024 + seq_len * seq_len * batch_size * 4 / 1024 / 1024,
                'compute_flops': seq_len * seq_len * d_model * 6,
                'kernel_count': 1,
                'accuracy': 'exact'
            },
            'flash': {
                'memory_mb': 3 * batch_size * seq_len * d_model * 4 / 1024 / 1024,
                'compute_flops': seq_len * seq_len * d_model * 6,
                'kernel_count': 'block_count',
                'accuracy': 'exact'
            }
        }

        return estimates.get(backend_name, {})

# ä½¿ç”¨ç¤ºä¾‹
def test_backend_manager():
    """æµ‹è¯•åç«¯ç®¡ç†å™¨"""

    print("Attentionåç«¯ç®¡ç†å™¨æµ‹è¯•")
    print("=" * 50)

    manager = AttentionBackendManager()

    # æµ‹è¯•ä¸åŒåœºæ™¯
    test_cases = [
        (256, 8, 512, 1000, False),   # çŸ­åºåˆ—ï¼Œå……è¶³å†…å­˜
        (512, 4, 768, 500, False),    # ä¸­ç­‰åºåˆ—ï¼Œä¸­ç­‰å†…å­˜
        (2048, 2, 1024, 200, False),   # é•¿åºåˆ—ï¼Œå†…å­˜å—é™
        (256, 8, 512, 1000, True),    # è°ƒè¯•æ¨¡å¼
    ]

    for seq_len, batch_size, d_model, memory, debug in test_cases:
        backend = manager.select_backend(seq_len, batch_size, d_model, memory, debug)
        performance = manager.get_performance_estimate(backend.name, seq_len, batch_size, d_model)

        print(f"\nåœºæ™¯: seq_len={seq_len}, batch_size={batch_size}, memory={memory}MB, debug={debug}")
        print(f"é€‰æ‹©åç«¯: {backend.name}")
        print(f"æ€§èƒ½ä¼°ç®—: å†…å­˜={performance['memory_mb']:.1f}MB, æ ¸å‡½æ•°æ•°={performance['kernel_count']}")

test_backend_manager()
```

## ğŸ¯ æ€»ç»“ä¸æœ€ä½³å®è·µ

### æ ¸å¿ƒè¦ç‚¹å›é¡¾

é€šè¿‡å¯¹æ¯”ä¸‰ç§Attentionå®ç°ï¼Œæˆ‘ä»¬å¯ä»¥æ€»ç»“å‡ºï¼š

1. **UnfusedAttention**ï¼šé€‚åˆå¼€å‘å’Œè°ƒè¯•ï¼Œæ€§èƒ½ä¸€èˆ¬ä½†æœ€æ˜“ç†è§£
2. **FusedAttention**ï¼šæ€§èƒ½å’Œå†…å­˜çš„å¹³è¡¡ç‚¹ï¼Œé€‚åˆä¸­ç­‰è§„æ¨¡é—®é¢˜
3. **FlashAttention**ï¼šé•¿åºåˆ—çš„æœ€ä¼˜é€‰æ‹©ï¼Œå†…å­˜æ•ˆç‡æé«˜

### é€‰æ‹©å»ºè®®

**æ ¹æ®åºåˆ—é•¿åº¦é€‰æ‹©ï¼š**
- **çŸ­åºåˆ— (< 512)**ï¼šUnfusedAttentionï¼ˆæ˜“äºè°ƒè¯•ï¼‰
- **ä¸­ç­‰åºåˆ— (512-2048)**ï¼šFusedAttentionï¼ˆæ€§èƒ½å¹³è¡¡ï¼‰
- **é•¿åºåˆ— (> 2048)**ï¼šFlashAttentionï¼ˆå†…å­˜æœ€ä¼˜ï¼‰

**æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©ï¼š**
- **ç ”ç©¶å¼€å‘**ï¼šUnfusedAttentionï¼ˆé€æ˜åº¦é«˜ï¼‰
- **ç”Ÿäº§è®­ç»ƒ**ï¼šFusedAttentionæˆ–FlashAttentionï¼ˆæ€§èƒ½ä¼˜å…ˆï¼‰
- **æ¨ç†éƒ¨ç½²**ï¼šæ ¹æ®å…·ä½“éœ€æ±‚åŠ¨æ€é€‰æ‹©

### æœªæ¥å‘å±•è¶‹åŠ¿

1. **è‡ªåŠ¨åç«¯é€‰æ‹©**ï¼šæ ¹æ®ç¡¬ä»¶å’Œåœºæ™¯è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åç«¯
2. **æ··åˆåç«¯ç­–ç•¥**ï¼šä¸åŒå±‚ä½¿ç”¨ä¸åŒåç«¯
3. **ç¡¬ä»¶ç‰¹å®šä¼˜åŒ–**ï¼šé’ˆå¯¹æ–°æ¶æ„çš„ä¸“é—¨ä¼˜åŒ–
4. **ç²¾åº¦å¯æ§**ï¼šåœ¨æ€§èƒ½å’Œç²¾åº¦é—´çµæ´»æƒè¡¡

---

**è®°ä½**ï¼šæ²¡æœ‰"æœ€å¥½"çš„Attentionå®ç°ï¼Œåªæœ‰"æœ€é€‚åˆ"çš„ã€‚ç†è§£æ¯ç§åç«¯çš„åŸç†å’Œç‰¹ç‚¹ï¼Œæ‰èƒ½åœ¨å®é™…åº”ç”¨ä¸­åšå‡ºæœ€ä¼˜çš„é€‰æ‹©ã€‚è¿™ç§æƒè¡¡çš„è‰ºæœ¯ï¼Œæ­£æ˜¯æ·±åº¦å­¦ä¹ å·¥ç¨‹çš„æ ¸å¿ƒæŠ€èƒ½ä¹‹ä¸€ã€‚

*ä¸‹ä¸€ç¯‡æ–‡ç« å°†æ·±å…¥è§£æPagedAttentionå’Œå†…å­˜ä¼˜åŒ–æŠ€æœ¯ï¼Œæ¢ç´¢å¦‚ä½•è§£å†³è¶…é•¿åºåˆ—çš„å†…å­˜ç“¶é¢ˆé—®é¢˜ã€‚* ğŸš€